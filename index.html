<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Dynamic PRM-Search-LLM Flow (5 Balls)</title>
  <style>
    :root { --bg:#f8fafc; --ink:#111827; --arrow:#94a3b8; --accent:#fb923c; }
    body{margin:0;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial;background:var(--bg);color:var(--ink)}
    .wrap{width:100%;margin:24px auto;padding:0}
    .panel{background:transparent;border:none;border-radius:0;box-shadow:none;padding:0}
    .hint{color:#475569;font-size:14px;margin-top:8px;text-align:center}
    .token{filter:drop-shadow(0 1px 2px rgba(0,0,0,.35));offset-rotate:0deg}
    @keyframes move{from{offset-distance:0%}to{offset-distance:100%}}
  </style>

  <meta charset="UTF-8">
  <title>Benchmarks</title>
  <!-- DataTables + jQuery -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.4/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.datatables.net/1.13.4/js/jquery.dataTables.min.js"></script>
  
  <meta charset="UTF-8">
  <style>
    .hanoi-section {
      font-family: sans-serif;
      background: #fefefe;
      padding: 2rem;
      text-align: center;
    }
    .peg-container {
      display: flex;
      justify-content: space-around;
      align-items: flex-end;
      height: 200px;
      margin-top: 3rem;
    }
    .peg {
      width: 10px;
      height: 150px;
      background: #333;
      position: relative;
    }
    .disk {
      position: absolute;
      height: 20px;
      border-radius: 5px;
      left: 50%;
      transform: translateX(-50%);
    }
    .disk1 { width: 100px; background: #FF6B6B; }
    .disk2 { width: 70px; background: #4ECDC4; }
    .disk3 { width: 40px; background: #1A535C; }

    .peg-label {
      margin-top: 1rem;
      font-weight: bold;
    }
  </style>

  <meta charset="UTF-8">
  <title>Claude Chart Test</title>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>


</head>

<body>

  <h1>Reasoning Architectures</h2>
  <h2>Pipeline</h2>

  <p>
    While DeepSeek R1 is a dedicated reasoning model trained with Chain-of-Thought (CoT) data and methods such as
    supervised fine-tuning (SFT) and reinforcement learning (RL), it's also possible to build a reasoning architecture around a
    standard language model.
  </p>

  <p>
    In this setup, an LLM (e.g., a Llama 1B model) produces multiple candidate answers. A reward model then
    scores these candidates, and a search strategy selects the best answer based on the rewards. This kind of architecture is described in the Hugging Face cookbook:
    <a href="https://huggingface.co/learn/cookbook/search_and_learn" target="_blank" rel="noopener">“Search &amp; Learn”</a>.
  </p>

  <p>
    The dynamic chart below visualizes the architecture: the LLM proposes candidates, the PRM scores them, and the search strategy
    steers the loop toward the final answer.
  </p>

<div class="wrap">
  <h3 style="text-align:center">Dynamic PRM-Guided Search Flow</h3>
  <div class="panel">
    <svg id="scene" viewBox="0 0 1200 280" width="100%" height="350">
      <defs>
        <marker id="arrow" markerWidth="10" markerHeight="10" refX="8" refY="3" orient="auto" markerUnits="userSpaceOnUse">
          <path d="M0,0 L9,3 L0,6 Z" fill="var(--arrow)" />
        </marker>
        <filter id="soft" x="-50%" y="-50%" width="200%" height="200%">
          <feGaussianBlur in="SourceGraphic" stdDeviation="3" />
        </filter>
      </defs>

      <script type="application/json" id="points">{
        "math": {"x":70,  "y":210},
        "llm":  {"x":310, "y":210},
        "prm":  {"x":620, "y":210},
        "search":{"x":620, "y":90 },
        "final": {"x":1070,"y":210}
      }</script>

      <g id="edges" stroke="var(--arrow)" stroke-width="3" marker-end="url(#arrow)" opacity=".65" fill="none"></g>
      <g id="nodes"></g>
      <circle cx="620" cy="90" r="62" fill="var(--accent)" opacity=".15" filter="url(#soft)" />
      <g id="tokenLayer"></g>
    </svg>
  </div>
</div>

<script>
(function(){
  const P = JSON.parse(document.getElementById('points').textContent);
  const svgNS = 'http://www.w3.org/2000/svg';
  const rightOf = (p)=>[p.x+90,p.y];
  const leftOf  = (p)=>[p.x-90,p.y];
  const topOf   = (p)=>[p.x,p.y-40];
  const C = (x1,y1,x2,y2,x,y)=>`C ${x1} ${y1}, ${x2} ${y2}, ${x} ${y}`;
  const seg=(from,to,bendY)=>{const midX=(from[0]+to[0])/2,by=bendY??from[1];return `M ${from[0]} ${from[1]} ${C(midX,by,midX,by,to[0],to[1])}`};

  const paths={
    mathToLLM: seg(rightOf(P.math), leftOf(P.llm)),
    llmToPRM: seg(rightOf(P.llm), leftOf(P.prm)),
    prmToSearch: seg(rightOf(P.prm), [P.search.x, P.search.y], P.prm.y - 80),
    searchToLLM: seg([P.search.x-90,P.search.y],[P.llm.x,P.llm.y-40], P.search.y-30),
    prmToFinal: seg(rightOf(P.prm), leftOf(P.final))
  };

  function addNode(group,x,y,label,fill){
    const g=document.createElementNS(svgNS,'g');
    g.setAttribute('transform',`translate(${x-90},${y-40})`);
    const rect=document.createElementNS(svgNS,'rect');
    rect.setAttribute('rx','18');rect.setAttribute('width','180');rect.setAttribute('height','80');
    rect.setAttribute('fill',fill);rect.setAttribute('style','filter:drop-shadow(0 1px 2px rgba(0,0,0,.15))');
    const text=document.createElementNS(svgNS,'text');
    text.setAttribute('x','90');text.setAttribute('y','44');text.setAttribute('text-anchor','middle');
    text.setAttribute('fill','#111827');text.setAttribute('style','font-weight:600');
    text.textContent=label;
    g.appendChild(rect);g.appendChild(text);group.appendChild(g);
  }

  const nodesG=document.getElementById('nodes');
  addNode(nodesG,P.math.x,P.math.y,'Math problem','#bae6fd');
  addNode(nodesG,P.llm.x,P.llm.y,'LLM','#d8b4fe');
  addNode(nodesG,P.prm.x,P.prm.y,'PRM','#fef08a');
  addNode(nodesG,P.search.x,P.search.y,'Search strategy','#fdba74');
  addNode(nodesG,P.final.x,P.final.y,'Final answer','#bae6fd');

  const edgesG=document.getElementById('edges');
  Object.values(paths).forEach(d=>{const path=document.createElementNS(svgNS,'path');path.setAttribute('d',d);edgesG.appendChild(path);});

  const tokens=[
    {d:paths.mathToLLM, delay:0.0, color:'#3b82f6'},
    {d:paths.llmToPRM, delay:0.8, color:'#7c3aed'},
    {d:paths.prmToSearch, delay:1.6, color:'#f59e0b'},
    {d:paths.searchToLLM, delay:2.4, color:'#10b981'},
    {d:paths.prmToFinal, delay:3.2, color:'#22c55e'}
  ];

  const tokenLayer=document.getElementById('tokenLayer');
  tokens.forEach(t=>{
    const c=document.createElementNS(svgNS,'circle');
    c.setAttribute('r','10');
    c.setAttribute('fill',t.color);
    c.classList.add('token');
    c.style.offsetPath=`path('${t.d}')`;
    c.style.animation=`move 4s linear ${t.delay}s infinite`;
    c.style.animationPlayState='running';
    tokenLayer.appendChild(c);
  });
})();
</script>

<!-- Edges -->
<g id="edges" stroke="var(--arrow)" stroke-width="3" marker-end="url(#arrow)" opacity=".65" fill="none"></g>


<!-- Nodes -->
<g id="nodes"></g>


<!-- Glow under Search -->
<circle cx="620" cy="90" r="62" fill="var(--accent)" opacity=".15" filter="url(#soft)" />


<!-- Tokens -->
<g id="tokenLayer"></g>
</svg>
</div>
</div>

<script>
function addNode(group,x,y,label,fill){
const g=document.createElementNS(svgNS,'g');
g.setAttribute('transform',`translate(${x-90},${y-40})`);
const rect=document.createElementNS(svgNS,'rect');
rect.setAttribute('rx','18');rect.setAttribute('width','180');rect.setAttribute('height','80');
rect.setAttribute('fill',fill);rect.setAttribute('style','filter:drop-shadow(0 1px 2px rgba(0,0,0,.15))');
const text=document.createElementNS(svgNS,'text');
text.setAttribute('x','90');text.setAttribute('y','44');text.setAttribute('text-anchor','middle');
text.setAttribute('fill','#111827');text.setAttribute('style','font-weight:600');
text.textContent=label;
g.appendChild(rect);g.appendChild(text);group.appendChild(g);
}


const nodesG=document.getElementById('nodes');
addNode(nodesG,P.math.x,P.math.y,'Math problem','#bae6fd');
addNode(nodesG,P.llm.x,P.llm.y,'LLM','#d8b4fe');
addNode(nodesG,P.prm.x,P.prm.y,'PRM','#fef08a');
addNode(nodesG,P.search.x,P.search.y,'Search strategy','#fdba74');
addNode(nodesG,P.final.x,P.final.y,'Final answer','#bae6fd');


// Draw edges
const edgesG=document.getElementById('edges');
Object.values(paths).forEach(d=>{
const path=document.createElementNS(svgNS,'path');
path.setAttribute('d',d);
edgesG.appendChild(path);
});


// Tokens definitions
const tokens=[
{d:paths.mathToLLM, delay:0.0, color:'#3b82f6'},
{d:paths.llmToPRM, delay:0.8, color:'#7c3aed'},
{d:paths.prmToSearch, delay:1.6, color:'#f59e0b'},
{d:paths.searchToLLM, delay:2.4, color:'#10b981'},
{d:paths.prmToFinal, delay:3.2, color:'#22c55e'}
];


const tokenLayer=document.getElementById('tokenLayer');
tokens.forEach(t=>{
const c=document.createElementNS(svgNS,'circle');
c.setAttribute('r','10');
c.setAttribute('fill',t.color);
c.classList.add('token');
c.dataset.base='6';
c.dataset.delay=t.delay;
c.style.offsetPath=`path('${t.d}')`;
tokenLayer.appendChild(c);
});


const playBtn=document.getElementById('playBtn');
const stepBtn=document.getElementById('stepBtn');
const speed=document.getElementById('speed');
const speedVal=document.getElementById('speedVal');
let playing=true;


function apply(){
const s=parseFloat(speed.value);
speedVal.textContent=s.toFixed(2)+'×';
document.querySelectorAll('.token').forEach(el=>{
const base=parseFloat(el.dataset.base||'6');
const dur=(base/s).toFixed(2)+'s';
const delay=parseFloat(el.dataset.delay||'0');
el.style.animation=`move ${dur} linear ${delay}s infinite`;
el.style.animationPlayState=playing?'running':'paused';
});
}
function restart(){
document.querySelectorAll('.token').forEach(el=>{
const anim=el.style.animation;el.style.animation='none';void el.offsetWidth;el.style.animation=anim;
el.style.animationPlayState=playing?'running':'paused';
});
}


playBtn.addEventListener('click',()=>{
playing=!playing;playBtn.textContent=playing?'Pause':'Play';
document.querySelectorAll('.token').forEach(el=>el.style.animationPlayState=playing?'running':'paused');
});
stepBtn.addEventListener('click',()=>{playing=false;playBtn.textContent='Play';document.querySelectorAll('.token').forEach(el=>el.style.animationPlayState='paused');restart();});
speed.addEventListener('input',apply);

apply();
})();
</script>

<h2 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Search-and-Learn Repository</h3>
<p>
    To reproduce the pipeline one can use the Search-and-Learn repository: <a href="https://github.com/huggingface/search-and-learn" 
    target="_blank" rel="noopener">huggingface/search-and-learn</a>. </p>
   <p> 
    Clone the repository, install dependencies as described in the repo, follow the 
    pipeline guide in the cookbook: <a href="https://huggingface.co/learn/cookbook/search_and_learn" target="_blank" rel="noopener">
      Search &amp; Learn tutorial</a>. </p>


  <h1>How Good Are Models at Reasoning?</h1>
  <h2>Benchmarks</h2>
  <p>
    Benchmarks are widely used to evaluate the capabilities of large language models (LLMs).
    They consist of curated problem sets focused on specific skills - such as mathematics, programming, scientific understanding, or medical diagnostics.
    By testing models on these tasks, we get a snapshot of their reasoning power and generalization abilities.
  </p>
  <h3>Benchmark Reasoning Categories</h3>
  <p>
    Please see the description of possible problem sets in the table below. Each category represents a distinct type of reasoning  challenge used in 
    benchmark evaluations. You can search through the table by typing keywords (e.g. <em>"math"</em>, <em>"medical"</em>, <em>"code"</em>) into the
    search bar in the top-right corner of the table.
  </p>
    <table id="benchmark-table" class="display">
      <thead>
        <tr>
          <th>Category</th>
          <th>Description</th>
          <th>Benchmarks</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Math Problems</td>
          <td>Solves mathematical problems showcasing capabilities of reasoning LLMs.</td>
          <td>AIME, MATH-500, OlympiadBench</td>
        </tr>
        <tr>
          <td>Code Problems</td>
          <td>Uses logical thinking and structured problem-solving in programming tasks.</td>
          <td>Codeforces, LiveCodeBench</td>
        </tr>
        <tr>
          <td>Scientific Problems</td>
          <td>Involves multi-domain reasoning across physics, chemistry, and biology.</td>
          <td>GPQA Diamond, MMLU-Pro</td>
        </tr>
        <tr>
          <td>Agent Reasoning</td>
          <td>Tests planning and decision-making in interactive and tool-using environments.</td>
          <td>WebShop, WebArena, SciWorld</td>
        </tr>
        <tr>
          <td>Medical Reasoning</td>
          <td>Mimics diagnostic reasoning and treatment planning in clinical contexts.</td>
          <td>MedQA, Medbullets</td>
        </tr>
        <tr>
          <td>Multimodal Reasoning</td>
          <td>Combines text and visual input to test cross-modal reasoning skills.</td>
          <td>MMMU, MathVista, MM-IQ</td>
        </tr>
      </tbody>
    </table>
    <br>
      A strong model is expected to perform well across multiple diverse benchmarks, demonstrating not just memorization or task-specific tricks, but real, 
      transferable reasoning. In this way, benchmarks help define what it means for a model to be universal rather than narrowly overfitted.
    <br>
    <script>
      $(document).ready(function () {
        $('#benchmark-table').DataTable({
          paging: false,
          info: false,
          language: {
            search: "🔍 Search benchmarks:"
          }
        });
      });
    </script>

<body>
  <h3>State-of-the-Art Model Performance on Benchmarks</h3>
  <p>
    The following table shows Pass@1 or Percentile scores across several benchmark tasks.
  </p>
  <table id="benchmarkTable" class="display">
    <thead>
      <tr>
        <th>Benchmark</th>
        <th>Metric</th>
        <th>DeepSeek-R1</th>
        <th>OpenAI-o1-1217</th>
        <th>DeepSeek-R1-32B</th>
        <th>OpenAI-o1-mini</th>
        <th>DeepSeek-V3</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>AIME 2024</td>
        <td>Pass@1</td>
        <td>79.8%</td>
        <td>79.2%</td>
        <td>72.6%</td>
        <td>63.6%</td>
        <td>39.2%</td>
      </tr>
      <tr>
        <td>Codeforces</td>
        <td>Percentile</td>
        <td>96.3%</td>
        <td>96.6%</td>
        <td>90.6%</td>
        <td>93.4%</td>
        <td>58.7%</td>
      </tr>
      <tr>
        <td>GPQA Diamond</td>
        <td>Pass@1</td>
        <td>71.5%</td>
        <td>75.7%</td>
        <td>62.1%</td>
        <td>60.0%</td>
        <td>59.1%</td>
      </tr>
      <tr>
        <td>MATH-500</td>
        <td>Pass@1</td>
        <td>97.3%</td>
        <td>96.4%</td>
        <td>94.3%</td>
        <td>90.0%</td>
        <td>90.2%</td>
      </tr>
      <tr>
        <td>MMLU</td>
        <td>Pass@1</td>
        <td>90.8%</td>
        <td>91.8%</td>
        <td>87.4%</td>
        <td>85.2%</td>
        <td>88.5%</td>
      </tr>
      <tr>
        <td>SWE-bench Verified</td>
        <td>Resolved</td>
        <td>49.2%</td>
        <td>48.9%</td>
        <td>41.6%</td>
        <td>36.8%</td>
        <td>42.0%</td>
      </tr>
    </tbody>
  </table>

  <p class="citation">(DeepSeek-AI et al., 2025)</p>

  <script>
    $(document).ready(function () {
      $('#benchmarkTable').DataTable({
        paging: false,
        info: false,
        responsive: true,
        language: {
          search: "🔍 Search benchmarks:"
        }
      });
    });
  </script>

<hr>

<h3>The Problem With Benchmarks</h3>
<p>Despite their usefulness, traditional benchmarks are facing growing criticism.</p>

<p>
  Task difficulty is often hard to define. What makes a problem "difficult" is often subjective. This makes it tricky to scale problem sets meaningfully 
  or test a model's performance on progressively harder tasks.
</p>
<p>
  Another concern is data leakage from training corpora. Many benchmark problems have ended up in the training data of large models, whether intentionally or not. 
  This makes it unclear whether the model is reasoning through a solution — or simply memorizing and regurgitating it.
</p>

<h2>Puzzle-Based Benchmarks</h2>
<p><em>Shojaee et al., 2025</em></p>
<p>
  To address these limitations, researchers are exploring alternative benchmarks. One compelling direction is using puzzle-like problems, such as the 
  Tower of Hanoi, where complexity can be precisely controlled — by simply increasing the number of disks. In this setup, models are evaluated not just 
  on their accuracy, but on how well they scale with increasing task difficulty.
</p>

<div class="hanoi-section">
<h4>Tower of Hanoi (3 Disks)</h4>
<p>Auto-solving animation (A → C)</p>

<div class="peg-container">
  <div class="peg" id="pegA"></div>
  <div class="peg" id="pegB"></div>
  <div class="peg" id="pegC"></div>
</div>

<div class="peg-container">
  <div class="peg-label">A</div>
  <div class="peg-label">B</div>
  <div class="peg-label">C</div>
</div>

<script>
  const pegs = {
    A: document.getElementById('pegA'),
    B: document.getElementById('pegB'),
    C: document.getElementById('pegC'),
  };

  const disks = [
    { class: 'disk1', size: 3 },
    { class: 'disk2', size: 2 },
    { class: 'disk3', size: 1 },
  ];

  let state;

  const sleep = (ms) => new Promise(res => setTimeout(res, ms));

  function resetPegs() {
    // Clear pegs
    pegs.A.innerHTML = '';
    pegs.B.innerHTML = '';
    pegs.C.innerHTML = '';

    // Reset state
    state = { A: [], B: [], C: [] };

    // Add disks back to peg A
    disks.forEach((d, i) => {
      const el = document.createElement('div');
      el.className = `disk ${d.class}`;
      el.style.bottom = `${i * 22}px`;
      pegs.A.appendChild(el);
      state.A.push(el);
    });
  }

  async function move(n, from, to, aux) {
    if (n === 0) return;
    await move(n - 1, from, aux, to);

    await sleep(800);
    const disk = state[from].pop();
    state[to].push(disk);
    pegs[to].appendChild(disk);
    disk.style.bottom = `${(state[to].length - 1) * 22}px`;

    await move(n - 1, aux, to, from);
  }

  async function loopHanoi() {
    while (true) {
      resetPegs();
      await move(3, 'A', 'C', 'B');
      await sleep(1000); // pause before restarting
    }
  }

  // Start looping animation
  loopHanoi();
</script>

</div>

<h3>When Reasoning Models Collapse</h3>
<p>
  A surprising trend emerges as task complexity increases: reasoning-tuned models initially outperform their baseline counterparts, 
  but beyond a certain threshold, both collapse in performance. This sharp decline challenges assumptions about the robustness of current 
  reasoning models and exposes their fragility under higher cognitive demands.
</p>

<h4 style="text-align: center;">Model Accuracy vs. Task Complexity</h4>
<canvas id="accuracyChart" class="chart-on-scroll" width="700" height="400"></canvas>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
  function renderAccuracyChart() {
  const ctx = document.getElementById('accuracyChart').getContext('2d');

  new Chart(ctx, {
    type: 'line',
    data: {
      labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
      datasets: [
        {
          label: 'DeepSeek-R1',
          data: [100, 98, 95, 90, 85, 70, 60, 10, 2, 1, 0, 0],
          borderColor: '#4ECDC4',
          backgroundColor: 'transparent',
          tension: 0.3
        },
        {
          label: 'DeepSeek-V3',
          data: [98, 97, 96, 85, 30, 20, 10, 5, 1, 0, 0, 0],
          borderColor: '#FF6B6B',
          backgroundColor: 'transparent',
          tension: 0.3
        }
      ]
    },
    options: {
      responsive: true,
      animation: {
        duration: 2500,
        easing: 'easeOutQuart'
      },
      plugins: {
        legend: {
          position: 'bottom'
        },
        title: {
          display: false
        }
      },
      scales: {
        y: {
          beginAtZero: false,
          max: 100,
          title: {
            display: true,
            text: 'Accuracy (%)'
          }
        },
        x: {
          title: {
            display: true,
            text: 'Task Complexity'
          }
        }
      }
    }
  });
  }
</script>

<script>
  const observer = new IntersectionObserver((entries, observer) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        if (entry.target.id === 'accuracyChart') {
          renderAccuracyChart();
        }
        observer.unobserve(entry.target); // only run once
      }
    });
  }, {
    threshold: 0.4 // Trigger when 40% visible
  });

  document.querySelectorAll('.chart-on-scroll').forEach(chart => {
    observer.observe(chart);
  });
</script>


<h3>Even With Help, They Struggle</h3>
<p>
  Interestingly, even when the model is given the solution in the prompt, performance doesn’t improve significantly.
  This suggests that the model doesn't simply fail to find a solution — it struggles to use or interpret one when given.
</p>

<h4 style="text-align: center;">Claude-3.7-Sonnet Performance</h4>
<canvas id="claudeChart" class="chart-on-scroll" width="700" height="400"></canvas>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<!-- Chart render function -->
<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {

    function renderClaudeChart() {
      const ctx = document.getElementById('claudeChart').getContext('2d');

      new Chart(ctx, {
        type: 'line',
        data: {
          labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
          datasets: [
            {
              label: 'Default',
              data: [99.5, 99.5, 99.5, 99.5, 90, 55, 70, 15, 5, 2, 1, 1],
              borderColor: '#DE6E4B',
              borderWidth: 3,
              pointStyle: 'rect',
              pointRadius: 6,
              tension: 0.3
            },
            {
              label: 'Algorithm Given',
              data: [99.5, 99.5, 99.5, 99.5, 90, 70, 65, 10, 2, 1, 1, 1],
              borderColor: '#B0422F',
              borderDash: [6, 4],
              borderWidth: 3,
              pointStyle: 'circle',
              pointRadius: 6,
              tension: 0.3
            }
          ]
        },
        options: {
          responsive: true,
          animation: {
            duration: 1200,
            easing: 'easeOutQuart'
          },
          plugins: {
            legend: { position: 'bottom' },
            title: {
              display: true,
              text: 'Accuracy vs Complexity (Number of Disks)'
            }
          },
          scales: {
            y: {
              beginAtZero: true,
              max: 100,
              title: { display: true, text: 'Accuracy (%)' }
            },
            x: {
              title: { display: true, text: 'Complexity (Number of Disks)' }
            }
          }
        }
      });
    }

    const observer = new IntersectionObserver((entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.target.id === 'claudeChart') {
          renderClaudeChart();
          observer.unobserve(entry.target);
        }
      });
    }, {
      threshold: 0.4
    });

    document.querySelectorAll('.chart-on-scroll').forEach(chart => {
      observer.observe(chart);
    });

  });
</script>

<h3>A Curious Token Length Effect</h3>
<p>
  Another surprising effect: as task complexity grows, models tend to produce shorter outputs - even though longer answers 
  would likely lead to better performance. This indicates a possible failure in internal planning or token budgeting, challenging the assumption that more capable models
  will naturally expand their answers as needed.</p>

<h4 style="text-align: center;">o3-mini (high): Output Token Count vs Complexity</h4>
<canvas id="tokenChart" class="chart-on-scroll" width="700" height="400"></canvas>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {

    function renderTokenChart() {
      const ctx = document.getElementById('tokenChart').getContext('2d');

      new Chart(ctx, {
        type: 'line',
        data: {
          labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
          datasets: [
            {
              label: 'Average Token Count',
              data: [500, 900, 2000, 4500, 8500, 20000, 18000, 12000, 7000, 5000, 3000, 2000],
              borderColor: '#0077BB',
              borderDash: [6, 4],
              borderWidth: 3,
              fill: false,
              tension: 0.3,
              pointRadius: 6,
              pointStyle: 'circle',
              backgroundColor: 'transparent'
            },
            {
              label: 'Individual Runs',
              type: 'scatter',
              data: [
                { x: 4, y: 4300 }, { x: 4, y: 4700 }, { x: 5, y: 8800 },
                { x: 5, y: 8000 }, { x: 6, y: 19500 }, { x: 6, y: 20500 },
                { x: 7, y: 18000 }, { x: 7, y: 17500 }, { x: 8, y: 11000 },
                { x: 9, y: 7500 }, { x: 9, y: 6900 }, { x: 10, y: 4500 },
                { x: 15, y: 3100 }, { x: 15, y: 2900 }, { x: 20, y: 2100 }
              ],
              backgroundColor: '#E63946',
              pointRadius: 4,
              showLine: false
            }
          ]
        },
        options: {
          responsive: true,
          animation: {
            duration: 1500,
            easing: 'easeOutQuart'
          },
          plugins: {
            legend: {
              position: 'bottom'
            },
            title: {
              display: true,
              text: 'Output Length vs. Complexity (Number of Disks)'
            }
          },
          scales: {
            y: {
              beginAtZero: true,
              title: {
                display: true,
                text: 'Output Tokens'
              }
            },
            x: {
              title: {
                display: true,
                text: 'Complexity (Number of Disks)'
              }
            }
          }
        }
      });
    }

    const observer = new IntersectionObserver((entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.target.id === 'tokenChart') {
          renderTokenChart();
          observer.unobserve(entry.target);
        }
      });
    }, { threshold: 0.4 });

    document.querySelectorAll('.chart-on-scroll').forEach(chart => {
      observer.observe(chart);
    });

  });
</script>


<h2>Open Questions Remain</h2>
<p>
  To what extent can current models really reason? While benchmarks remain a useful tool, these findings highlight the fragility of current 
  approaches and the limitations of today's so-called “reasoning models.” There is still a long road ahead to build systems that reason 
  reliably under increasing task difficulty — and to prove that their reasoning is more than just a memorized pattern.
</p>

</body>
</html>
