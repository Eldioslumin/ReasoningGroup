<!DOCTYPE html>
<html lang="en">
<head>
<style>
  body {
    background: #f8fafc;
    color: #222;
    font-family: 'Inter', Arial, sans-serif;
    margin: 0;
  }
  header {
    background: #2563eb;
    color: #fff;
    padding: 38px 0 24px 0;
    text-align: center;
    border-bottom-left-radius: 18px;
    border-bottom-right-radius: 18px;
    box-shadow: 0 4px 24px #94a3b888;
  }
  header h1 {
    margin: 0;
    font-size: 2.5em;
    font-weight: 700;
    letter-spacing: 0.01em;
  }
  header p {
    margin: 18px 0 0 0;
    font-size: 1.18em;
    opacity: 0.8;
    font-weight: 400;
  }
  main {
    max-width: 900px;
    margin: 38px auto 0 auto;
    padding: 0 24px;
  }

  footer {
    background:#2563eb;
    color:#fff  !important;;
    padding:38px 0 24px;
    text-align:center;
    box-shadow:0 4px 24px #94a3b888;
    display:flex;               
    justify-content:center;

    width:100vw;
    margin-left:calc(50% - 50vw);
    margin-right:calc(50% - 50vw);
    border-top-left-radius:18px;   
    border-top-right-radius:18px;
  }

footer p{
  margin:0;
  max-width:900px;         
  padding:0 24px;            
  text-align:center;
  color:#fff;
  }

  .highlight-box {
  background: #fef9c3;      /* light yellow */
  border-left: 6px solid #facc15; /* accent border */
  padding: 18px 22px;
  border-radius: 10px;
  margin: 28px 0;
  font-size: 1.05em;
  line-height: 1.6;
  color: #374151;
  box-shadow: 0 3px 12px rgba(0,0,0,0.05);
}

.loss2-sec{--ink:#0f172a;--muted:#64748b;--line:#e5e7eb}
.loss2-sec h2{margin:1.25rem 0 .6rem 0; font-size:1.35rem; font-weight:700}
.loss2-sec .sub{color:var(--muted); font-size:15px; margin:.25rem 0 .9rem}
.loss2-sec .eq{margin:.6rem 0 .35rem}
.loss2-sec .katex{font-size:95%}
.loss2-sec .katex.display{margin:.35rem 0}
.loss2-sec p{font-size:15px; line-height:1.6; color:var(--ink); margin:.4rem 0}
.loss2-sec .two-col{display:grid; grid-template-columns:1fr 1fr; gap:1rem; margin:.6rem 0}
.loss2-sec .ans{font-size:15px; line-height:1.55}
.loss2-sec .ans .tag{font-weight:700; margin-right:.25rem}
.loss2-sec .ctrl{display:grid; grid-template-columns:1fr 1fr; gap:.75rem 1rem; align-items:center; margin:.6rem 0}
.loss2-sec label{color:var(--muted); font-size:15px}
.loss2-sec input[type="range"]{width:100%}
.loss2-sec .stat{margin:.35rem 0; font-size:15px}
.loss2-sec code.inline{background:#f8fafc; border:1px solid var(--line); padding:.1rem .4rem; border-radius:6px}
@media (max-width: 820px){
  .loss2-sec .two-col{grid-template-columns:1fr}
  .loss2-sec .ctrl{grid-template-columns:1fr}
}

footer, footer * { color:#fff !important; }
</style>
</head>

<body> 

<meta charset="UTF-8">
<!-- KaTeX JS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<!-- KaTeX JS -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>


</section>

<!-- Heitke's part - scaling reasoning -->
<title>Unlocking Machine Reasoning: The RLM Approach</title>

<section>
  <header>
    <h1>Unlocking Machine Reasoning: The RLM Approach</h1>
    <p>How Language Models Move From Memorization to Deliberative Problem-Solving (Allegedly):<br>Unpacking Strategies and Methods Driving Reasoning in LLMs.
    </p>
  </header>
</section>


<section class="wrap" id="blog-intro">
  <p>
    In this blog, we‚Äôll explore how these strategies are implemented, and why some approaches seem to foster genuine reasoning more effectively than others.
    Our aim is to explore the field of Reasoning Models and make
    the topic approachable in an educational format. We move from the general concepts and ideas behind
    these models to the mathematical formulas that ground them, as well as the different frameworks that
    implement them and how they perform across benchmarks.
  </p>

  <p>
    Along the way, readers will gain a good overview of reasoning models, the different methods and
    structures they use, their practical applications, and how they perform across benchmarks. By the
    end, you should have a clearer picture of both the technical underpinnings and the open questions
    that drive ongoing research.
  </p>

  <p>
    <em>So, are these models really thinking, or are we just thinking that they are? Let‚Äôs find out!</em>
  </p>

<section style="margin-top:60px; margin-bottom:40px;">
  <h2 style="
    font-size:2em;
    font-weight:700;
    letter-spacing:0.01em;
    margin-bottom:6px;
    background:linear-gradient(90deg, #010610 70%, #eab308 100%);
    -webkit-background-clip:text;
    -webkit-text-fill-color:transparent;
    background-clip:text;
    text-align:center;
    ">
    Unlocking Reasoning: What Makes RLMs Different?
  </h2>
  <!-- Section content -->
</section>

<title>RLMs Image with Two Speech Bubbles (No False Pointer)</title>
  <style>
    section { background: #f8fafc; color: #222; font-family: sans-serif; }
    .centered { text-align: center; margin-top: 44px; }
    .robot-img { max-width: 380px; width: 100%; border-radius: 18px; box-shadow: 0 4px 24px #94a3b888; }
    .flexrow {
      display: flex; justify-content: center; align-items: flex-start; gap: 32px;
      margin: 0 auto 24px auto; max-width: 900px;
    }
    .bubble {
      background: #fffbe9;
      color: #222;
      max-width: 320px;
      min-width: 180px;
      padding: 18px 22px;
      border-radius: 18px;
      font-size: 1.13em;
      line-height: 1.52;
      box-shadow: 0 2px 18px #eab30855;
      text-align: left;
      position: relative;
      margin-top: 40px;
    }
    /* Only the right bubble gets left pointer */
    .bubble.right:before {
      content: '';
      position: absolute;
      left: -22px;
      top: 24px;
      width: 0; height: 0;
      border: 16px solid transparent;
      border-right: 22px solid #fffbe9;
      border-left: 0;
    }
    /* Only the left bubble gets right pointer */
    .bubble.left:after {
      content: '';
      position: absolute;
      right: -22px;
      top: 24px;
      width: 0; height: 0;
      border: 16px solid transparent;
      border-left: 22px solid #fffbe9;
      border-right: 0;
    }
    /* Summary box */
    .summary {
      max-width: 540px;
      font-size: 1.13em;
      line-height: 1.52;
      color: #333;
      background: #fff;
      padding: 18px 26px;
      border-radius: 14px;
      box-shadow: 0 2px 18px #e0e7ff55;
      text-align: left;
      margin: 0 auto;
      position: relative;
      z-index: 1;
    }
    .summary strong { color: #2563eb; }
    /* Layout below image: left bubble + summary */
    .flexrow-below {
      display: flex; justify-content: center; align-items: flex-start; gap: 32px;
      margin: 0 auto; max-width: 900px;
    }
    @media (max-width: 900px) {
      .flexrow, .flexrow-below { flex-direction: column; gap: 0; align-items: center; }
      .robot-img { max-width: 95vw;}
      .bubble { max-width: 95vw; margin-top: 18px;}
      .bubble.right:before { left: 22px; top: -16px; border-right: none; border-top: 22px solid #fffbe9; border-left: 16px solid transparent; border-bottom: 0;}
      .bubble.left:after { right: 22px; top: -16px; border-left: none; border-top: 22px solid #fffbe9; border-right: 16px solid transparent; border-bottom: 0;}
      .summary { max-width: 95vw; }
    }
  </style>


<section>
  <div class="flexrow">
    <div style="display:flex; flex-direction:column; align-items:center;">
      <img src="Bild4.png" alt="Robot reasoning vs lookup" class="robot-img">
      <div class="caption" style="font-size:0.9em; color:#555; margin-top:6px;">
        Image: Alex Shipps/MIT CSAIL
      </div>
    </div>
    <div class="bubble right">
      <strong>This is the key idea behind RLMs:</strong>
      <br>
      Moving beyond surface fluency to actual reasoning, where tasks are worked through step-by-step.
    </div>
  </div>
    <!-- Below: left bubble + summary -->
    <div class="flexrow-below" style="margin-top:18px;">
      <div class="summary">
        <strong>Large Language Models (LLMs)</strong> are built to tackle general-purpose tasks‚Äîanswering questions, solving problems, generating ideas, and following instructions. But many standard LLMs often fall short when it comes to true reasoning. Instead of logical analysis or planning, they tend to memorize and interpolate, skipping the deeper steps that lead to genuine understanding.
        <br><br>
        <strong>Reasoning Language Models (RLMs)</strong> represent a new direction: they introduce deliberate problem-solving strategies like step-by-step thinking, exploration, and self-verification. Unlike conventional LLMs, RLMs use search-based inference, generating multiple candidate solutions and choosing the best, rather than relying on a single one-pass answer.
      </div>
    </div>
  </div>
</section>








<title>Scaling Laws & AIME Chart Side-by-Side</title>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<style>
  section { font-family: sans-serif; background: #f8fafc; color: #111827;}
  .main-row {
    display: flex;
    align-items: flex-start;
    justify-content: center;
    gap: 56px;
    max-width: 1400px;
    margin: 48px auto 0 auto;
    flex-wrap: wrap;
  }
  .visual-box {
    background: #fff;
    border-radius: 18px;
    box-shadow: 0 4px 22px #94a3b822;
    padding: 28px 34px;
    flex: 1 1 420px;
    max-width: 630px;
    min-width: 320px;
    margin-bottom: 32px;
  }
  /* --- Vertical flow row --- */
  .flow-row {
    display: flex;
    flex-direction: column; /* vertical stack */
    align-items: center;
    justify-content: flex-start;
    gap: 36px;  /* spacing between boxes and arrows */
    margin: 38px 0;
  }
  .section-title {
    position: relative;
    display: inline-block;
    font-size: 1.15em;
    font-weight: bold;
    cursor: pointer;
    padding: 12px 22px;
    border-radius: 8px;
    transition: background 0.2s;
    z-index:2;
    white-space: nowrap;
    margin-bottom: 12px;
  }
  .section-title.scaling { background: #e0f2fe; color: #2563eb; }
  .section-title.wall { background: #fee2e2; color: #b91c1c; }
  .section-title.solution { background: #dcfce7; color: #15803d; }
  .arrow { font-size:2.3em; color: #94a3b8; }
  .popup {
    position: absolute;
    left: 50%; transform: translateX(-50%);
    top: 120%;
    background: #fffbe9;
    border: 1.5px solid #eab308;
    border-radius: 12px;
    padding: 18px 24px;
    min-width: 240px;
    font-size: 1em;
    color: #222;
    box-shadow: 0 6px 32px #eab30844;
    opacity: 0;
    pointer-events: none;
    transition: opacity 0.18s;
    z-index:50;
    text-align:left;
  }
  .section-title.active .popup,
  .section-title:hover .popup {
    opacity: 1;
    pointer-events: auto;
  }
  .caption {margin:16px 0 0 0; color:#444;}
  @media (max-width: 900px) {
    .main-row {
      flex-direction: column;
      gap: 0;
      align-items: center;
    }
    .visual-box { max-width: 98vw; padding: 18px 8vw;}
    .flow-row {
      gap: 44px; /* more vertical space on mobile */
    }
  }
</style>

<section>
<div class="main-row">
<!-- Left: Scaling Laws Flow -->
<div class="visual-box">
  <h3 style="text-align:center; margin-top:0;">How Scaling Laws Led to Test-Time Compute</h3>
  <div class="flow-row">
    <div class="section-title scaling" tabindex="0" onclick="toggleActive(this)">
      Era of Scaling Laws
      <div class="popup" style="text-align:center">
        Progress driven by scale:<br>Accuracy increases with model size, more data, and more training compute.
      </div>
    </div>
    <span class="arrow">&#8595;</span>
    <div class="section-title wall" tabindex="0" onclick="toggleActive(this)">
      Hitting a Wall

      
      <div class="popup" style="text-align:center">
        Scaling hits practical limits:<br>Eventually computational resources and data become scarce, accuracy gains plateau.
      </div>
    </div>
    <span class="arrow">&#8595;</span>
    <div class="section-title solution" tabindex="0" onclick="toggleActive(this)">
      Test-Time Compute
      <div class="popup" style="text-align:center">
        Solution:<br>By increasing compute during inference, performance on new data<br>can be boosted at test-time, so without retraining.
      </div>
    </div>
  </div>
</div>


<!-- Right: Chart.js plot -->
<div class="visual-box">
  <h3 style="text-align:center; margin-top:0;">Train-Time Compute vs Test-Time Compute</h3>
  <div style="max-width:600px; margin: 24px auto;">
    <canvas id="aimeChart" height="320"></canvas>
  </div>

</div>
</div>
<script>

// For accessibility: toggle tooltip on click (not just hover)
function toggleActive(el) {
  document.querySelectorAll('.section-title').forEach(s => {
    if (s !== el) s.classList.remove('active');
  });
  el.classList.toggle('active');
}
// Optional: close tooltip when clicking elsewhere
document.body.addEventListener('click', function(e){
  if (!e.target.classList.contains('section-title')) {
    document.querySelectorAll('.section-title').forEach(s => s.classList.remove('active'));
  }
});

// Chart.js setup
const trainCompute = [32, 42, 51, 52, 55, 58, 59, 70];
const testCompute = [20, 25, 38, 48, 59, 72, 75];
const computeBudget = [1,2,3,4,5,6,7,8];

new Chart(document.getElementById('aimeChart').getContext('2d'), {
  type: 'line',
  data: {
    labels: computeBudget,
    datasets: [
      {
        label: 'Train-Time Compute',
        data: trainCompute,
        borderColor: '#4e80c4',
        backgroundColor: 'transparent',
        borderWidth: 3,
        tension: 0.35,
        pointRadius: 5
      },
      {
        label: 'Test-Time Compute',
        data: testCompute,
        borderColor: '#f59e42',
        backgroundColor: 'transparent',
        borderWidth: 3,
        borderDash: [7,4],
        tension: 0.35,
        pointRadius: 5
      }
    ]
  },
  options: {
    responsive: true,
    plugins: {
      legend: { position: 'bottom'},
      title: { display: false }
    },
    scales: {
      y: {
        beginAtZero: true,
        title: { display:true, text:"AIME Accuracy (%)" },
        grid: {
      drawTicks: true,   // keep tick marks
      drawOnChartArea: false  // remove the horizontal grid lines
    }
      },
      x: {
        type: 'logarithmic',
        title: { display:true, text:"Compute budget (log scale)" },
        ticks: { display:false },
        grid: {
      drawTicks: true,   // keep tick marks
      drawOnChartArea: false  // remove the horizontal grid lines
    }
      }
    }
  }
});
</script>

</section>

<div style="max-width:900px; margin:2rem auto; font-size:1.05em; color:#333; text-align:left; line-height:1.5;">
    Scaling alone drove major gains over the past decade, but its limits are becoming clear. 
    Test-time compute offers a complementary approach: models can "think harder" when needed, 
    improving reasoning and robustness without retraining. This shift intends moving from 
    brute-force scaling toward more adaptable AI.
</div>







  <title>Direct vs Reasoning Model Flow</title>
  <style>
    section { background: #f8fafc; font-family: sans-serif; color: #111827; }
    .switcher { text-align: center; margin: 35px 0 14px 0; }
    button { font-size: 1em; padding: 10px 20px; margin: 0 8px; border-radius:6px; border:none; background:#e0e7ef; cursor:pointer; }
    .flow-wrap { display:flex; justify-content:center; align-items:center; gap:60px; }
    .label { text-align:center; font-weight:600; font-size:1.1em; margin-top:10px }
  </style>

<section>
  <h2 style="text-align:center;">Direct vs Reasoning Model</h2>
  <div class="switcher">
    <button onclick="showFlow('direct')">Direct Model</button>
    <button onclick="showFlow('reasoning')">Reasoning Model</button>
  </div>
  <div class="flow-wrap">
    <svg id="flowSVG" viewBox="0 0 750 180" width="750" height="180">
      <!-- Will be filled dynamically -->
    </svg>
  </div>
  <div id="flowLabel" class="label"></div>
<script>
function showFlow(mode) {
  const svg = document.getElementById('flowSVG');
  svg.innerHTML = ""; // Clear SVG
  if (mode === 'direct') {
    // Draw direct path
    svg.innerHTML = `
      <rect x="40" y="60" width="120" height="60" rx="18" fill="#bae6fd" />
      <rect x="590" y="60" width="120" height="60" rx="18" fill="#fde68a" />
      <text x="100" y="95" text-anchor="middle" fill="#222" font-size="20" font-weight="600">Input</text>
      <text x="650" y="95" text-anchor="middle" fill="#222" font-size="20" font-weight="600">Output</text>
      <path d="M160,90 Q375,40 590,90" stroke="#60a5fa" stroke-width="5" fill="none" marker-end="url(#arrow)" />
      <defs>
        <marker id="arrow" markerWidth="12" markerHeight="12" refX="8" refY="4" orient="auto" markerUnits="userSpaceOnUse">
          <path d="M0,0 L12,4 L0,8 Z" fill="#60a5fa"/>
        </marker>
      </defs>
    `;
    document.getElementById('flowLabel').textContent = "Direct mapping: Input leads to Output.";
  } else {
    // Reasoning model: input ‚Üí reasoning tokens ‚Üí output
    svg.innerHTML = `
      <rect x="40" y="60" width="120" height="60" rx="18" fill="#bae6fd" />
      <rect x="590" y="60" width="120" height="60" rx="18" fill="#fde68a" />
      <rect x="270" y="30" width="210" height="120" rx="30" fill="#fbbf24" opacity="0.18"/>
      <text x="100" y="95" text-anchor="middle" fill="#222" font-size="20" font-weight="600">Input</text>
      <text x="650" y="95" text-anchor="middle" fill="#222" font-size="20" font-weight="600">Output</text>
      <text x="375" y="55" text-anchor="middle" fill="#fb923c" font-size="18" font-weight="700">Intermediate Steps</text>
      <text x="375" y="95" text-anchor="middle" fill="#fb923c" font-size="18" font-weight="700">Tokens</text>
      <text x="375" y="125" text-anchor="middle" fill="red" font-size="18" font-weight="700">Reasoning</text>
      <path id="flowPath" d="M160,90 Q375,40 590,90" stroke="#fb923c" stroke-width="5" fill="none" marker-end="url(#arrow2)" />
      <defs>
        <marker id="arrow2" markerWidth="12" markerHeight="12" refX="8" refY="4" orient="auto" markerUnits="userSpaceOnUse">
          <path d="M0,0 L12,4 L0,8 Z" fill="#60a5fa"/>
        </marker>
      </defs>
    `;
    // Add moving tokens (circles) along the path
    let tokens = [];
    for (let i=0; i<5; ++i) {
      let c = document.createElementNS("http://www.w3.org/2000/svg","circle");
      c.setAttribute("r",12);
      c.setAttribute("fill",["#60a5fa","#fbbf24","#f59e42","#fb923c","#fde68a"][i]);
      c.setAttribute("id", `token${i}`);
      svg.appendChild(c);
      tokens.push(c);
    }
    function animateTokens() {
      const path = svg.getElementById ? svg.getElementById('flowPath') : document.getElementById('flowPath');
      if (!path) return;
      const total = path.getTotalLength();
      let start = performance.now();
      function frame(now) {
        let t = ((now-start)/1000)%1; // 0..1 loop
        tokens.forEach((c, i) => {
          let pct = (t + i*0.18)%1;
          let pt = path.getPointAtLength(pct*total);
          c.setAttribute("cx", pt.x);
          c.setAttribute("cy", pt.y);
        });
        requestAnimationFrame(frame);
      }
      frame(start);
    }
    setTimeout(animateTokens,150); // wait for DOM
    document.getElementById('flowLabel').textContent = "Reasoning model: Intermediate reasoning steps represented by reasoning tokens.";
  }
}
// Show direct by default
showFlow('direct');
</script>
</section>

<div style="max-width:900px; margin:2rem auto; font-size:1.05em; color:#333; text-align:left; line-height:1.5;">
  Instead of mapping inputs directly to outputs, reasoning models break the task into intermediate steps. 
  These reasoning tokens are intended to make the model‚Äôs internal process visible, helping it structure and refine 
  its reasoning. This approach seemingly improves accuracy on complex problems while keeping the model‚Äôs 
  outputs interpretable.
</div>


<div style="max-width:720px; margin:60px auto; text-align:center;">
  <details style="background:#f1f5f9; border-radius:12px; padding:20px; cursor:pointer; box-shadow:0 3px 12px rgba(0,0,0,0.05);">
    <summary style="font-size:1.4em; font-weight:600; color:#2563eb;">
      ü§î But what are these intermediate steps?
    </summary>
    <p style="margin-top:15px; font-size:1.1em; color:#374151;">
      And how are RLMs actually achieved?
      <br>
      To answer that, we‚Äôll look at reasoning architectures such as<br> 
      <b>search-based methods,<br>reward models,<br></b> and <b>decision-tree style reasoning</b>.
    </p>
  </details>
</div>



<!--CoT Explanation-->

<section style="margin:60px auto; max-width:1000px;">
  <h2 style="text-align:center;">Chain of Thought (CoT) Example</h2>
  <p style="text-align:justify; line-height:1.6;">
    One of the first attempts to elicit reasoning in LLMs was related to how to prompt them. 
    Perhpaps the most well known example of this is Chain of Thought (CoT). 
    In CoT, we improve the quality of the LLM's answers by prompting them to think their answer step by step.
    This simple tricked improved accuracy in a variety of problems and proved that the reasoning capabilities of LLMs might be dormant.
    While the techinique improves accuracy and generalizes well over any area, it still has its issues. 
    CoT is not a scalable technique, as it still relies on the knowledge already existing in the LM.

    Here you can compare how a model answers <em>with</em> and <em>without</em> Chain of Thought prompting, as explained in <a href="https://arxiv.org/abs/2205.11916" 
   target="_blank" 
   rel="noopener noreferrer" 
   title="Chain of Thought Prompting Elicits Reasoning in Large Language Models (Kojima et al., 2022)">
   Kojima et al., 2022
</a>.

  </p>

  <!-- Chat Window -->
  <div id="cot-chat" style="border:1px solid #e5e7eb; border-radius:12px; background:#f9fafb; padding:15px; height:340px; overflow-y:auto; font-family:sans-serif;">
    <p style="color:#6b7280; text-align:center; margin:0;">Start the conversation below üëá</p>
  </div>

  <!-- User options -->
  <div style="text-align:center; margin-top:15px;">
    <button onclick="askSimple()" style="padding:8px 14px; background:#2563eb; color:white; border:none; border-radius:6px; cursor:pointer; margin-right:10px;">
      Ask without CoT
    </button>
    <button onclick="askCoT()" style="padding:8px 14px; background:#16a34a; color:white; border:none; border-radius:6px; cursor:pointer;">
      Ask with CoT
    </button>
  </div>


<script>
const chatBox = document.getElementById("cot-chat");

// helper to add messages
function addMessage(text, sender="user"){
  const msg = document.createElement("div");
  msg.style.marginBottom = "10px";
  let bubbleStyle = sender === "user"
    ? "background:#3b82f6; color:white; text-align:right; float:right;"
    : "background:white; border:1px solid #d1d5db; text-align:left; float:left;";
  msg.innerHTML = `<div style="${bubbleStyle} display:inline-block; padding:8px 12px; border-radius:12px; max-width:75%;">${text}</div><div style="clear:both;"></div>`;
  chatBox.appendChild(msg);
  chatBox.scrollTop = chatBox.scrollHeight;
}

function askSimple(){
  chatBox.innerHTML = ""; // reset
  addMessage("How many 'r' are in raspberry?","user");
  setTimeout(()=> addMessage("There are 1 'r'.","bot"), 800);
}

function askCoT(){
  chatBox.innerHTML = ""; // reset
  addMessage("How many 'r' are in raspberry. Let's think step by step.","user");

  setTimeout(()=> addMessage("The word is 'raspberry'.","bot"), 800);
  setTimeout(()=> addMessage("Let's count: r (1), a, s, p, b, e, r (2), r (3), y.","bot"), 1600);
  setTimeout(()=> addMessage("So there are 3 'r's in 'raspberry'.","bot"), 2400);

  // New user follow-up
  setTimeout(()=> {
    addMessage(`The word is 'raspberry'.\nLet's count: r (1), a, s, p, b, e, r (2), r (3), y.\nSo there are 3 'r's in 'raspberry'.\nTherefore, the answer is:`,"user");
  }, 3400);

  // Final concise answer
  setTimeout(()=> addMessage("3 ‚úÖ","bot"), 4200);
}
  </script>
  <p>The first part is for creating the chain of thought itself, 
    while the second messages ensures a definitive, concise answer.
  </p>
</section>



  <!-- SFT Explanation Section -->
  <div style="max-width:1000px; margin:60px auto; text-align:center;"> 
      <h2 style="text-align:center;">Supervised Fine-Tuning</h2>

    <p style="font-size:16px; line-height:1.6; text-align:justify;">
      However useful CoT is, only being able retrieve the latent capabilities of the model is a hard limit of the technique.
      A reasoning model should be able to go beyond its training data, and actually think beyond its preobtained information.
      To remedy this, researchers started testing different techniques and algorithms to enhance the reasoning capabilities of LLMs.
      <strong>Supervised Fine-Tuning (SFT)</strong> is a post-training technique where we update 
      our model so its answers resamble a specific dataset. 
      This is useful for alignment, which means developing models that specialize 
      in a specific topic or must follow certain guidelines. 
      
      
      <br> <br>In more mathematical terms, given a set of prompts <m>x</m>, 
      and their corresponding preferred answers <m>y</m>, 
      we want our model responses given its current distributions to resamble <m>y</m>.
      This can be achieved by updating our parameters based on a loss formula. 
      
        <div id="sft-grad" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"></div>

          <script>
          (function ensureKatex(cb){
            if (window.katex) return cb();
            const l=document.createElement("link");
            l.rel="stylesheet"; 
            l.href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css";
            document.head.appendChild(l);
            const s=document.createElement("script");
            s.src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js";
            s.onload=cb;
            document.head.appendChild(s);
          })(function(){
            katex.render(String.raw`
              \nabla_\theta \, \mathcal{L}_{\text{SFT}}(\theta) 
              = - \mathbb{E}_{x,y} \Bigg[ \sum_{t=1}^{T} 
              \nabla_\theta \log p_\theta \big(y_t \mid x, y_{<t}\big) \Bigg]
            `, document.getElementById('sft-grad'), { displayMode:true });
          });
          </script>
      <p style="font-size:16px; line-height:1.6; text-align:justify;">
      We last update our parameters through gradient descent, minimazing the loss. And then our output will resamble more our preferred output.
      </p>

      <div class="highlight-box"> 
        By training model through SFT on CoT outputs, we can ensure that our LLMs will produce these reasoning steps autonomously, 
        without the need to prompt them to do so, enhancing their reasoning capabilities. </div>  

      <p style="font-size:16px; line-height:16.; text-align:justify;">
      If you want to see this technique in action, make sure to read the <a href="https://arxiv.org/abs/2501.19393" target="_blank" rel="noopener">S1 model paper</a>, 
      which shows the whole process of creating a RLM from scratch, 
      inlcuding data collection through CoT prompting, SFT as a post-training technique, 
      and Budget Forcing during inference (or controlling the lenght of the model's answers).
      </p>


    </div>



<div style="max-width:1000px; margin:20px auto; text-align:center;">
    <h2 style="margin-bottom: 30px; color: #1f2937;">Supervised Fine-Tuning (SFT) Process</h2>
    
    <svg id="sft-demo" width="100%" height="220" viewBox="0 0 900 220">
        <!-- Prompt -->
        <rect id="node-x" x="40" y="20" width="240" height="80" rx="14" fill="#bae6fd" stroke="#0ea5e9" stroke-width="1"/>
        <text x="160" y="45" text-anchor="middle" font-weight="600" fill="#0c4a6e">
            Prompt: 
            <tspan x="160" dy="1.2em">"How to reset my password?"</tspan>
        </text>

        <!-- Preferred Answer -->
        <rect x="320" y="10" width="520" height="60" rx="14" fill="#bbf7d0" stroke="#10b981" stroke-width="1"/>
        <text x="580" y="35" text-anchor="middle" font-weight="600" fill="#064e3b">Preferred: "Go to configurations ‚Üí reset password"</text>

        <!-- Model Output -->
        <rect id="node-yhat" x="320" y="120" width="520" height="60" rx="14" fill="#fb923c" stroke="#ea580c" stroke-width="1"/>
        <text id="output-text" x="580" y="155" text-anchor="middle" font-weight="600" fill="#9a3412">Model Output: "Throw water to your computer"</text>

        <!-- Arrow marker definition -->
        <defs>
            <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                <polygon points="0 0, 10 3.5, 0 7" fill="#ef4444"/>
            </marker>
        </defs>
    </svg>

    <!-- Loss Progress Bar -->
    <div style="margin-top:20px;">
        <input type="range" id="loss-slider" min="0" max="3" value="0" step="1" style="width:80%; height: 8px;">
        <div style="display:flex; justify-content:space-between; width:80%; margin:10px auto; font-size:14px; color:#374151;">
            <span>High Loss</span>
            <span>Update Weights</span>
            <span>Update Weights</span>
            <span>Zero Loss</span>
        </div>
    </div>

    <div style="margin-top: 30px; text-align: left; max-width: 800px; margin-left: auto; margin-right: auto;">
        <h3 style="color: #1f2937; margin-bottom: 15px;">How SFT Works:</h3>
        <p style="color: #4b5563; line-height: 1.6; margin-bottom: 10px;">
            <strong>Step 1:</strong> The model receives a prompt and generates an answer that may not be optimal.
        </p>
        <p style="color: #4b5563; line-height: 1.6; margin-bottom: 10px;">
            <strong>Step 2:</strong> The model's output is compared to the preferred/correct answer, creating a loss signal.
        </p>
        <p style="color: #4b5563; line-height: 1.6; margin-bottom: 10px;">
            <strong>Step 3:</strong> Through backpropagation, the model's weights are updated to reduce this loss.
        </p>
        <p style="color: #4b5563; line-height: 1.6;">
            <strong>Step 4:</strong> After many iterations, the model learns to produce outputs closer to the preferred answers.
        </p>
    </div>
</div>

<script>
const outputs = [
    'Model: "Throw water to your computer"',   // high loss
    'Model: "Click something in settings"',     // step 1
    'Model: "Go to settings and find password"',// step 2
    'Model: "Go to configurations ‚Üí reset password"' // perfect match
];
const colors = ["#fb923c","#fcd34d","#fef08a","#bbf7d0"];

document.getElementById("loss-slider").addEventListener("input", function(e){
    const val = parseInt(e.target.value);
    document.getElementById("output-text").textContent = outputs[val];
    document.getElementById("node-yhat").setAttribute("fill", colors[val]);

    // Show loss arrow when not fully trained
    document.getElementById("loss-arrow").style.opacity = (val < 3 ? 1 : 0);
});
</script>

<div class="wrap">
  <style>
  :root{
    /* colors */
    --bg:#f8fafc; --ink:#0f172a; --muted:#64748b;
    --arrow:#cbd5e1; --node:#bae6fd; --token:#3b82f6;
    --model:#7dd3fc; --rm:#fde68a; --human:#fecaca; --rect:#e2e8f0;
    /* spacing */
    --s0:0; --s1:.25rem; --s2:.5rem; --s3:.75rem; --s4:1rem;
    --s5:1.25rem; --s6:1.5rem; --s7:2rem; --s8:2.5rem;
    /* type scale */
    --fs-h1:22px; --fs-h2:1.35rem; --fs-h3:1.1rem;
    --fs-p:15px; --fs-sub:14px; --fs-lg:18px;
  }

  html,body{height:100%}
  body{margin:0;background:var(--bg);color:var(--ink);
    font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial}

  .wrap{max-width:1200px;margin-inline:auto;padding:var(--s6) var(--s3) var(--s6)}

  h1,h2,h3{line-height:1.25;font-weight:700}
  h1{font-size:var(--fs-h1);margin-block:var(--s6) var(--s2);text-align:center}
  h2{font-size:var(--fs-h2);margin-block:var(--s5) var(--s3)}
  h3{font-size:var(--fs-h3);margin-block:var(--s5) var(--s3)}
  p{font-size:var(--fs-p);line-height:1.6;margin-block:var(--s3)}
  .p-lg{font-size:var(--fs-lg)}
  .sub{margin-block:var(--s1) var(--s4);text-align:center;color:var(--muted);font-size:var(--fs-sub)}
  .mr{margin-right:var(--s6)}

  .eq{margin-block:var(--s3)}
  .stat{margin-block:var(--s2);font-size:var(--fs-p)}
  code.inline{background:#f8fafc;border:1px solid #e5e7eb;padding:.1rem .4rem;border-radius:6px}

  .ctrl{display:grid;grid-template-columns:1fr 1fr;gap:var(--s3) var(--s4);align-items:center;margin-block:var(--s4)}
  .ctrl label{color:var(--muted);font-size:var(--fs-p)}
  .ctrl input[type="range"]{width:100%}
  @media (max-width: 820px){.ctrl{grid-template-columns:1fr}}

  /* SVG & shared node/edge styles */
  svg{display:block;width:100%;height:auto}
  svg text{fill:var(--ink)}
  .node{filter:drop-shadow(0 2px 4px rgba(0,0,0,.08))}
  .edge{stroke:var(--arrow);stroke-width:3;fill:none;marker-end:url(#arrowhead)}
  .rect-node{fill:var(--rect);stroke:var(--ink);stroke-width:2}
  .model-node{fill:var(--model)}
  .rm-node{fill:var(--rm)}
  .value-node{fill:var(--rm)} /* value shares color with rm for simplicity */
  .human-node{fill:var(--human)}
  .token{fill:var(--token);r:6;filter:drop-shadow(0 1px 2px rgba(0,0,0,.35))}


  /* KaTeX tweaks */
  .katex{font-size:95%}
  .katex.display{margin-block:var(--s2)}
</style>

  <h2>Reinforcement Learning</h2>
<p>
  <strong>Reinforcement learning</strong> is a way to train a system by trial and reward.
  An agent tries actions, receives a numeric reward, and gradually learns a rule for choosing actions that
  lead to higher reward. That rule is called the <em>policy</em>.
</p>

<p>
  In fine tuning for language models, we apply this idea to align outputs with human preferences.
  A separate <strong>reward model</strong> turns human judgments into scores, so we train the reward model first.
</p>

<p>
  In the classic setup known as <strong>Reinforcement Learning from Human Feedback</strong>,
  people compare two answers to the same prompt, the reward model learns to score those answers,
  and the policy learns to pick the ones that score higher.
  The diagram below shows how feedback trains the reward model and how that model guides the policy during fine tuning.
</p>
</div>

<title>RLHF Flow Diagram</title>
</head>



<body>
  <div class="wrap" id="rlhf-flow">
    <h1>Reward Model Training Flow</h1>
  
    <style>
      #rlhf-flow .edge{ stroke:#cbd5e1; stroke-width:3; fill:none; marker-end:url(#grpoArrow); }
      #rlhf-flow .back{ stroke:#64748b; stroke-width:3; fill:none; stroke-dasharray:7 5; marker-end:url(#grpoArrow); }
      #rlhf-flow .node{ filter:drop-shadow(0 2px 4px rgba(0,0,0,.08)); }
      #rlhf-flow .rect-node{ fill:#e2e8f0; stroke:#0f172a; stroke-width:2; }
      #rlhf-flow .model-node{ fill:#7dd3fc; stroke:#0f172a; stroke-width:2; }
      #rlhf-flow .value-node{ fill:#fde68a; stroke:#0f172a; stroke-width:2; }
      #rlhf-flow .small{ font-size:12px; fill:#334155; }
    </style>


    <svg id="flowSvg">

      <rect x="20" y="120" width="60" height="40" class="node rect-node" rx="5"/>
      <text x="50" y="145" text-anchor="middle" font-size="16" font-weight="bold">X</text>

      <ellipse cx="180" cy="140" rx="60" ry="30" class="node model-node"/>
      <text x="180" y="145" text-anchor="middle" font-size="16" font-weight="bold">Model</text>

      <rect x="300" y="120" width="120" height="40" class="node rect-node" rx="5"/>
      <text x="360" y="145" text-anchor="middle" font-size="14" font-weight="bold">continuations</text>

      <ellipse cx="520" cy="140" rx="50" ry="30" class="node rm-node"/>
      <text x="520" y="145" text-anchor="middle" font-size="16" font-weight="bold">RM</text>

      <rect x="630" y="120" width="80" height="40" class="node rect-node" rx="5"/>
      <text x="670" y="145" text-anchor="middle" font-size="14" font-weight="bold">reward</text>

      <rect x="780" y="120" width="60" height="40" class="node rect-node" rx="5"/>
      <text x="810" y="145" text-anchor="middle" font-size="14" font-weight="bold">loss</text>

      <ellipse cx="360" cy="260" rx="70" ry="35" class="node human-node"/>
      <text x="360" y="255" text-anchor="middle" font-size="14" font-weight="bold">Human</text>
      <text x="360" y="270" text-anchor="middle" font-size="14" font-weight="bold">Labeler</text>

      <path id="edge_x_model" class="edge" d="M 80 140 H 120" />

      <path id="edge_x_cont" class="edge" d="M 80 140 V 80 H 360 V 120" />

      <path id="edge_x_human" class="edge" d="M 80 140 V 260 H 290" />

      <path id="edge_model_cont" class="edge" d="M 240 140 H 300" />

      <path id="edge_cont_rm" class="edge" d="M 420 140 H 470" />

      <path id="edge_rm_reward" class="edge" d="M 570 140 H 630" />

      <path id="edge_reward_loss" class="edge" d="M 710 140 H 780" />

      <path id="edge_cont_human" class="edge" d="M 360 160 V 225" />

      <path id="edge_human_loss" class="edge" d="M 430 260 H 780 V 140" />

      <path id="edge_loss_rm" class="edge" d="M 840 140 H 900 V 80 H 520 V 110" />

      <circle class="token" id="tok1"/>
      <circle class="token" id="tok2"/>
      <circle class="token" id="tok3"/>
      <circle class="token" id="tok_loss_rm" opacity="0"/>

      <animateMotion id="a1_1" href="#tok1" dur="0.9s" begin="indefinite" fill="freeze"><mpath href="#edge_x_model"/></animateMotion>
      <animateMotion id="a1_2" href="#tok1" dur="0.8s" begin="a1_1.end+0.05s" fill="freeze"><mpath href="#edge_model_cont"/></animateMotion>
      <animateMotion id="a1_3" href="#tok1" dur="0.8s" begin="a1_2.end+0.05s" fill="freeze"><mpath href="#edge_cont_rm"/></animateMotion>
      <animateMotion id="a1_4" href="#tok1" dur="0.9s" begin="a1_3.end+0.05s" fill="freeze"><mpath href="#edge_rm_reward"/></animateMotion>
      <animateMotion id="a1_5" href="#tok1" dur="0.9s" begin="a1_4.end+0.05s" fill="freeze"><mpath href="#edge_reward_loss"/></animateMotion>

      <animateMotion id="a2_1" href="#tok2" dur="1.0s" begin="indefinite" fill="freeze"><mpath href="#edge_x_cont"/></animateMotion>
      <animateMotion id="a2_2" href="#tok2" dur="0.8s" begin="a2_1.end+0.05s" fill="freeze"><mpath href="#edge_cont_human"/></animateMotion>
      <animateMotion id="a2_3" href="#tok2" dur="1.0s" begin="indefinite" fill="freeze"><mpath href="#edge_human_loss"/></animateMotion>

      <animateMotion id="a3_1" href="#tok3" dur="1.2s" begin="indefinite" fill="freeze"><mpath href="#edge_x_human"/></animateMotion>
      <animateMotion id="a3_2" href="#tok3" dur="1.0s" begin="indefinite" fill="freeze"><mpath href="#edge_human_loss"/></animateMotion>

      <animateMotion id="a_lr_1" href="#tok_loss_rm" dur="1.2s" begin="indefinite" fill="freeze"><mpath href="#edge_loss_rm"/></animateMotion>
      <animateMotion id="a_lr_2" href="#tok_loss_rm" dur="0.9s" begin="a_lr_1.end+0.05s" fill="freeze"><mpath href="#edge_rm_reward"/></animateMotion>
      <animateMotion id="a_lr_3" href="#tok_loss_rm" dur="0.9s" begin="a_lr_2.end+0.05s" fill="freeze"><mpath href="#edge_reward_loss"/></animateMotion>
    </svg>
  </div>

  <script>
    const svg = document.getElementById('flowSvg');
    const wrapper = document.getElementById('rlhf-flow');

    const a1_first = document.getElementById('a1_1');
    const a2_first = document.getElementById('a2_1');
    const a3_first = document.getElementById('a3_1');

    const a1_last = document.getElementById('a1_5');
    const a2_last = document.getElementById('a2_3');
    const a3_last = document.getElementById('a3_2');

    const a_lr_1 = document.getElementById('a_lr_1');
    const a_lr_2 = document.getElementById('a_lr_2');
    const a_lr_3 = document.getElementById('a_lr_3');

    const a2_toHuman = document.getElementById('a2_2'); 
    const a3_toHuman = document.getElementById('a3_1'); 
    const a2_fromHuman = document.getElementById('a2_3'); 
    const a3_fromHuman = document.getElementById('a3_2'); 

    const tok1 = document.getElementById('tok1');
    const tok2 = document.getElementById('tok2');
    const tok3 = document.getElementById('tok3');
    const tokLRM = document.getElementById('tok_loss_rm');

    let arrivedCount = 0;  
    let humanArrivals = 0;  
    let running = false;
    let visible = false;

    function resetTokens(){
      [tok1,tok2,tok3,tokLRM].forEach(t=>{
        t.setAttribute('r','0');
        t.setAttribute('opacity', t === tokLRM ? '0' : '1');
      });
      requestAnimationFrame(()=>[tok1,tok2,tok3].forEach(t=>t.setAttribute('r','6')));
    }

    function startCycle(){
      if (running) return;
      running = true;
      arrivedCount = 0;
      humanArrivals = 0;
      resetTokens();
      a1_first.beginElement();
      a2_first.beginElement();
      a3_first.beginElement();
    }

    function beginLossChain(){
      [tok1,tok2,tok3].forEach(t=>t.setAttribute('opacity','0'));
      tokLRM.setAttribute('opacity','1');
      tokLRM.setAttribute('r','6');
      a_lr_1.beginElement();
    }

    function maybeFire(){
      if (arrivedCount === 3){
        beginLossChain();
      }
    }

    function tryLeaveHuman(){
      if (humanArrivals === 2){
        a2_fromHuman.beginElement();
        a3_fromHuman.beginElement();
      }
    }

    a2_toHuman.addEventListener('endEvent', ()=>{ humanArrivals++; tryLeaveHuman(); });
    a3_toHuman.addEventListener('endEvent', ()=>{ humanArrivals++; tryLeaveHuman(); });

    a1_last.addEventListener('endEvent', ()=>{ arrivedCount++; maybeFire(); });
    a2_last.addEventListener('endEvent', ()=>{ arrivedCount++; maybeFire(); });
    a3_last.addEventListener('endEvent', ()=>{ arrivedCount++; maybeFire(); });

    a_lr_3.addEventListener('endEvent', ()=>{
      setTimeout(()=>{
        running = false;
        tokLRM.setAttribute('opacity','0');
        if (visible) startCycle();
      }, 350);
    });

    const io = new IntersectionObserver((entries)=>{
      entries.forEach(entry=>{
        if(entry.isIntersecting){
          visible = true;
          try { svg.unpauseAnimations(); } catch(e){}
          startCycle();
        }else{
          visible = false;
          try { svg.pauseAnimations(); } catch(e){}
        }
      });
    }, {threshold: 0.25});
    io.observe(wrapper);

    window.addEventListener('load', ()=>{
      const r = wrapper.getBoundingClientRect();
      if (r.top < window.innerHeight && r.bottom > 0){
        visible = true;
        try { svg.unpauseAnimations(); } catch(e){}
        startCycle();
      }
    });
  </script>

<div class="wrap">
  <h2 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Reward Model Training Data</h2>

  <p>
    Training a reward model requires pairs of answers that can be compared. Given the same prompt, annotators decide which answer
    is better. The chosen answer becomes the <strong>preferred output</strong>, while the less helpful one is marked as not preferred.
  </p>

  <svg viewBox="0 0 1400 1100" width="100%" height="820" xmlns="http://www.w3.org/2000/svg">
    <defs>
      <marker id="exArrow" markerWidth="10" markerHeight="10" refX="5" refY="5" orient="auto">
        <path d="M0,0 L10,5 L0,10 Z" fill="#94a3b8"/>
      </marker>
      <filter id="exShadow" x="-50%" y="-50%" width="200%" height="200%">
        <feDropShadow dx="0" dy="1.5" stdDeviation="1.8" flood-opacity="0.25"/>
      </filter>
      <style>
        .ex-body{
          font: 28px/1.6 ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial;
          color:#111827; overflow-wrap:break-word; word-break:normal;
        }
        .ex-card{ fill:#eaf2ff; stroke:#94a3b8; rx:12; }
        .ex-badge{ fill:#93c5fd; stroke:#334155; }
        .ex-badge text{ fill:#0f172a; font-weight:800; font-size:26px; }
        .ex-edge{ stroke:#94a3b8; stroke-width:3; fill:none; marker-end:url(#exArrow); }
        .ex-y1 strong{
          text-decoration: underline;
          text-decoration-thickness: 4px;
          text-underline-offset: 3px;
          text-decoration-color: #22c55e;
          font-weight: 800;
        }
        .ex-y2 strong{
          text-decoration: underline;
          text-decoration-thickness: 4px;
          text-underline-offset: 3px;
          text-decoration-color: #ef4444;
          font-weight: 800;
        }
      </style>
    </defs>

    <rect x="460" y="70" width="480" height="90" class="ex-card" filter="url(#exShadow)"/>
    <text x="700" y="122" text-anchor="middle" font-size="28" font-weight="700" fill="#0f172a">
      How do I reset my phone password?
    </text>

    <g transform="translate(700,230)">
      <circle r="44" fill="#93c5fd" stroke="#334155" filter="url(#exShadow)"/>
      <text y="8" text-anchor="middle" font-size="42" font-weight="800" fill="#0f172a">X</text>
    </g>

    <rect id="ex-y1Card" x="140" y="360" width="560" height="520" class="ex-card" filter="url(#exShadow)"/>
    <foreignObject x="155" y="375" width="530" height="490">
      <div class="ex-body ex-y1">
        To reset your phone password, the <strong>first step</strong> is to go to the settings menu of your phone.
        <strong>Once in settings</strong>, locate the security or ‚Äúlock screen‚Äù settings.
        <strong>Here, you will be able to reset your password</strong> and create a new one.
      </div>
    </foreignObject>

    <rect id="ex-y2Card" x="760" y="360" width="560" height="520" class="ex-card" filter="url(#exShadow)"/>
    <foreignObject x="775" y="375" width="530" height="490">
      <div class="ex-body ex-y2">
        You use the password manager to reset your phone password. To reset your phone password, you need:
        <ul style="margin:.6rem 0 0 1.4rem; padding:0;">
          <li><strong>your phone password</strong></li>
          <li><strong>a password manager app</strong></li>
          <li>
            <strong>a password manager app in your phone</strong>, either on your phone or in a trusted mobile app store
            like iCloud, Facebook Messenger, or Phone Passcode
          </li>
        </ul>
      </div>
    </foreignObject>

    <path class="ex-edge" d="M 700 274 V 300 H 420 V 360" />
    <path class="ex-edge" d="M 700 274 V 300 H 1040 V 360" />

    <g transform="translate(180,905)">
      <rect x="-40" y="-26" width="180" height="96" rx="28" class="ex-badge" filter="url(#exShadow)"/>
      <text x="45" y="32" font-size="42" text-anchor="middle" font-weight="800" fill="#0f172a">y1</text>
    </g>
    <g transform="translate(1280,905)">
      <rect x="-40" y="-26" width="180" height="96" rx="28" class="ex-badge" filter="url(#exShadow)"/>
      <text x="45" y="32" font-size="42" text-anchor="middle" font-weight="800" fill="#0f172a">y2</text>
    </g>

    <g transform="translate(700,1000)">
      <g transform="translate(-120,0)">
        <circle r="22" fill="#22c55e"/>
        <path d="M -7 0 L -2 6 L 11 -8" stroke="#fff" stroke-width="4" fill="none" stroke-linecap="round" stroke-linejoin="round"/>
        <text x="36" y="9" font-size="32" font-weight="800" fill="#0f172a">y1</text>
      </g>
      <text x="0" y="10" text-anchor="middle" font-size="42" font-weight="800" fill="#0f172a">‚âª</text>
      <g transform="translate(120,0)">
        <circle r="22" fill="#f43f5e" opacity=".95"/>
        <path d="M -8 -8 L 8 8 M -8 8 L 8 -8" stroke="#fff" stroke-width="4" stroke-linecap="round"/>
        <text x="36" y="9" font-size="42" font-weight="800" fill="#0f172a">y2</text>
      </g>
    </g>
  </svg>

  <p style="margin-top:.75rem; font-size:18px;">
    In this example database in
    <a href="https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise/viewer/default/train?f%5Bchosen%5D%5Bmin%5D=1&f%5Bchosen%5D%5Bmax%5D=311&f%5Bchosen%5D%5Btransform%5D=length&row=15&views%5B%5D=train" target="_blank" rel="noopener">HuggingFace</a>
    the annotators preferred answer <strong>y1</strong>.
  </p>

  <p>
  In this example, the prompt <em>"How do I reset my phone password?"</em> produces two candidate answers. Annotators judged that
  <strong>y1</strong> was a better response. When analysing both answers we can see that in answer <strong>y1</strong>, underlined in green, there are a series of steps on how to reset a phone password.
  The generated answer <strong>y2</strong> lists items that might be required but does not explain how to do the task, as we can see underlined in red.
</p>
</div>



<style>
  .loss2-sec{--ink:#0f172a;--muted:#64748b;--line:#e5e7eb}
  .loss2-sec h2{margin:1.25rem 0 .6rem 0; font-size:1.35rem; font-weight:700}
  .loss2-sec .sub{color:var(--muted); font-size:15px; margin:.25rem 0 .9rem}
  .loss2-sec .eq{margin:.6rem 0 .35rem}
  .loss2-sec .katex{font-size:95%}
  .loss2-sec .katex.display{margin:.35rem 0}
  .loss2-sec p{font-size:15px; line-height:1.6; color:var(--ink); margin:.4rem 0}
  .loss2-sec .two-col{display:grid; grid-template-columns:1fr 1fr; gap:1rem; margin:.6rem 0}
  .loss2-sec .ans{font-size:15px; line-height:1.55}
  .loss2-sec .ans .tag{font-weight:700; margin-right:.25rem}
  .loss2-sec .ctrl{display:grid; grid-template-columns:1fr 1fr; gap:.75rem 1rem; align-items:center; margin:.6rem 0}
  .loss2-sec label{color:var(--muted); font-size:15px}
  .loss2-sec input[type="range"]{width:100%}
  .loss2-sec .stat{margin:.35rem 0; font-size:15px}
  .loss2-sec code.inline{background:#f8fafc; border:1px solid var(--line); padding:.1rem .4rem; border-radius:6px}
  @media (max-width: 820px){
    .loss2-sec .two-col{grid-template-columns:1fr}
    .loss2-sec .ctrl{grid-template-columns:1fr}
  }
</style>

<div class="wrap loss2-sec" id="pairwise-loss-v2">
  <h2>Pairwise loss for the reward model</h2>

  <div class="eq" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"><span id="loss2-eq"></span></div>

  <p>
    This is the pairwise loss we use to train the reward model. Start by taking the reward score for the
    currently preferred answer \(r(x,y_b)\) and exponentiating it. Using the exponential turns arbitrary
    scores into positive quantities and lets the model express how much better one answer is than another.
  </p>
  <p>
    Next, normalize by the sum of exponentials for both candidates \(y_1\) and \(y_2\). That normalization
    forms a two way softmax and yields a probability that the preferred answer is indeed the better one under
    the current reward model.
  </p>
  <p>
    We then take the negative log of that probability. This is the standard maximum likelihood objective:
    it is numerically stable, converts products into sums over a dataset, and penalizes confident mistakes
    more than small ones. During training we minimize the average of this loss across many labeled comparisons,
    which is why the expectation is implied.
  </p>

  <h3>Let us look at a numeric example.</h3>
  <p>
    Considering the same prompt <em>‚ÄúHow do I reset my phone password?‚Äù</em> and answers <strong>y1</strong>
    and <strong>y2</strong> as in the example above, you can play with the values that would have been assigned
    to each answer by the model and choose your preferred answer in order to better understand the effects
    these values have on the learning process of the reward model.
  </p>

  <div class="ctrl">
    <label for="r1b">Reward \(r(x,y_1)\): <strong id="r1bv">3.2</strong></label>
    <input id="r1b" type="range" min="-2" max="6" step="0.1" value="3.2"/>
    <label for="r2b">Reward \(r(x,y_2)\): <strong id="r2bv">1.1</strong></label>
    <input id="r2b" type="range" min="-2" max="6" step="0.1" value="1.1"/>
    <label>Preferred \(b\):</label>
    <div>
      <label style="margin-right:.9rem"><input type="radio" name="pref2" value="y1" checked/> \(y_1\)</label>
      <label><input type="radio" name="pref2" value="y2"/> \(y_2\)</label>
    </div>
  </div>

  <p class="stat">
    With these settings, the model assigns a probability of <code class="inline" id="pout2">0.8910</code> to the preferred answer,
    which gives a loss of \( -\log p \) equal to <code class="inline" id="lout2">0.0501</code>.
  </p>

  <div class="eq"><span id="loss2-work"></span></div>

  <p>
    When the two rewards are close, the softmax sits near one half and the loss stays comparatively large, indicating that the model is not very sure.
    As the preferred reward grows much larger than the alternative, the probability approaches one, the loss drops toward zero, and the
    reward model is saying I am confident this answer is better. If the non preferred answer receives the higher reward, the probability
    for the preferred answer becomes small and the loss grows, pushing the model to revise the scores on future updates.
  </p>
</div>

<!-- ================= PPO GRAPH ================= -->
<div class="wrap" id="ppo-flow">
  <h1>Proximal Policy Optimization (PPO)</h1>

  <style>
    #ppo-flow .edge{ stroke:#cbd5e1; stroke-width:3; fill:none; marker-end:url(#grpoArrow); }
    #ppo-flow .back{ stroke:#64748b; stroke-width:3; fill:none; stroke-dasharray:7 5; marker-end:url(#grpoArrow); }
    #ppo-flow .node{ filter:drop-shadow(0 2px 4px rgba(0,0,0,.08)); }
    #ppo-flow .rect-node{ fill:#e2e8f0; stroke:#0f172a; stroke-width:2; }
    #ppo-flow .model-node{ fill:#7dd3fc; stroke:#0f172a; stroke-width:2; }
    #ppo-flow .value-node{ fill:#fde68a; stroke:#0f172a; stroke-width:2; }
    #ppo-flow .small{ font-size:12px; fill:#334155; }
  </style>

  <svg id="ppoSvg" height="100%" width="80%">

    <rect x="20" y="170" width="50" height="36" rx="6" class="node rect-node"/>
    <text x="45" y="193" text-anchor="middle" font-size="14" font-weight="700">q</text>

    <ellipse cx="195" cy="188" rx="75" ry="34" class="node value-node"/>
    <text x="195" y="193" text-anchor="middle" font-size="14" font-weight="700">Policy Model</text>

    <rect x="330" y="170" width="60" height="36" rx="6" class="node rect-node"/>
    <text x="360" y="193" text-anchor="middle" font-size="14" font-weight="700">o</text>

    <ellipse cx="500" cy="115" rx="80" ry="30" class="node model-node"/>
    <text x="500" y="120" text-anchor="middle" font-size="13" font-weight="700">Reference Model</text>

    <ellipse cx="500" cy="188" rx="80" ry="30" class="node model-node"/>
    <text x="500" y="193" text-anchor="middle" font-size="14" font-weight="700">Reward Model</text>

    <ellipse cx="500" cy="265" rx="80" ry="30" class="node value-node"/>
    <text x="500" y="270" text-anchor="middle" font-size="14" font-weight="700">Value Model</text>


    <text x="640" y="192" text-anchor="middle" font-weight="700" font-size="16">‚äï</text>
    <text x="640" y="170" text-anchor="middle" font-weight="700" style="fill:#64748b;font-size:13px">KL</text>


    <rect x="700" y="170" width="60" height="36" rx="6" class="node rect-node"/>
    <text x="730" y="193" text-anchor="middle" font-size="14" font-weight="700">r</text>

    <rect x="700" y="248" width="60" height="36" rx="6" class="node rect-node"/>
    <text x="730" y="271" text-anchor="middle" font-size="14" font-weight="700">v</text>

    <rect x="800" y="206" width="80" height="40" rx="6" class="node rect-node"/>
    <text x="840" y="231" text-anchor="middle" font-size="14" font-weight="700">GAE</text>

    <rect x="940" y="206" width="42" height="40" rx="6" class="node rect-node"/>
    <text x="962" y="231" text-anchor="middle" font-size="14" font-weight="700">A</text>

    <path id="ppo_e_q_pol"   class="edge" d="M 70 188 H 120" />
    <path id="ppo_e_pol_o"   class="edge" d="M 270 188 H 330" />

    <path id="ppo_e_o_ref"   class="edge" d="M 390 188 V 115 H 420" />
    <path id="ppo_e_o_rm"    class="edge" d="M 390 188 H 420" />
    <path id="ppo_e_o_val"   class="edge" d="M 390 188 V 265 H 420" />

    <path id="ppo_e_ref_join" class="edge" d="M 580 115 V 170 H 628" />
    <path id="ppo_e_rm_join"  class="edge" d="M 580 188 H 628" />

    <path id="ppo_e_join_r"  class="edge" d="M 652 188 H 700" />
    <path id="ppo_e_val_v"   class="edge" d="M 580 265 H 700" />

    <path id="ppo_e_r_gae"   class="edge" d="M 760 188 V 206 H 800" />
    <path id="ppo_e_v_gae"   class="edge" d="M 760 266 V 246 H 800" />

    <path id="ppo_e_gae_a"   class="edge" d="M 880 226 H 940" />
    <path id="ppo_e_gae_val" class="edge" d="M 840 246 V 315 H 500 V 295" />

    <path id="ppo_e_a_pol"   class="edge" d="M 972 206 V 30 H 195 V 154" />

    <circle id="ppo_t_main" class="token" opacity="0"/>
    <circle id="ppo_t_ref"  class="token" opacity="0"/>   
    <circle id="ppo_t_rm"   class="token" opacity="0"/>   
    <circle id="ppo_t_val"  class="token" opacity="0"/>   

    <animateMotion id="ppo_m_main1" href="#ppo_t_main" dur="0.9s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_q_pol"/></animateMotion>
    <animateMotion id="ppo_m_main2" href="#ppo_t_main" dur="0.8s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_pol_o"/></animateMotion>

    <animateMotion id="ppo_m_ref1" href="#ppo_t_ref" dur="0.9s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_o_ref"/></animateMotion>
    <animateMotion id="ppo_m_ref2" href="#ppo_t_ref" dur="0.8s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_ref_join"/></animateMotion>

    <animateMotion id="ppo_m_rm1"   href="#ppo_t_rm" dur="0.8s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_o_rm"/></animateMotion>
    <animateMotion id="ppo_m_rm2"   href="#ppo_t_rm" dur="0.6s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_rm_join"/></animateMotion>
    <animateMotion id="ppo_m_rm3"   href="#ppo_t_rm" dur="0.6s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_join_r"/></animateMotion>
    <animateMotion id="ppo_m_rm4"   href="#ppo_t_rm" dur="0.7s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_r_gae"/></animateMotion>
    <animateMotion id="ppo_m_rm5"   href="#ppo_t_rm" dur="0.7s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_gae_a"/></animateMotion>
    <animateMotion id="ppo_m_rm6"   href="#ppo_t_rm" dur="0.9s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_a_pol"/></animateMotion>

    <animateMotion id="ppo_m_val1"  href="#ppo_t_val" dur="0.9s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_o_val"/></animateMotion>
    <animateMotion id="ppo_m_val2"  href="#ppo_t_val" dur="0.7s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_val_v"/></animateMotion>
    <animateMotion id="ppo_m_val3"  href="#ppo_t_val" dur="0.7s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_v_gae"/></animateMotion>
    <animateMotion id="ppo_m_val4"  href="#ppo_t_val" dur="0.9s" begin="indefinite" fill="freeze"><mpath href="#ppo_e_gae_val"/></animateMotion>
  </svg>
</div>

<script>
(function(){
  const svg  = document.getElementById('ppoSvg');
  const wrap = document.getElementById('ppo-flow');

  const m = id => document.getElementById(id);
  const el = id => document.getElementById(id);

  const t_main = el('ppo_t_main');
  const t_ref  = el('ppo_t_ref');
  const t_rm   = el('ppo_t_rm');
  const t_val  = el('ppo_t_val');

  const mm1=m('ppo_m_main1'), mm2=m('ppo_m_main2');

  const r1=m('ppo_m_ref1'),  r2=m('ppo_m_ref2');
  const rm1=m('ppo_m_rm1'), rm2=m('ppo_m_rm2'), rm3=m('ppo_m_rm3'), rm4=m('ppo_m_rm4'), rm5=m('ppo_m_rm5'), rm6=m('ppo_m_rm6');
  const v1=m('ppo_m_val1'), v2=m('ppo_m_val2'), v3=m('ppo_m_val3'), v4=m('ppo_m_val4');

  let running=false, visible=false, atJoin=0, atGAE=0;

  function reset(){
    [t_main,t_ref,t_rm,t_val].forEach(x=>x.setAttribute('opacity','0'));
    atJoin=0; atGAE=0;
  }

  function keepVisible(token, firstAnim, lastAnim){
    firstAnim.addEventListener('beginEvent', ()=> token.setAttribute('opacity','1'));
    lastAnim .addEventListener('endEvent',   ()=> token.setAttribute('opacity','0'));
  }
  keepVisible(t_main, mm1, mm2);
  keepVisible(t_ref,  r1,  r2);
  keepVisible(t_rm,   rm1, rm6);
  keepVisible(t_val,  v1,  v4);

  function start(){
    if(running) return;
    running=true;
    reset();
    mm1.beginElement();                    
  }

  mm1.addEventListener('endEvent', ()=> mm2.beginElement());            

  mm2.addEventListener('endEvent', ()=>{                                 
    r1.beginElement(); rm1.beginElement(); v1.beginElement();
  });

  r1.addEventListener('endEvent', ()=> r2.beginElement());
  r2.addEventListener('endEvent', ()=> { atJoin++; if(atJoin===2) rm3.beginElement(); });

  rm1.addEventListener('endEvent', ()=> rm2.beginElement());
  rm2.addEventListener('endEvent', ()=> { atJoin++; if(atJoin===2) rm3.beginElement(); });

  rm3.addEventListener('endEvent', ()=> rm4.beginElement());

  v1.addEventListener('endEvent', ()=> v2.beginElement());
  v2.addEventListener('endEvent', ()=> v3.beginElement());

  function hitGAE(){
    if(++atGAE===2){
      rm5.beginElement();        
      v4.beginElement();         
    }
  }
  rm4.addEventListener('endEvent', hitGAE);
  v3 .addEventListener('endEvent', hitGAE);

  rm5.addEventListener('endEvent', ()=> rm6.beginElement());

  rm6.addEventListener('endEvent', ()=>{
    running=false;
    if(visible) setTimeout(start, 700);
  });

  const io = new IntersectionObserver((entries)=>{
    entries.forEach(e=>{
      if(e.isIntersecting){
        visible=true;
        try{ svg.unpauseAnimations(); }catch(_){}
        start();
      }else{
        visible=false;
        try{ svg.pauseAnimations(); }catch(_){}
      }
    });
  }, {threshold:0.25});
  io.observe(wrap);

  window.addEventListener('load', ()=>{
    const r = wrap.getBoundingClientRect();
    if (r.top < innerHeight && r.bottom > 0){
      visible=true;
      try{ svg.unpauseAnimations(); }catch(_){}
      start();
    }
  });
})();
</script>

 

<!-- ========== VALUE MODEL ========== -->
<div class="wrap loss2-sec" id="value-loss-v2">


<p>
  As we can see in the graph above, there are four models involved in the PPO algorithm. Two frozen models, the 
  <b>Reference Model</b> and the <b>Reward Model</b>, were trained beforehand and do not update their weights as part of the PPO algorithm.  
  In this section we will focus on the <b>Value Model</b> and the <b>Policy Model</b>, both of which are trained simultaneously within PPO.  
  First, we will explain how the Value Model is trained, then the Policy Model, and finally we will bring everything together in an interactive example.
</p>



  <h2>Value model</h2>

  <div class="eq" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"><span id="v2-eq"></span></div>

  <p>
    The value head answers for a state \(s_t\): how good is this situation.
    It predicts the discounted return \(G_t\) with
    \(V(s_t)=\mathbb{E}[G_t\mid S_t=s_t]\) where
    \(G_t=\sum_{k=0}^{\infty}\gamma^{k}R_{t+1+k}\).
    We train it with the small squared error
    \(\mathcal{L}_{\text{value}}=\tfrac12\big(V(s_t)-G_t\big)^2\).
  </p>

  <p>
    Why estimate \(V\)? The value model provides the baseline that turns raw reward into a teaching signal for the policy.
    When an observed or predicted reward is higher than \(V(s_t)\), the outcome was better than expected and the advantage
    becomes positive; if it is lower, the advantage is negative and the policy should make that behavior less likely.
    Here, \(s_t\) is the current state, \(R_{t+k}\) are future rewards, \(0\le\gamma<1\) discounts distant outcomes, and
    \(G_t\) is the total discounted return the model is trying to predict.
  </p>

  <h3>Let us look at a numeric example.</h3>
  <p>
    Use the same phone password path of rewards:
    \([0.5,\,0.2,\,0.8,\,0.7,\,1.2]\).
    Move the discount and the prediction and we compute
    the return \(G_1\), the value loss and a simple advantage
    \(\hat A \approx r(x,y)-V(s_1)\) that we will reuse in the PPO block.
  </p>

  <div class="ctrl">
    <label for="v2-gam">Discount \(\gamma\): <strong id="v2-gamv">0.90</strong></label>
    <input id="v2-gam" type="range" min="0.50" max="0.99" step="0.01" value="0.90"/>

    <label for="v2-V">Prediction \(V(s_1)\): <strong id="v2-Vv">2.30</strong></label>
    <input id="v2-V" type="range" min="0.0" max="4.0" step="0.05" value="2.30"/>

    <label for="v2-r">Reward score \(r(x,y)\): <strong id="v2-rv">2.50</strong></label>
    <input id="v2-r" type="range" min="-2.0" max="4.0" step="0.05" value="2.50"/>
  </div>

  <p class="stat">
    Return \(G_1\) = <code class="inline" id="v2-G">2.6256</code> ‚Ä¢
    Value loss \(\tfrac12(V-G)^2\) = <code class="inline" id="v2-L">0.0530</code> ‚Ä¢
    Advantage \(\hat A \approx r-V\) = <code class="inline" id="v2-A">0.2000</code>
  </p>

    <p>
    The Discount factor \(\gamma\) controls how much future rewards matter. If you increase \(\gamma\) toward \(1\), later rewards get more weight, so
    \(G_1\) rises when the tail of the trajectory is mostly positive (as in this example). If you decrease \(\gamma\) toward \(0\), only the next rewards count, so \(G_1\) shrinks and the target becomes short-sighted and more
    reactive to immediate outcomes. Move the \(\gamma\) slider above to see how \(G_1\), the value loss, and the implied advantage change.
  </p>

  <div class="eq"><span id="v2-work"></span></div>
</div>

<!-- ========== PPO POLICY UPDATE ========== -->
<div class="wrap loss2-sec" id="ppo-clip-v2">
  <h2>PPO policy update</h2>

  <div class="eq" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"><span id="ppo2-eq"></span></div>

  <p>
    The ratio tells how the new policy changes the action probability
    for the same state and action,
    \(r_t(\theta)=\frac{\pi_{\theta}(a_t\mid s_t)}{\pi_{\text{old}}(a_t\mid s_t)}\).
    The advantage \(\hat A_t\) gives direction and strength.
    The clip at \(1\pm\varepsilon\) keeps the update conservative.
  </p>

  <p>
    How the pieces interact: the <em>policy model</em> proposes tokens with probability \(\pi_\theta\); the
    <em>reward model</em> scores outcomes; the <em>value model</em> predicts how much reward remains.
    Together they produce \(\hat A_t\), the signal that says whether the taken action was better or worse than expected.
    If \(r_t=1\) the new policy behaves like the old one; \(r_t&gt;1\) increases the action‚Äôs probability; \(r_t&lt;1\)
    decreases it. Clipping limits the step so learning is steady rather than jumpy. In practice \(\hat A_t\) is often
    computed with GAE to blend information across several steps for a smoother update.
  </p>

  <h3 style="margin:1rem 0 .5rem; font-size:1.1rem; font-weight:700;">Let us connect it to the same example.</h3>
  <p>
    We reuse the simple advantage from the value block above
    or you can set it directly. Move the three sliders to see
    the unclipped and the clipped objective terms.
  </p>

  <div class="ctrl">
    <label for="ppo2-pold">\(\pi_{\text{old}}(a\mid s)\): <strong id="ppo2-poldv">0.10</strong></label>
    <input id="ppo2-pold" type="range" min="0.01" max="0.90" step="0.01" value="0.10"/>

    <label for="ppo2-pnew">\(\pi_{\theta}(a\mid s)\): <strong id="ppo2-pnewv">0.12</strong></label>
    <input id="ppo2-pnew" type="range" min="0.01" max="0.90" step="0.01" value="0.12"/>

    <label for="ppo2-eps">\(\varepsilon\): <strong id="ppo2-epsv">0.20</strong></label>
    <input id="ppo2-eps" type="range" min="0.05" max="0.40" step="0.01" value="0.20"/>

    <label for="ppo2-A">\(\hat A\) (optional override): <strong id="ppo2-Av">0.20</strong></label>
    <input id="ppo2-A" type="range" min="-1.0" max="1.0" step="0.01" value="0.20"/>
  </div>

  <p class="stat">
    Ratio \(r_t\) = <code class="inline" id="ppo2-r">1.2000</code> ‚Ä¢
    Unclipped \(r_t \hat A\) = <code class="inline" id="ppo2-u">0.2400</code> ‚Ä¢
    Clipped \(\text{clip}(r_t)\hat A\) = <code class="inline" id="ppo2-c">0.2200</code>
  </p>

  <div class="eq"><span id="ppo2-work"></span></div>

  <h4 style="margin:.6rem 0 .25rem; font-size:1rem; font-weight:700;">Interpreting the changes</h4>
  <p>
    Changing \(\pi_{\theta}(a\mid s)\) or \(\pi_{\text{old}}(a\mid s)\) directly changes the ratio \(r_t\).
    With \(\hat A_t&gt;0\), increasing \(r_t\) makes the update larger until it hits the upper clip \(1+\varepsilon\);
    with \(\hat A_t&lt;0\), decreasing \(r_t\) makes the update more negative until the lower clip \(1-\varepsilon\) engages.
    When \(r_t=1\), the new policy matches the old and the update size is controlled only by \(\hat A_t\).
  </p>
  <p>
    Changing the advantage \(\hat A_t\) (either via the value module above or by sliding it here) sets both the
    <em>direction</em> and the <em>magnitude</em> of the update. A positive \(\hat A_t\) asks PPO to make the action more likely;
    a negative \(\hat A_t\) asks it to make the action less likely. Larger \(|\hat A_t|\) means a stronger push until clipping caps it.
  </p>
  <p>
    Changing \(\varepsilon\) widens or narrows the safe band \([\,1-\varepsilon,\,1+\varepsilon\,]\).
    A larger \(\varepsilon\) allows bigger policy moves (faster learning but less stable),
    while a smaller \(\varepsilon\) keeps updates conservative (more stable but slower).
    Use the sliders to see how the unclipped term \(r_t\hat A_t\) can exceed the clipped term once the ratio crosses the band.
  </p>

  <div class="eq"><span id="ppo2-work"></span></div>
</div>


<script>

  function initRewardPairwise(){
    const $=(id)=>document.getElementById(id);
    const container = $('pairwise-loss-v2');
    const r1=$('r1b'), r2=$('r2b'), r1v=$('r1bv'), r2v=$('r2bv');
    const pout=$('pout2'), lout=$('lout2');
    const eqMain=$('loss2-eq'), eqWork=$('loss2-work');

    katex.render(String.raw`\mathcal{L}(r)=-\log\!\left(\frac{e^{\,r(x,y_b)}}{e^{\,r(x,y_1)}+e^{\,r(x,y_2)}}\right)`,
                 eqMain, {displayMode:true, throwOnError:false});

    try{
      renderMathInElement(container,{
        delimiters:[
          {left:"\\(", right:"\\)", display:false},
          {left:"\\[", right:"\\]", display:true}
        ],
        throwOnError:false
      });
    }catch(e){}

    function fmt(x,d=4){ return Number(x).toFixed(d); }
    function compute(){
      const v1=parseFloat(r1.value), v2=parseFloat(r2.value);
      r1v.textContent=v1.toFixed(1);
      r2v.textContent=v2.toFixed(1);
      const e1=Math.exp(v1), e2=Math.exp(v2), Z=e1+e2;
      const p1=e1/Z, p2=e2/Z;
      const pref=document.querySelector('input[name="pref2"]:checked').value;
      const pb=(pref==='y1')?p1:p2;
      const L=-Math.log(pb);
      pout.textContent=fmt(pb);
      lout.textContent=fmt(L);
      const rb=(pref==='y1')?v1:v2;
      const erb=(pref==='y1')?e1:e2;
      const work = String.raw`
        -\log\!\left(\frac{e^{${rb.toFixed(1)}}}{e^{${v1.toFixed(1)}}+e^{${v2.toFixed(1)}}}\right)
        = -\log\!\left(\frac{${fmt(erb)}}{${fmt(e1)}+${fmt(e2)}}\right)
        = -\log(${fmt(pb,3)}) \approx ${fmt(L)}
      `;
      katex.render(work, eqWork, {displayMode:true, throwOnError:false});
    }
    ['input','change'].forEach(ev=>{
      r1.addEventListener(ev, compute);
      r2.addEventListener(ev, compute);
      document.querySelectorAll('input[name="pref2"]').forEach(el=>el.addEventListener(ev, compute));
    });
    compute();
  }

  function initValueAndPPO(){
    katex.render(String.raw`
      V(s_t)=\mathbb{E}\!\big[G_t\mid S_t=s_t\big],\quad
      G_t=\sum_{k=0}^{\infty}\gamma^{k}R_{t+1+k},\quad
      \mathcal{L}_{\text{value}}=\tfrac12\!\left(V(s_t)-G_t\right)^2
    `, document.getElementById('v2-eq'), {displayMode:true, throwOnError:false});

    katex.render(String.raw`
      J_{\text{clip}}(\theta)=\mathbb{E}\!\left[
        \min\!\big(r_t(\theta)\hat A_t,\,
        \mathrm{clip}(r_t(\theta),\,1-\varepsilon,\,1+\varepsilon)\hat A_t\big)
      \right],\qquad
      r_t(\theta)=\frac{\pi_{\theta}(a_t\mid s_t)}{\pi_{\text{old}}(a_t\mid s_t)}
    `, document.getElementById('ppo2-eq'), {displayMode:true, throwOnError:false});

    try{
      renderMathInElement(document.getElementById('value-loss-v2'),{
        delimiters:[{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}],
        throwOnError:false
      });
      renderMathInElement(document.getElementById('ppo-clip-v2'),{
        delimiters:[{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}],
        throwOnError:false
      });
    }catch(e){}

    const gam = document.getElementById('v2-gam');
    const Vpred = document.getElementById('v2-V');
    const rxy = document.getElementById('v2-r');
    const gLab = document.getElementById('v2-gamv');
    const VLab = document.getElementById('v2-Vv');
    const rLab = document.getElementById('v2-rv');
    const Gout = document.getElementById('v2-G');
    const Lout = document.getElementById('v2-L');
    const Aout = document.getElementById('v2-A');
    const vWork = document.getElementById('v2-work');

    const R = [0.5,0.2,0.8,0.7,1.2];
    const fmt = (x,d=4)=>Number(x).toFixed(d);

    function computeValue(){
      const g = +gam.value, V1 = +Vpred.value, rx = +rxy.value;
      gLab.textContent = g.toFixed(2);
      VLab.textContent = V1.toFixed(2);
      rLab.textContent = rx.toFixed(2);
      let G1 = 0, parts = [];
      for(let k=0;k<R.length;k++){
        const term = Math.pow(g,k)*R[k];
        G1 += term;
        parts.push(`${k===0?'':' + '}\\gamma^{${k}}\\cdot ${R[k].toFixed(1)}`);
      }
      const L = 0.5*(V1-G1)*(V1-G1);
      const A = rx - V1;

      Gout.textContent = fmt(G1);
      Lout.textContent = fmt(L);
      Aout.textContent = fmt(A);

      katex.render(String.raw`
        G_1 = ${parts.join('')} = ${fmt(G1)} \qquad
        \mathcal{L}_{\text{value}}=\tfrac12\big(${V1.toFixed(2)}-${fmt(G1)}\big)^2 = ${fmt(L)} \qquad
        \hat A \approx ${rx.toFixed(2)} - ${V1.toFixed(2)} = ${fmt(A)}
      `, vWork, {displayMode:true, throwOnError:false});

      const Aoverride = document.getElementById('ppo2-A');
      if(!Aoverride.dataset.touched){
        Aoverride.value = A.toFixed(2);
        document.getElementById('ppo2-Av').textContent = Aoverride.value;
        computePPO();
      }
    }

    const pOld = document.getElementById('ppo2-pold');
    const pNew = document.getElementById('ppo2-pnew');
    const eps = document.getElementById('ppo2-eps');
    const Ainp = document.getElementById('ppo2-A');
    const pOldv = document.getElementById('ppo2-poldv');
    const pNewv = document.getElementById('ppo2-pnewv');
    const epsv = document.getElementById('ppo2-epsv');
    const Av = document.getElementById('ppo2-Av');
    const rOut = document.getElementById('ppo2-r');
    const uOut = document.getElementById('ppo2-u');
    const cOut = document.getElementById('ppo2-c');
    const ppoWork = document.getElementById('ppo2-work');

    function computePPO(){
      const po = +pOld.value, pn = +pNew.value, e = +eps.value, A = +Ainp.value;
      pOldv.textContent = po.toFixed(2);
      pNewv.textContent = pn.toFixed(2);
      epsv.textContent = e.toFixed(2);
      Av.textContent = A.toFixed(2);
      const rt = pn / po;
      const uncl = rt * A;
      const rtClip = Math.max(1-e, Math.min(1+e, rt));
      const clp = rtClip * A;
      const fmt2 = (x)=>Number(x).toFixed(4);
      rOut.textContent = fmt2(rt);
      uOut.textContent = fmt2(uncl);
      cOut.textContent = fmt2(clp);
      katex.render(String.raw`
        r_t=\frac{${pn.toFixed(2)}}{${po.toFixed(2)}}=${fmt2(rt)},\qquad
        \min\!\Big(r_t\hat A,\ \mathrm{clip}(r_t,\,1-${e.toFixed(2)},\,1+${e.toFixed(2)})\hat A\Big)
        = \min\!\big(${fmt2(uncl)},\ ${fmt2(clp)}\big)
      `, ppoWork, {displayMode:true, throwOnError:false});
    }

    Ainp.addEventListener('input', ()=>{ Ainp.dataset.touched = '1'; Av.textContent=Ainp.value; computePPO(); });
    ['input','change'].forEach(ev=>{
      [gam,Vpred,rxy].forEach(el=>el.addEventListener(ev, computeValue));
      [pOld,pNew,eps].forEach(el=>el.addEventListener(ev, computePPO));
    });
    computeValue();
    computePPO();
  }
</script>

<!--HERE WAS GRPO-->

<!-- ================= GRPO (from scratch, with split + converge arrows) (Juan)================= -->
<div class="wrap" id="grpo-flow">
  <h1>Group Relative Policy Optimization (GRPO)</h1>

  <style>
    /* minimal edge styles to match PPO look */
    #grpo-flow .edge{ stroke:#cbd5e1; stroke-width:3; fill:none; marker-end:url(#grpoArrow); }
    #grpo-flow .back{ stroke:#64748b; stroke-width:3; fill:none; stroke-dasharray:7 5; marker-end:url(#grpoArrow); }
    /* fallback for shapes if your page doesn't already style these */
    #grpo-flow .node{ filter:drop-shadow(0 2px 4px rgba(0,0,0,.08)); }
    #grpo-flow .rect-node{ fill:#e2e8f0; stroke:#0f172a; stroke-width:2; }
    #grpo-flow .model-node{ fill:#7dd3fc; stroke:#0f172a; stroke-width:2; }
    #grpo-flow .value-node{ fill:#fde68a; stroke:#0f172a; stroke-width:2; }
    #grpo-flow .small{ font-size:12px; fill:#334155; }
  </style>

  <svg id="grpoSvg" viewBox="0 0 1500 360" aria-label="GRPO diagram">
    <defs>
      <marker id="grpoArrow" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
        <polygon points="0 0, 10 3.5, 0 7" fill="#cbd5e1"/>
      </marker>
    </defs>

    <!-- Input q -->
    <rect x="40" y="150" width="56" height="38" rx="6" class="node rect-node"/>
    <text x="68" y="173" text-anchor="middle" font-weight="700">q</text>

    <!-- Policy -->
    <ellipse cx="210" cy="170" rx="90" ry="34" class="node value-node"/>
    <text x="210" y="175" text-anchor="middle" font-weight="700">Policy Model</text>

    <!-- Outputs (o1..o4) -->
    <text x="410" y="70" class="small" text-anchor="middle">Group of Outputs</text>
    <rect x="360" y="95"  width="70" height="34" rx="8" class="node rect-node"/><text x="395" y="116" text-anchor="middle">o1</text>
    <rect x="360" y="140" width="70" height="34" rx="8" class="node rect-node"/><text x="395" y="161" text-anchor="middle">o2</text>
    <rect x="360" y="185" width="70" height="34" rx="8" class="node rect-node"/><text x="395" y="206" text-anchor="middle">o3</text>
    <rect x="360" y="230" width="70" height="34" rx="8" class="node rect-node"/><text x="395" y="251" text-anchor="middle">o4</text>

    <!-- Reference & Reward models (same level) -->
    <ellipse cx="640" cy="125" rx="100" ry="30" class="node model-node"/>
    <text x="640" y="130" text-anchor="middle" font-weight="700">Reference Model</text>

    <ellipse cx="640" cy="240" rx="100" ry="30" class="node model-node"/>
    <text x="640" y="245" text-anchor="middle" font-weight="700">Reward Model</text>

    <!-- Rewards (r1..r4) -->
    <text x="860" y="70" class="small" text-anchor="middle">Rewards</text>
    <rect x="820" y="95"  width="70" height="34" rx="8" class="node rect-node"/><text x="855" y="116" text-anchor="middle">r1</text>
    <rect x="820" y="140" width="70" height="34" rx="8" class="node rect-node"/><text x="855" y="161" text-anchor="middle">r2</text>
    <rect x="820" y="185" width="70" height="34" rx="8" class="node rect-node"/><text x="855" y="206" text-anchor="middle">r3</text>
    <rect x="820" y="230" width="70" height="34" rx="8" class="node rect-node"/><text x="855" y="251" text-anchor="middle">r4</text>

    <!-- Group Computation -->
    <ellipse cx="1040" cy="205" rx="100" ry="30" class="node rect-node"/>
    <text x="1040" y="210" text-anchor="middle" font-weight="700">Group Computation</text>

    <!-- Advantages (A1..A4) -->
    <text x="1265" y="70" class="small" text-anchor="middle">Advantage</text>
    <rect x="1225" y="95"  width="70" height="34" rx="8" class="node rect-node"/><text x="1260" y="116" text-anchor="middle">A1</text>
    <rect x="1225" y="140" width="70" height="34" rx="8" class="node rect-node"/><text x="1260" y="161" text-anchor="middle">A2</text>
    <rect x="1225" y="185" width="70" height="34" rx="8" class="node rect-node"/><text x="1260" y="206" text-anchor="middle">A3</text>
    <rect x="1225" y="230" width="70" height="34" rx="8" class="node rect-node"/><text x="1260" y="251" text-anchor="middle">A4</text>

    <!-- ===================== ARROWS ===================== -->

    <!-- q -> policy, policy -> outputs (to middle of stack) -->
    <path class="edge" d="M 96 169 H 120" style="marker-end:none"/>
    <path class="edge" d="M 300 170 H 360" />

    <!-- outputs -> ONE vertical spine -->
    <path class="edge" d="M 430 112 H 470" style="marker-end:none"/>
    <path class="edge" d="M 430 157 H 470" style="marker-end:none"/>
    <path class="edge" d="M 430 202 H 470" style="marker-end:none"/>
    <path class="edge" d="M 430 247 H 470" style="marker-end:none"/>
    <!-- vertical spine -->
    <path id="o_spine" class="edge" d="M 470 112 V 247" style="marker-end:none"/>

    <!-- single horizontal from spine to a split point -->
    <path id="o_spine_to_split" class="edge" d="M 470 180 H 550" style="marker-end:none"/>

    <!-- split to Reference (up) and Reward (down) -->
    <path class="edge" d="M 550 180 L 550 135" />
    <path class="edge" d="M 550 180 L 550 230" />


    <!-- Reward Model -> single join, then fan to r1..r4 (or straight to join) -->
    <path id="rm_to_join" class="edge" d="M 740 240 H 765" style="marker-end:none" />

    <path id="vertical_rw_model" class="edge" d="M 765 247 L 765 112" style="marker-end:none" />
    <path class="edge" d="M 764 112 L 820 112" />
    <path class="edge" d="M 765 157 L 820 157" />
    <path class="edge" d="M 765 202 L 820 202" />
    <path class="edge" d="M 765 247 L 820 247" />

    <!-- Rewards -> ONE junction (converge) -> Group Computation -->
    <path class="edge" d="M 890 112 L 905 112" style="marker-end:none"/>
    <path class="edge" d="M 890 157 L 905 157" style="marker-end:none"/>
    <path class="edge" d="M 890 202 L 905 202" style="marker-end:none"/>
    <path class="edge" d="M 890 247 L 905 247" style="marker-end:none"/>

    <!-- vertical spine -->
    <path id="r_spine" class="edge" d="M 905 112 V 247" style="marker-end:none"/>

    <path class="edge" d="M 905 202 H 940" />

    <!-- Group Computation -> Advantages (straight midline) -->
    <path class="edge" d="M 1140 205 H 1170" style="marker-end:none"/>
    <path id="Group_spine" class="edge" d="M 1170 112 V 247" style="marker-end:none"/>
    
    <path class="edge" d="M 1170 112 H 1220" />
    <path class="edge" d="M 1170 160 H 1220" />
    <path class="edge" d="M 1170 205 H 1220" />
    <path class="edge" d="M 1170 247 H 1220" />


    <!-- Advantages -> Policy (feedback) -->

    <text x="1265" y="70" class="small" text-anchor="middle">Advantage</text>
    <path class="edge" d="M 1265 10 V 60 60" style="marker-end:none" />
    <path class="edge" d="M 1265 10 H 200" style="marker-end:none"/>
    <path class="edge" d="M 200 9 V 130" />

    <!-- KL arc: Reference -> Policy -->
    <path class="edge" d="M 640 10 V 94 60" style="marker-end:none" />

    <text x="500" y="5" class="small" font-weight="700">GRPO update</text>
    <text x="530" y="25" class="small" font-weight="700">KL</text>


<!-- ================= GRPO Animation ================= -->
<svg id="grpoSvg" viewBox="0 0 1500 360" aria-label="GRPO diagram">

  <!-- ===== Tokens ===== -->
  <circle id="g_t_main" r="6" fill="#3b82f6" stroke="#1e40af" stroke-width="1.5" opacity="0"/>
  <!-- Outputs -->
  <circle id="g_t_o1" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_o2" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_o3" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_o4" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_outJoin" r="6" fill="#3b82f6" stroke="#1e40af" stroke-width="1.5" opacity="0"/>
  <!-- Ref + Reward -->
  <circle id="g_t_ref" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>


  <circle id="g_t_refHold"
        r="5" cx="640" cy="90"
        fill="#3b82f6" stroke="#1e40af" stroke-width="1.2"
        opacity="0"/>

  <circle id="g_t_refBack" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>

  <circle id="g_t_rewSeed" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <!-- Rewards -->
  <circle id="g_t_r1" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_r2" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_r3" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_r4" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_rJoin" r="6" fill="#3b82f6" stroke="#1e40af" stroke-width="1.5" opacity="0"/>
  <!-- Group -->
  <circle id="g_t_group" r="6" fill="#3b82f6" stroke="#1e40af" stroke-width="1.5" opacity="0"/>
  <!-- Advantages -->
  <circle id="g_t_a1" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_a2" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_a3" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_a4" r="5" fill="#3b82f6" stroke="#1e40af" stroke-width="1.2" opacity="0"/>
  <circle id="g_t_aJoin" r="6" fill="#3b82f6" stroke="#1e40af" stroke-width="1.5" opacity="0"/>
  <!-- Final merge -->
  <circle id="g_t_merge" r="6" fill="#3b82f6" stroke="#1e40af" stroke-width="1.5" opacity="0"/>

  <!-- ===== Invisible Paths (simplified for clarity) ===== -->
  <!-- q -> Policy -> Outputs spine -->
  <path id="g_p_main" d="M 96 169 H 360" fill="none" stroke="none"/>
  <!-- Outputs to join -->
  <path id="g_p_o1" d="M 360 170 L 430 112 L 470 112 V 180 H 550" fill="none" stroke="none"/>
  <path id="g_p_o2" d="M 360 170 L 430 157 L 470 157 V 180 H 550" fill="none" stroke="none"/>
  <path id="g_p_o3" d="M 360 170 L 430 202 L 470 202 V 180 H 550" fill="none" stroke="none"/>
  <path id="g_p_o4" d="M 360 170 L 430 247 L 470 247 V 180 H 550" fill="none" stroke="none"/>
  <path id="g_p_outJoin" d="M 550 180 H 550" fill="none" stroke="none"/>
  <!-- Split Ref/Reward -->
  <path id="g_p_split_ref" d="M 550 180 V 135 H 640" fill="none" stroke="none"/>
  <path id="g_p_split_rew" d="M 550 180 V 230 H 640" fill="none" stroke="none"/>
  <!-- Rewards -->
  <path id="g_p_r1" d="M 640 240 H 765 V 112 H 900 V 205" fill="none" stroke="none"/>
<path id="g_p_r2" d="M 640 240 H 765 V 157 H 900 V 205" fill="none" stroke="none"/>
<path id="g_p_r3" d="M 640 240 H 765 V 202 H 900 V 205" fill="none" stroke="none"/>
<path id="g_p_r4" d="M 640 240 H 765 V 247 H 900 V 205" fill="none" stroke="none"/>
  <path id="g_p_rJoin" d="M 900 205 H 940" fill="none" stroke="none"/>
  <!-- Group -->
  <path id="g_p_group" d="M 940 205 H 1040" fill="none" stroke="none"/>
  <!-- Advantages -->
  <path id="g_p_a1" d="M 1040 205 H 1170 V 112 H 1220" fill="none" stroke="none"/>
  <path id="g_p_a2" d="M 1040 205 H 1170 V 160 H 1220" fill="none" stroke="none"/>
  <path id="g_p_a3" d="M 1040 205 H 1170 V 205 H 1220" fill="none" stroke="none"/>
  <path id="g_p_a4" d="M 1040 205 H 1170 V 247 H 1220" fill="none" stroke="none"/>
  <path id="g_p_aJoin" d="M 1220 205 V 10 H 210 V 150" fill="none" stroke="none"/>
  <!-- Final merge into Policy -->
  <path id="g_p_merge" d="M 220 150 L 210 170" fill="none" stroke="none"/>

  <!-- ===== AnimateMotion blocks ===== -->
  <animateMotion id="g_m_main" href="#g_t_main" dur="2s" begin="indefinite"><mpath href="#g_p_main"/></animateMotion>
  <animateMotion id="g_m_o1" href="#g_t_o1" dur="1.5s" begin="indefinite"><mpath href="#g_p_o1"/></animateMotion>
  <animateMotion id="g_m_o2" href="#g_t_o2" dur="1.5s" begin="indefinite"><mpath href="#g_p_o2"/></animateMotion>
  <animateMotion id="g_m_o3" href="#g_t_o3" dur="1.5s" begin="indefinite"><mpath href="#g_p_o3"/></animateMotion>
  <animateMotion id="g_m_o4" href="#g_t_o4" dur="1.5s" begin="indefinite"><mpath href="#g_p_o4"/></animateMotion>
  <animateMotion id="g_m_outJoin" href="#g_t_outJoin" dur="0.5s" begin="indefinite"><mpath href="#g_p_outJoin"/></animateMotion>
  <animateMotion id="g_m_refSplit" href="#g_t_ref" dur="1s" begin="indefinite"><mpath href="#g_p_split_ref"/></animateMotion>
  <animateMotion id="g_m_rewSeed" href="#g_t_rewSeed" dur="1s" begin="indefinite"><mpath href="#g_p_split_rew"/></animateMotion>
  <animateMotion id="g_m_r1" href="#g_t_r1" dur="1.5s" begin="indefinite"><mpath href="#g_p_r1"/></animateMotion>
  <animateMotion id="g_m_r2" href="#g_t_r2" dur="1.5s" begin="indefinite"><mpath href="#g_p_r2"/></animateMotion>
  <animateMotion id="g_m_r3" href="#g_t_r3" dur="1.5s" begin="indefinite"><mpath href="#g_p_r3"/></animateMotion>
  <animateMotion id="g_m_r4" href="#g_t_r4" dur="1.5s" begin="indefinite"><mpath href="#g_p_r4"/></animateMotion>
  <animateMotion id="g_m_rJoin" href="#g_t_rJoin" dur="0.5s" begin="indefinite"><mpath href="#g_p_rJoin"/></animateMotion>
  <animateMotion id="g_m_group" href="#g_t_group" dur="1s" begin="indefinite"><mpath href="#g_p_group"/></animateMotion>
  <animateMotion id="g_m_a1" href="#g_t_a1" dur="1.2s" begin="indefinite"><mpath href="#g_p_a1"/></animateMotion>
  <animateMotion id="g_m_a2" href="#g_t_a2" dur="1.2s" begin="indefinite"><mpath href="#g_p_a2"/></animateMotion>
  <animateMotion id="g_m_a3" href="#g_t_a3" dur="1.2s" begin="indefinite"><mpath href="#g_p_a3"/></animateMotion>
  <animateMotion id="g_m_a4" href="#g_t_a4" dur="1.2s" begin="indefinite"><mpath href="#g_p_a4"/></animateMotion>
  <animateMotion id="g_m_aJoin" href="#g_t_aJoin" dur="2s" begin="indefinite"><mpath href="#g_p_aJoin"/></animateMotion>
  <animateMotion id="g_m_merge" href="#g_t_merge" dur="0.7s" begin="indefinite"><mpath href="#g_p_merge"/></animateMotion>

  <path id="g_p_refBack" d="M 640 125 V 10 H 220 V 150" fill="none" stroke="none"/>
<animateMotion id="g_m_refBack" href="#g_t_refBack" dur="1.7s" begin="indefinite">
  <mpath href="#g_p_refBack"/>
</animateMotion>



</svg>

<script>
(function(){
  const $ = id => document.getElementById(id);

  // Tokens
  const tMain=$('g_t_main'), tO=[ $('g_t_o1'),$('g_t_o2'),$('g_t_o3'),$('g_t_o4') ], tOutJoin=$('g_t_outJoin');
  const tRef=$('g_t_ref'), tRewSeed=$('g_t_rewSeed');
  const tR=[ $('g_t_r1'),$('g_t_r2'),$('g_t_r3'),$('g_t_r4') ], tRJoin=$('g_t_rJoin');
  const tGroup=$('g_t_group');
  const tA=[ $('g_t_a1'),$('g_t_a2'),$('g_t_a3'),$('g_t_a4') ], tAJoin=$('g_t_aJoin');
  const tMerge=$('g_t_merge');

  // Motions
  const mMain=$('g_m_main'), mO=[ $('g_m_o1'),$('g_m_o2'),$('g_m_o3'),$('g_m_o4') ], mOutJoin=$('g_m_outJoin');
  const mRefSplit=$('g_m_refSplit'), mRewSeed=$('g_m_rewSeed');
  const mR=[ $('g_m_r1'),$('g_m_r2'),$('g_m_r3'),$('g_m_r4') ], mRJoin=$('g_m_rJoin');
  const mGroup=$('g_m_group');
  const mA=[ $('g_m_a1'),$('g_m_a2'),$('g_m_a3'),$('g_m_a4') ], mAJoin=$('g_m_aJoin');
  const mMerge=$('g_m_merge');

  const tRefBack = $('g_t_refBack');
  const mRefBack = $('g_m_refBack');

  const tRefHold = $('g_t_refHold');


  let outDone=0, rDone=0, aDone=0, advArr=false, refArr=false;

  function reset(){ outDone=rDone=aDone=0; advArr=refArr=false; }

  // Start
  function start(){ reset(); mMain.beginElement(); }

  // Flow
  mMain.addEventListener('beginEvent', ()=> tMain.setAttribute('opacity','1'));
  mMain.addEventListener('endEvent', ()=>{ tMain.setAttribute('opacity','0'); mO.forEach(m=>m.beginElement()); });

  // Outputs
  mO.forEach((mo,i)=>{ 
    mo.addEventListener('beginEvent', ()=> tO[i].setAttribute('opacity','1'));
    mo.addEventListener('endEvent', ()=>{ tO[i].setAttribute('opacity','0'); if(++outDone===4){ mOutJoin.beginElement(); }});
  });
  mOutJoin.addEventListener('beginEvent', ()=> tOutJoin.setAttribute('opacity','1'));
  mOutJoin.addEventListener('endEvent', ()=>{
    tOutJoin.setAttribute('opacity','0');  // hide the merged output ball

    // spawn new balls at the split point
    tRef.setAttribute('opacity','1');
    tRewSeed.setAttribute('opacity','1');

    // animate them
    mRefSplit.beginElement();
    mRewSeed.beginElement();
  });
  // Ref stays
  mRefSplit.addEventListener('beginEvent', ()=> tRef.setAttribute('opacity','1'));

  mRefSplit.addEventListener('endEvent', ()=>{
  tRef.setAttribute('opacity','0');          // travelling ball disappears
  $('g_t_refHold').setAttribute('opacity','1'); // static ball appears
});
  
  mRefBack.addEventListener('beginEvent', ()=> tRefBack.setAttribute('opacity','1'));
  mRefBack.addEventListener('endEvent',   ()=> tRefBack.setAttribute('opacity','0'));

  // Reward seed
  mRewSeed.addEventListener('beginEvent', ()=> tRewSeed.setAttribute('opacity','1'));
  mRewSeed.addEventListener('endEvent', ()=>{ tRewSeed.setAttribute('opacity','0'); mR.forEach(m=>m.beginElement()); });

  // Rewards
  // Rewards
mR.forEach((mr,i)=>{ 
  mr.addEventListener('beginEvent', ()=> tR[i].setAttribute('opacity','1'));
  mr.addEventListener('endEvent', ()=>{
    tR[i].setAttribute('opacity','0');   // hide each reward ball when done
    if (++rDone === 4) {
      // when all 4 are done, launch the join ball
      mRJoin.beginElement();
    }
  });
});

// Reward join ball
mRJoin.addEventListener('beginEvent', ()=> tRJoin.setAttribute('opacity','1'));
mRJoin.addEventListener('endEvent', ()=>{
  tRJoin.setAttribute('opacity','0');
  mGroup.beginElement();   // continue to Group Computation
});


  // Group
  mGroup.addEventListener('beginEvent', ()=> tGroup.setAttribute('opacity','1'));
  mGroup.addEventListener('endEvent', ()=>{ tGroup.setAttribute('opacity','0'); mA.forEach(m=>m.beginElement()); });

  // Advantages
mA.forEach((ma,i)=>{ 
  ma.addEventListener('beginEvent', ()=> tA[i].setAttribute('opacity','1'));
  ma.addEventListener('endEvent', ()=>{ 
    tA[i].setAttribute('opacity','0'); 
    if(++aDone===4) mAJoin.beginElement(); 
  });
});

mAJoin.addEventListener('beginEvent', ()=>{
  tAJoin.setAttribute('opacity','1');
  tRefHold.setAttribute('opacity','0');   // hide the waiting ball
  mRefBack.beginElement();                // launch the returning ball (synced)
});

mAJoin.addEventListener('endEvent', ()=>{
  tAJoin.setAttribute('opacity','0'); 
  mMerge.beginElement(); 
});

  // Merge final
  mMerge.addEventListener('beginEvent', ()=> tMerge.setAttribute('opacity','1'));
  mMerge.addEventListener('endEvent', ()=>{ tMerge.setAttribute('opacity','0'); setTimeout(start,1000); });

  // Kickstart
  window.addEventListener('load', start);
})();
</script>



  </svg>
</div>



<!-- ========== GRPO Math and Formulas (back to Lara) ========== -->
<div class="wrap loss2-sec" id="grpo-sec">
  <h2>GRPO: group-relative policy optimization</h2>

  <div class="eq" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"><span id="grpo-eq"></span></div>


  <p>
    GRPO updates the policy with a clipped objective like PPO but <strong>does not use a value model</strong>.
    For a group of candidates for the same prompt, it standardizes the rewards to get a group-relative advantage.
    This idea, presented in <em>DeepSeekMath</em> by Shao et&nbsp;al., 2024
    (<a href="https://doi.org/10.48550/arXiv.2402.03300" target="_blank" rel="noopener">arXiv:2402.03300</a>),
    removes the need for an explicit baseline \(V(s)\): we only ask ‚Äúhow good is this output compared with its peers?‚Äù.
  </p>

  <div class="eq" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"><span id="grpo-adv-eq"></span></div>

  <p>
    Positive \(A_i\) means ‚Äúbetter than the group‚Äù, negative \(A_i\) means ‚Äúworse than the group‚Äù.
    As in PPO, the update is <em>clipped</em> for stability, and a KL penalty keeps the new policy
    close to a reference model (typically the base LM) to preserve fluency and avoid reward hacking:
  </p>

  <div class="eq" style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;"><span id="grpo-kl-eq"></span></div>


  <h3 style="margin:1rem 0 .5rem; font-size:1.1rem; font-weight:700;">Interactive example (same prompt)</h3>
  <p>
    Set the three rewards \(r_1,r_2,r_3\) for one prompt, pick which output \(o_i\) we optimize, and adjust the probabilities.
    We compute \(A_i\), the ratio \(r_t=\pi_\theta/\pi_{\text{old}}\), its clipped version, the KL penalty, and the GRPO objective.
  </p>

  <div class="ctrl">
    <label for="g-r1">Reward \(r_1\): <strong id="g-r1v">2.50</strong></label>
    <input id="g-r1" type="range" min="-1.0" max="4.0" step="0.05" value="2.50"/>

    <label for="g-r2">Reward \(r_2\): <strong id="g-r2v">1.80</strong></label>
    <input id="g-r2" type="range" min="-1.0" max="4.0" step="0.05" value="1.80"/>

    <label for="g-r3">Reward \(r_3\): <strong id="g-r3v">0.70</strong></label>
    <input id="g-r3" type="range" min="-1.0" max="4.0" step="0.05" value="0.70"/>

    <label>Optimize sample:</label>
    <div>
      <label style="margin-right:1.0rem"><input type="radio" name="g-which" value="1" checked/> \(o_1\)</label>
      <label style="margin-right:1.0rem"><input type="radio" name="g-which" value="2"/> \(o_2\)</label>
      <label><input type="radio" name="g-which" value="3"/> \(o_3\)</label>
    </div>

    <label for="g-pold">\(\pi_{\text{old}}(o_i\mid q)\): <strong id="g-poldv">0.10</strong></label>
    <input id="g-pold" type="range" min="0.01" max="0.90" step="0.01" value="0.10"/>

    <label for="g-pnew">\(\pi_{\theta}(o_i\mid q)\): <strong id="g-pnewv">0.13</strong></label>
    <input id="g-pnew" type="range" min="0.01" max="0.90" step="0.01" value="0.13"/>

    <label for="g-pref">\(\pi_{\text{ref}}(o_i\mid q)\): <strong id="g-prefv">0.12</strong></label>
    <input id="g-pref" type="range" min="0.01" max="0.90" step="0.01" value="0.12"/>

    <label for="g-eps">\(\varepsilon\): <strong id="g-epsv">0.20</strong></label>
    <input id="g-eps" type="range" min="0.05" max="0.40" step="0.01" value="0.20"/>

    <label for="g-beta">\(\beta\): <strong id="g-betav">0.10</strong></label>
    <input id="g-beta" type="range" min="0.00" max="0.50" step="0.01" value="0.10"/>
  </div>

  <p class="stat">
    mean \(=\) <code class="inline" id="g-mean">1.6667</code> ‚Ä¢ std \(=\) <code class="inline" id="g-std">0.9074</code> ‚Ä¢
    \(A_i\) \(=\) <code class="inline" id="g-Ai">0.9186</code>
  </p>

  <p class="stat">
    Ratio \(r_t\) \(=\) <code class="inline" id="g-rt">1.3000</code> ‚Ä¢
    Unclipped \(r_tA_i\) \(=\) <code class="inline" id="g-un">1.1932</code> ‚Ä¢
    Clipped \(\text{clip}(r_t)\,A_i\) \(=\) <code class="inline" id="g-cl">1.0105</code> ‚Ä¢
    \(D_{\text{KL}}\) \(=\) <code class="inline" id="g-kl">0.0040</code>
  </p>

  <p class="stat">
    GRPO objective \(=\) <code class="inline" id="g-L">1.0065</code>
  </p>

  <div class="eq"><span id="grpo-work"></span></div>

    <h4 style="margin:.8rem 0 .35rem; font-size:1rem; font-weight:700;">Interpreting the changes</h4>
  <p>
    <strong>\(r_t=\pi_{\theta}/\pi_{\text{old}}\)</strong> sets direction: with \(A_i&gt;0\) increasing \(r_t\) boosts the objective
    (until clipping at \(1+\varepsilon\)); with \(A_i&lt;0\) decreasing \(r_t\) helps. <strong>\(\varepsilon\)</strong> widens or narrows
    the safe band \([1-\varepsilon,\,1+\varepsilon]\): large \(\varepsilon\) learns faster but is less stable.
    <strong>\(\beta\)</strong> scales the KL anchor: higher \(\beta\) pulls the policy toward the reference distribution
    \(\pi_{\text{ref}}\) (useful to preserve base-model fluency and curb reward hacking).
  </p>
  <p>
    Unlike PPO, GRPO‚Äôs ‚Äúbaseline‚Äù comes from the <em>group</em>.
    Raising other candidates can shrink your \(A_i\) even if your reward stays the same, because
    \(A_i\) measures how much better or worse \(o_i\) is relative to its peers.
  </p>

  <div class="eq"><span id="grpo-work"></span></div>



<script>
  function initGRPO(){
    katex.render(String.raw`
      \mathcal{L}_{\mathrm{GRPO}}(\theta)
      = \frac{1}{G}\sum_{i=1}^{G}
      \Big(
        \min\!\big(r_t^{(i)}\,A_i,\ \mathrm{clip}(r_t^{(i)},\,1-\varepsilon,\,1+\varepsilon)\,A_i\big)
      \Big)
      \;-\;\beta\,D_{\mathrm{KL}}\!\big(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}}\big)
    `, document.getElementById('grpo-eq'), {displayMode:true, throwOnError:false});

    katex.render(String.raw`
      A_i \;=\; \frac{r_i-\operatorname{mean}\!\big(\{r_1,\ldots,r_G\}\big)}
                      {\operatorname{std}\!\big(\{r_1,\ldots,r_G\}\big)}
    `, document.getElementById('grpo-adv-eq'), {displayMode:true, throwOnError:false});

    katex.render(String.raw`
      D_{\mathrm{KL}}\!\big(\pi_{\theta}\,\|\,\pi_{\mathrm{ref}}\big)
      \;=\; \frac{\pi_{\mathrm{ref}}(o_i\mid q)}{\pi_{\theta}(o_i\mid q)}
            \;-\;\log\!\frac{\pi_{\mathrm{ref}}(o_i\mid q)}{\pi_{\theta}(o_i\mid q)}\;-\;1
    `, document.getElementById('grpo-kl-eq'), {displayMode:true, throwOnError:false});

    try{
      renderMathInElement(document.getElementById('grpo-sec'),{
        delimiters:[{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}],
        throwOnError:false
      });
    }catch(e){}

    const $=(id)=>document.getElementById(id);
    const r1=$('g-r1'), r2=$('g-r2'), r3=$('g-r3');
    const r1v=$('g-r1v'), r2v=$('g-r2v'), r3v=$('g-r3v');
    const pold=$('g-pold'), pnew=$('g-pnew'), pref=$('g-pref');
    const poldv=$('g-poldv'), pnewv=$('g-pnewv'), prefv=$('g-prefv');
    const eps=$('g-eps'), bet=$('g-beta');
    const epsv=$('g-epsv'), betv=$('g-betav');

    const meanOut=$('g-mean'), stdOut=$('g-std'), AiOut=$('g-Ai');
    const rtOut=$('g-rt'), unOut=$('g-un'), clOut=$('g-cl'), klOut=$('g-kl'), LOut=$('g-L');
    const work=$('grpo-work');

    function fmt(x,d=4){ return Number(x).toFixed(d); }
    function sampleStd(a){
      const m = a.reduce((s,x)=>s+x,0)/a.length;
      const v = a.reduce((s,x)=>s+(x-m)*(x-m),0)/(a.length-1);
      return [m, Math.sqrt(Math.max(v,0))];
    }
    function idx(){ return parseInt(document.querySelector('input[name="g-which"]:checked').value,10)-1; }

    function compute(){
      r1v.textContent=(+r1.value).toFixed(2);
      r2v.textContent=(+r2.value).toFixed(2);
      r3v.textContent=(+r3.value).toFixed(2);
      poldv.textContent=(+pold.value).toFixed(2);
      pnewv.textContent=(+pnew.value).toFixed(2);
      prefv.textContent=(+pref.value).toFixed(2);
      epsv.textContent=(+eps.value).toFixed(2);
      betv.textContent=(+bet.value).toFixed(2);

      const R=[+r1.value,+r2.value,+r3.value];
      const [mu,sd]=sampleStd(R);
      const k=idx();
      const Ai = sd>0 ? (R[k]-mu)/sd : 0;

      const rt = (+pnew.value)/(+pold.value);
      const rtc = Math.max(1-(+eps.value), Math.min(1+(+eps.value), rt));
      const un = rt*Ai, cl = rtc*Ai;

      const ratioRef = (+pref.value)/(+pnew.value);
      const Dkl = ratioRef - Math.log(ratioRef) - 1;
      const L = Math.min(un,cl) - (+bet.value)*Dkl;

      meanOut.textContent=fmt(mu);
      stdOut.textContent=fmt(sd);
      AiOut.textContent=fmt(Ai);
      rtOut.textContent=fmt(rt);
      unOut.textContent=fmt(un);
      clOut.textContent=fmt(cl);
      klOut.textContent=fmt(Dkl);
      LOut.textContent=fmt(L);

      katex.render(String.raw`
        \begin{aligned}
        &\text{mean}=\frac{r_1+r_2+r_3}{3}
          =\frac{${(+r1.value).toFixed(2)}+${(+r2.value).toFixed(2)}+${(+r3.value).toFixed(2)}}{3}
          =${fmt(mu)} ,\quad \text{std}\approx ${fmt(sd)}\\[2pt]
        &A_i=\frac{${R[k].toFixed(2)}-${fmt(mu)}}{${fmt(sd)}}\approx ${fmt(Ai)}\\[4pt]
        &r_t=\frac{\pi_{\theta}}{\pi_{\text{old}}}
          =\frac{${(+pnew.value).toFixed(2)}}{${(+pold.value).toFixed(2)}}=${fmt(rt)},\ \ 
          \mathrm{clip}(r_t,\,1-${(+eps.value).toFixed(2)},\,1+${(+eps.value).toFixed(2)})=${fmt(rtc)}\\[2pt]
        &\text{unclipped}=r_tA_i=${fmt(rt)}\cdot${fmt(Ai)}=${fmt(un)},\quad
          \text{clipped}=${fmt(rtc)}\cdot${fmt(Ai)}=${fmt(cl)}\\[4pt]
        &D_{\mathrm{KL}}
          =\frac{${(+pref.value).toFixed(2)}}{${(+pnew.value).toFixed(2)}}
           -\log\!\left(\frac{${(+pref.value).toFixed(2)}}{${(+pnew.value).toFixed(2)}}\right)-1
          \approx ${fmt(Dkl)}\\[2pt]
        &\text{GRPO}=\min(${fmt(un)},\,${fmt(cl)})\;-\;${(+bet.value).toFixed(2)}\cdot${fmt(Dkl)}
          \approx \boxed{\,${fmt(L)}\,}
        \end{aligned}
      `, work, {displayMode:true, throwOnError:false});
    }

    ['input','change'].forEach(ev=>{
      [r1,r2,r3,pold,pnew,pref,eps,bet].forEach(el=>el.addEventListener(ev, compute));
      document.querySelectorAll('input[name="g-which"]').forEach(el=>el.addEventListener(ev, compute));
    });
    compute();
  }

  

    (function loadKaTeXOnce(cb){
    if (window.katex && window.renderMathInElement) return cb();
    function add(href, rel){
      const el = document.createElement(rel==='css'?'link':'script');
      if(rel==='css'){ el.rel='stylesheet'; el.href=href; }
      else { el.defer=true; el.src=href; }
      document.head.appendChild(el); return el;
    }
    if(!window.katex) add('https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css','css');
    const afterMain = ()=> add('https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js').onload = cb;
    if(!window.katex) add('https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js').onload = afterMain;
    else afterMain();
  })(initAllMath);

  function initAllMath(){
    initRewardPairwise();
    initValueAndPPO();
    initGRPO();
  }

</script>
</body>









<!-- DPO Interactive Visualization -->
<div id="dpo-visualization" style="width:100%;margin:2.5rem 0;padding:1.5rem 0;">
  <h2 style="text-align:center;">Direct Preference Optimization (DPO)</h2>


  <!-- ================= DPO GRAPH ================= -->
  <div class="wrap" id="dpo-flow">

    <style>
      #dpo-flow .edge{ stroke:#cbd5e1; stroke-width:3; fill:none; marker-end:url(#grpoArrow); }
      #dpo-flow .back{ stroke:#64748b; stroke-width:3; fill:none; stroke-dasharray:7 5; marker-end:url(#grpoArrow); }
      #dpo-flow .node{ filter:drop-shadow(0 2px 4px rgba(0,0,0,.08)); }
      #dpo-flow .rect-node{ fill:#e2e8f0; stroke:#0f172a; stroke-width:2; }
      #dpo-flow .model-node{ fill:#7dd3fc; stroke:#0f172a; stroke-width:2; }
      #dpo-flow .value-node{ fill:#fde68a; stroke:#0f172a; stroke-width:2; }
      #dpo-flow .small{ font-size:12px; fill:#334155; }
    </style>

    <svg id="dpoSvg">
      <!-- Human Preference -->
      <rect x="20" y="100" width="140" height="40" rx="6" class="node rect-node"/>
      <text x="90" y="125" text-anchor="middle" font-size="14" font-weight="700">Human Preference</text>

      <!-- Policy Model -->
      <ellipse cx="300" cy="120" rx="90" ry="36" class="node value-node"/>
      <text x="300" y="125" text-anchor="middle" font-size="14" font-weight="700">Policy Model</text>

      <!-- Preferred Response -->
      <rect x="500" y="70" width="140" height="40" rx="6" class="node rect-node"/>
      <text x="570" y="95" text-anchor="middle" font-size="14" font-weight="700">Preferred (y‚Å∫)</text>

      <!-- Dispreferred Response -->
      <rect x="500" y="150" width="140" height="40" rx="6" class="node rect-node"/>
      <text x="570" y="175" text-anchor="middle" font-size="14" font-weight="700">Dispreferred (y‚Åª)</text>

      <!-- DPO Loss -->
      <ellipse cx="750" cy="120" rx="70" ry="36" class="node model-node"/>
      <text x="750" y="125" text-anchor="middle" font-size="14" font-weight="700">DPO Loss</text>

      <!-- Paths -->
      <path id="dpo_path1" class="edge" d="M160 120 H 210" marker-end="url(#dpoArrow)"/>
      <path id="dpo_path2" class="edge" d="M390 120 H 410" marker-end="url(#dpoArrow)"/>
      <path id="dpo_path3" class="edge" d="M390 120 V 90 H 500" marker-end="url(#dpoArrow)"/>
      <path id="dpo_path4" class="edge" d="M390 120 V 150 H 500" marker-end="url(#dpoArrow)"/>
      <path id="dpo_path5" class="edge" d="M640 90 H 680" marker-end="url(#dpoArrow)"/>
      <path id="dpo_path6" class="edge" d="M640 170 H 680" marker-end="url(#dpoArrow)"/>

      <!-- Moving token -->
      <circle id="dpo_token" r="6" fill="#2563eb" opacity="0"/>
    </svg>
  </div>

  <script>
  (function(){
    const token = document.getElementById('dpo_token');

    const paths = [
      document.getElementById('dpo_path1'),
      document.getElementById('dpo_path2'),
      document.getElementById('dpo_path3'),
      document.getElementById('dpo_path4'),
      document.getElementById('dpo_path5'),
      document.getElementById('dpo_path6')
    ];

    let running = false, visible = false, i=0;

    function animateNext() {
      if (i >= paths.length) {
        running = false;
        i = 0;
        setTimeout(() => { if(visible) startAnimation(); }, 700);
        return;
      }

      const path = paths[i];
      const length = path.getTotalLength();
      token.setAttribute('opacity','1');
      let start = null;

      function step(timestamp) {
        if (!start) start = timestamp;
        const progress = (timestamp - start)/700; // 700ms per segment
        const point = path.getPointAtLength(Math.min(progress*length, length));
        token.setAttribute('cx', point.x);
        token.setAttribute('cy', point.y);
        if(progress < 1) {
          requestAnimationFrame(step);
        } else {
          i++;
          animateNext();
        }
      }
      requestAnimationFrame(step);
    }

    function startAnimation(){
      if(running) return;
      running = true;
      i=0;
      animateNext();
    }

    const wrap = document.getElementById('dpo-flow');
    const io = new IntersectionObserver(entries=>{
      entries.forEach(e=>{
        visible = e.isIntersecting;
        if(visible) startAnimation();
      });
    }, {threshold:0.25});
    io.observe(wrap);

    window.addEventListener('load', ()=>{
      const r = wrap.getBoundingClientRect();
      if(r.top < innerHeight && r.bottom > 0){
        visible = true;
        startAnimation();
      }
    });
  })();
  </script>




  <p style="font-size:1.07em;">
    <strong>DPO fine-tunes models directly on human preferences.</strong>
    Instead of training a separate reward model, DPO learns to choose the preferred answer out of a pair, optimizing the model so that preferred responses become more likely.<br>
  </p>
  <div style="display:flex;gap:32px;justify-content:space-between;align-items:center;">
    <!-- Preferred Response -->
    <div style="flex:1;text-align:center;">
      <div style="padding:10px 0 8px 0;">Preferred response<br><span style="font-size:1.1em;color:#15803d;">\(y^+\)</span></div>
      <input id="dpo-pplus" type="range" min="0.01" max="0.99" step="0.01" value="0.35" style="width:80%;">
      <div style="margin-top:6px;">Probability: <strong id="dpo-pplusv">0.35</strong></div>
    </div>
    <div style="flex:1;text-align:center;">
      <div style="padding:10px 0 8px 0;">Dispreferred response<br><span style="font-size:1.1em;color:#b91c1c;">\(y^-\)</span></div>
      <input id="dpo-pminus" type="range" min="0.01" max="0.99" step="0.01" value="0.13" style="width:80%;">
      <div style="margin-top:6px;">Probability: <strong id="dpo-pminusv">0.13</strong></div>
    </div>
  </div>
  <div style="margin:1.2em 0 0 0;text-align:center;">
    <label for="dpo-beta">\(\beta\) (preference sharpness): <strong id="dpo-betav">1.0</strong></label>
    <input id="dpo-beta" type="range" min="0.1" max="4.0" step="0.01" value="1.0" style="width:50%;vertical-align:middle;">
  </div>
  <div style="margin:1.2em 0 0 0; font-size:1.08em;background:#fffbe9;border-radius:9px;padding:0.7em 1em;color:#222;">
    <div style="flex:1;text-align:center;">
      Log probability difference:<br>
      \(\Delta = \log \pi_\theta(y^+) - \log \pi_\theta(y^-)\) = <span id="dpo-delta">0.9808</span>
    </div>
    <div style="flex:1;text-align:center;">
      DPO loss:<br>
      \(-\log \left( \frac{e^{\beta \Delta}}{e^{\beta \Delta} + 1} \right)\) = <span id="dpo-loss">0.3125</span>
    </div>
  </div>
  <p style="margin-top:1em;font-size:1.03em;color:#444;text-align:center">
    <strong>Goal:</strong> Maximize the difference between preferred and dispreferred log probabilities.<br>
    The loss shrinks as the model makes the preferred response much more likely than the dispreferred one.
  </p>
</div>

<script>
const $ = id => document.getElementById(id);
function fmt(x, d=4) { return Number(x).toFixed(d); }
function updateDPO() {
  const pp = +$('dpo-pplus').value, pm = +$('dpo-pminus').value, beta = +$('dpo-beta').value;
  $('dpo-pplusv').textContent = fmt(pp,2);
  $('dpo-pminusv').textContent = fmt(pm,2);
  $('dpo-betav').textContent = fmt(beta,2);
  // Log probability difference
  const delta = Math.log(pp) - Math.log(pm);
  $('dpo-delta').textContent = fmt(delta,4);
  // DPO Loss
  const num = Math.exp(beta * delta);
  const denom = num + 1;
  const loss = -Math.log(num/denom);
  $('dpo-loss').textContent = fmt(loss,4);
}
['input','change'].forEach(ev=>{
  ['dpo-pplus','dpo-pminus','dpo-beta'].forEach(id=>$(id).addEventListener(ev,updateDPO));
});
updateDPO();
</script>


<!-- ================= Interactive Preference Table ================= -->
<div class="wrap" style="max-width:1200px;margin:2rem auto;padding:1rem 1.5rem;">
  <h2 style="text-align:center;margin-bottom:1.5rem;">Recap: Alignment Strategies</h2>

  <table style="width:100%;border-collapse:collapse;font-size:1.03em;">
    <thead>
      <tr style="text-align:left;border-bottom:2px solid #cbd5e1;">
        <th style="padding:0.6rem 1rem;">Method</th>
        <th style="padding:0.6rem 1rem;">Learn to ‚Ä¶</th>
        <th style="padding:0.6rem 1rem;">Models Involved</th>
        <th style="padding:0.6rem 1rem;">Details</th>
      </tr>
    </thead>
    <tbody>
      <tr class="hover-row">
        <td>SFT</td>
        <td>predict high quality response</td>
        <td class="details">base model</td>
        <td class="details">fine-tunes on input-output pairs; no preference modeling; efficient but alignment limited</td>
      </tr>
      <tr class="hover-row">
        <td>PPO</td>
        <td>maximize expected reward under preferences</td>
        <td class="details">policy model, reward model (scalar), value model</td>
        <td class="details">classic RLHF setup; uses trained reward model for policy learning with value function</td>
      </tr>
      <tr class="hover-row">
        <td>GRPO</td>
        <td>rank output group-wise over multiple completions</td>
        <td class="details">policy model, reward model (ranking-based)</td>
        <td class="details">compare candidate responses; ranking replaces scalar reward signal and model</td>
      </tr>
      <tr class="hover-row">
        <td>DPO</td>
        <td>produce preferred responses</td>
        <td class="details">policy model</td>
        <td class="details">requires preference data; optimizes policy model directly</td>
      </tr>
    </tbody>
  </table>
</div>

<style>
.hover-row .details {
  opacity: 0;
  max-height: 0;
  overflow: hidden;
  transition: all 0.3s ease;
}
.hover-row:hover .details {
  opacity: 1;
  max-height: 100px;
}
.hover-row td {
  padding: 0.8rem 1rem;
  border-bottom: 1px solid #e2e8f0;
  transition: background 0.3s;
}
.hover-row:hover {
  background: #f1f5f9;
}
</style>

<div style="max-width:1000px;margin:2rem auto;font-size:1.05em;color:#333;text-align:center;line-height:1.5;">
  <strong>From simple instruction tuning to advanced preference alignment:</strong> these methods show how LLMs are trained not just to compute, but to reason in ways that align with human intent.
</div>




















<!-- Rodion's part - Reasoning Architecture, Benchmarks -->

<head>


  <title>Reasoning Search Flow</title>
  <style>
    :root { --bg:#f8fafc; --ink:#111827; --arrow:#94a3b8; --accent:#fb923c; }
    body{margin:0;font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial;background:var(--bg);color:var(--ink)}
    .wrap{width:100%;margin:24px auto;padding:0}
    .panel{ display:flow-root; margin-bottom:0; }
    .hint{color:#475569;font-size:14px;margin-top:8px;text-align:center}
    .token{filter:drop-shadow(0 1px 2px rgba(0,0,0,.35));offset-rotate:0deg}
    .sub{ margin: 6px 0 8px; }
    #scene{ display:block; margin:0; }
    @keyframes move{from{offset-distance:0%}to{offset-distance:100%}}
  </style>


  <title>Best-of-N Search</title>
  <style>
    :root{--bg:#f8fafc;--ink:#0f172a;--arrow:#cbd5e1;--good:#22c55e;--bad:#f43f5e;--node:#bae6fd;--token:#3b82f6}
    body{margin:0;background:var(--bg);color:var(--ink);font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial}
    h1{margin:18px 0 6px;text-align:center;font-size:22px}
    .wrap{max-width:1200px;margin:0 auto;padding:0 12px 16px}
    svg{display:block;width:100%;height:380px}
    .node{filter:drop-shadow(0 2px 4px rgba(0,0,0,.08))}
    .token{r:10;fill:var(--token);filter:drop-shadow(0 1px 2px rgba(0,0,0,.35))}
    @keyframes travel{from{offset-distance:0%}to{offset-distance:100%}}
    .pulse{animation:pulse 1.2s ease-in-out infinite}
    @keyframes pulse{0%{opacity:.6}50%{opacity:1}100%{opacity:.6}}
    button.run-btn{display:block;margin:0 auto 12px;padding:8px 16px;font-size:14px;border:1px solid #d1d5db;border-radius:6px;background:#fff;cursor:pointer}
    button.run-btn:hover{background:#f3f4f6}
    /* Unfold animation for arrows */
    .edge{stroke:var(--arrow);stroke-width:3;fill:none}
    .edge.draw{transition:stroke-dashoffset 800ms ease}
    .fin{opacity:0;transform-box:fill-box;transform-origin:center;}
    .fin.show{opacity:1;transition:opacity .4s ease, transform .4s ease;transform:scale(1.05)}
  </style>

  <title>Beam Search</title>
  <style>
    :root{
      --bg:#f8fafc; --ink:#0f172a; --muted:#64748b;
      --arrow:#cbd5e1; --node:#bae6fd; --token:#3b82f6;
      --good:#22c55e; --bad:#f43f5e;
    }
    body{margin:0;background:var(--bg);color:var(--ink);
        font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial}
    h1{margin:18px 0 6px;text-align:center;font-size:22px}
    .sub{margin:0 0 8px;text-align:center;color:var(--muted);font-size:14px}
    .wrap{max-width:1200px;margin:0 auto;padding:0 12px 16px}
    svg{display:block;width:100%;height:420px}
    .node{filter:drop-shadow(0 2px 4px rgba(0,0,0,.08))}
    .edge{stroke:var(--arrow);stroke-width:3;fill:none}
    .edge.draw{transition:stroke-dashoffset 700ms ease}
    .mid{fill:#e5e7eb}
    .mid.good{fill:var(--good)}
    .mid.bad{fill:var(--bad);opacity:.9}
    .leaf{fill:#e5e7eb}
    .leaf.good{fill:var(--good)}
    .leaf.bad{fill:var(--bad);opacity:.95}
    .pulse{animation:pulse 1.2s ease-in-out infinite}
    @keyframes pulse{0%{opacity:.65}50%{opacity:1}100%{opacity:.65}}
    @keyframes travel{from{offset-distance:0%}to{offset-distance:100%}}
  </style>

  <title>Diverse Verifier Tree Search ‚Äî Dynamic Visualization</title>
    <style>
      :root{
        --bg:#f8fafc; --ink:#0f172a; --muted:#64748b;
        --arrow:#cbd5e1; --node:#bae6fd; --token:#3b82f6;
        --good:#22c55e; --bad:#f43f5e;
      }
      body{margin:0;background:var(--bg);color:var(--ink);
          font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial}
      h1{margin:18px 0 6px;text-align:center;font-size:22px}
      .sub{margin:0 0 8px;text-align:center;color:var(--muted);font-size:14px}
      .wrap{max-width:1200px;margin:0 auto;padding:0 12px 16px}
      svg{display:block;width:100%;height:420px}
      .node{filter:drop-shadow(0 2px 4px rgba(0,0,0,.08))}
      .edge{stroke:var(--arrow);stroke-width:3;fill:none}
      .edge.draw{transition:stroke-dashoffset 700ms ease}
      .token{fill:var(--token);r:8;filter:drop-shadow(0 1px 2px rgba(0,0,0,.35))}
      .mid{r:10}
      .mid.good{fill:var(--good)}
      .mid.bad{fill:var(--bad);opacity:.9}
      .leaf{r:12}
      .leaf.good{fill:var(--good)}
      .leaf.bad{fill:var(--bad);opacity:.95}
      .pulse{animation:pulse 1.2s ease-in-out infinite}
      @keyframes pulse{0%{opacity:.65}50%{opacity:1}100%{opacity:.65}}
      @keyframes travel{from{offset-distance:0%}to{offset-distance:100%}}
  </style>

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Strategies Performance Comparison</title>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <style>
    body{margin:0;font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Helvetica Neue,Arial;background:#f8fafc;color:#0f172a}
    h1{text-align:center;margin:18px 0 6px;font-size:22px}
    #chart{display:block;margin:0 auto;max-width:900px}
    .axis path,.axis line{stroke:#94a3b8}
    .baseline{stroke:#475569;stroke-dasharray:6 6;opacity:.8}
    .legend text{font-size:12px;dominant-baseline:middle;fill:#0f172a}
  </style>

  <meta charset="UTF-8">
  <title>Benchmarks</title>
  <!-- DataTables + jQuery -->
  <link rel="stylesheet" href="https://cdn.datatables.net/1.13.4/css/jquery.dataTables.min.css">
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.datatables.net/1.13.4/js/jquery.dataTables.min.js"></script>
  
  <meta charset="UTF-8">
  <style>
    .hanoi-section {
      font-family: sans-serif;
      background: #fefefe;
      padding: 2rem;
      text-align: center;
    }
    .peg-container {
      display: flex;
      justify-content: space-around;
      align-items: flex-end;
      height: 200px;
      margin-top: 3rem;
    }
    .peg {
      width: 10px;
      height: 150px;
      background: #333;
      position: relative;
    }
    .disk {
      position: absolute;
      height: 20px;
      border-radius: 5px;
      left: 50%;
      transform: translateX(-50%);
    }
    .disk1 { width: 100px; background: #FF6B6B; }
    .disk2 { width: 70px; background: #4ECDC4; }
    .disk3 { width: 40px; background: #1A535C; }

    .peg-label {
      margin-top: 1rem;
      font-weight: bold;
    }
  </style>




<!-- Here was the R1 STUFF BEFORE-->

    <div class="wrap">
        <div class="intro-section">
            <h2>The R1 Breakthrough: Emergent Reasoning Through Reinforcement Learning</h2>
            
            <p>
                All the techniques explained above are used and combined nowadays to create state of the art reasoning models.
                Perhaps one of the most emblematic reasoning model releases is the R1 series, with their accompanying paper:
                <a href="https://arxiv.org/pdf/2501.12948">"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning" (2025)</a>
                By analzying their research, we hope it is clearer how this techniques are used in practice.
              </p>
            
            <h3>R1 Series Development</h3>
            <p>
              The DeepSeek team worked with three different approaches for training reasoning models.
              With just Reinforcement Learning on Chain-of-Thought data, they trained DeepSeek-R1-Zero, a model that surpassed similar sized models in reasoning.
              But after proving the relevancy of Reinforcement Learning, the researchers  noticed some problems in the output of DeepSeek-R1-Zero,
              mostly related to readability and language mixing. So while the model would reason, it did so in ways humans could not always interpret.
              Therefore, a new training paradigm with extra steps was created to train the DeepSeek-R1 model, specifically designed to prevent these issues while keeping the improved reasoning capabilities.
              The trainig scheme included Supervised-Fine-Tuning, two stages of Reinforcement Learning, and Rejection Sampling.
              In addition to these two approaches, they also described the process of distillation, 
              where the output of a larger and more complex LLM is used to train a smaller LLM to improve its reasoning capabilities. 
            </p>


            <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>How R1 Models are Trained</title>
        <style>
        .wrap {
            margin-top: 0px;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .switcher {
            text-align: center;
            margin: 20px 0;
        }
        
        .switcher button {
            background: #3b82f6;
            color: white;
            border: none;
            padding: 10px 20px;
            margin: 0 10px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 16px;
        }
        
        .switcher button:hover {
            background: #2563eb;
        }
        
        .flow-wrap {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 20px 0;
        }
        
        #r1SVG {
            max-width: 100%;
            height: auto;
        }
        
        .label {
            text-align: center;
            font-weight: 600;
            margin: 10px 0;
            font-size: 18px;
        }
        
        #r1Stages, #r1zerostages {
            max-width: 720px;
            margin: 20px auto;
            line-height: 1.6;
            display: none;
        }
        
        #r1Stages h3, #r1zerostages h3 {
            margin-bottom: 20px;
            color: #1f2937;
        }
        
        #r1Stages p, #r1zerostages p {
            margin-bottom: 15px;
            color: #374151;
        }
        
        #r1Stages strong, #r1zerostages strong {
            color: #111827;
        }
    </style>

    <div class="wrap" id="r1-flow">
        <h2 style="text-align:center;">How R1 Models are Trained</h2>
        <div class="switcher">
            <button onclick="showR1('zero')">R1-Zero</button>
            <button onclick="showR1('r1')">R1</button>
        </div>

        <div class="flow-wrap">
            <svg id="r1SVG" viewBox="0 0 950 140" width="850" height="140">
                <!-- Will be filled dynamically -->
            </svg>
        </div>
        <div id="r1Label" class="label"></div>

        <div id="r1Stages">
            <h3>R1 Training Stages</h3>

            <p><strong>Stage 1 ‚Äì Cold Start (SFT with CoT)</strong><br>
            We fine-tune the base model on a small set of high-quality chain-of-thought examples.
            This avoids unstable early RL and improves initial readability and structure.
            Outputs follow a consistent format with reasoning + summary.</p>

            <p><strong>Stage 2 ‚Äì Reasoning-Oriented RL (GRPO)</strong><br>
            We apply GRPO to improve performance on math, logic, and science tasks.
            The reward combines task accuracy and language consistency to encourage readable, correct outputs.</p>

            <p><strong>Stage 3 ‚Äì Rejection Sampling + SFT</strong><br>
            We sample outputs from the RL model and keep only the best completions.
            We add non-reasoning tasks to improve general capabilities like writing and QA.
            This results in a cleaner, broader training set.</p>

            <p><strong>Stage 4 ‚Äì RL for All Scenarios</strong><br>
            We apply another RL phase to align the model with human preferences.
            We focus on helpfulness (using the summary) and harmlessness (using the full output).
            This final stage refines both reasoning and safety across diverse prompts.</p>
        </div>

        <div id="r1zerostages">
            <h3>R1-Zero Training Process</h3>
            <p><strong>Unique Training Stage</strong><br>
            We only use Reinforcement Learning to improve the base model, 
            allowing the exploration of Chain-of-Thought when faced with difficult tasks.
            R1 proved to researchers that Reinforcement Learning is a powerful enough technique
            to improve the reasoninng capabilities of LLMs by itself, without the need of SFT.
          </p>
        </div>
    </div>

    <script>
    function showR1(mode) {
        const svg = document.getElementById('r1SVG');
        svg.innerHTML = "";
        const r1stages = document.getElementById("r1Stages");
        const r1zerostages = document.getElementById("r1zerostages");

        if (mode === "zero") {
            r1stages.style.display = "none";
            r1zerostages.style.display = "block";
            svg.setAttribute("viewBox", "0 0 950 140");
            svg.setAttribute("height", "140");

           // R1-Zero training: Policy model trained via RL, no SFT warmup
            svg.innerHTML = `
                <rect x="275" y="40" width="160" height="60" rx="16" fill="#bae6fd"/>
                <rect x="515" y="40" width="200" height="60" rx="16" fill="#bbf7d0"/>

                <text x="355" y="75" text-anchor="middle" font-size="18" font-weight="600">Base LLM</text>
                <text x="615" y="75" text-anchor="middle" font-size="18" font-weight="600">RL Training</text>

                <defs>
                    <marker id="r1Arrow" markerWidth="12" markerHeight="12" refX="8" refY="4" orient="auto">
                        <path d="M0,0 L12,4 L0,8 Z" fill="#60a5fa"/>
                    </marker>
                </defs>
                <path d="M435,70 H515" stroke="#60a5fa" stroke-width="4" />
            `;
            document.getElementById("r1Label").textContent =
                "R1-Zero is trained directly with Reinforcement Learning.";
        } else {
            r1stages.style.display = "block";
            r1zerostages.style.display = "none";
            svg.setAttribute("viewBox", "0 0 950 140");
            svg.setAttribute("height", "140");

            // R1 training: staged approach
            svg.innerHTML = `
                <rect x="20" y="40" width="180" height="60" rx="14" fill="#bfdbfe"/>
                <rect x="240" y="40" width="200" height="60" rx="14" fill="#bbf7d0"/>
                <rect x="460" y="40" width="220" height="60" rx="14" fill="#fde68a"/>
                <rect x="720" y="40" width="200" height="60" rx="14" fill="#fca5a5"/>

                <text x="110" y="65" text-anchor="middle" font-size="14" font-weight="600">Step 1:</text>
                <text x="110" y="80" text-anchor="middle" font-size="14" font-weight="600">SFT with CoT</text>
                
                <text x="340" y="65" text-anchor="middle" font-size="14" font-weight="600">Step 2:</text>
                <text x="340" y="80" text-anchor="middle" font-size="14" font-weight="600">RL for Reasoning</text>
                
                <text x="570" y="60" text-anchor="middle" font-size="14" font-weight="600">Step 3:</text>
                <text x="570" y="75" text-anchor="middle" font-size="13" font-weight="600">Rejection Sampling</text>
                <text x="570" y="88" text-anchor="middle" font-size="13" font-weight="600">+ SFT</text>
                
                <text x="820" y="65" text-anchor="middle" font-size="14" font-weight="600">Step 4:</text>
                <text x="820" y="80" text-anchor="middle" font-size="14" font-weight="600">RL for Alignment</text>

                <defs>
                    <marker id="r1ArrowBlue" markerWidth="12" markerHeight="12" refX="8" refY="4" orient="auto">
                        <path d="M0,0 L12,4 L0,8 Z" fill="#3b82f6"/>
                    </marker>
                </defs>
                <path d="M200,70 H240" stroke="#60a5fa" stroke-width="3" />
                <path d="M440,70 H460" stroke="#60a5fa" stroke-width="3" />
                <path d="M680,70 H720" stroke="#60a5fa" stroke-width="3" />
            `;
            document.getElementById("r1Label").textContent =
                "R1 is trained in four stages: SFT, RL, rejection sampling, and RL again.";
        }
    }

    // Initialize with R1 view
    showR1("r1");
    </script>
    <h3>Evidence of Emergent Reasoning</h3>
            <p>
                The most compelling evidence of R1's success is how the model discovered to "think longer" about difficult problems. During training, something remarkable happened that wasn't explicitly programmed:
            </p>
            
            <figure style="text-align:center; max-width: 1000px; margin: 2rem auto;">
            <img src="R1_Graph.jpg" alt="Average length per response during training" style="width: 100%; height: auto; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">

            <figcaption style="margin-top: 0.75rem; font-size: 0.95rem; color: #1e293b;">
                <strong>Figure:</strong> DeepSeek-R1-Zero average response length during training.
                Notice how the model spontaneously learns to generate longer, more detailed reasoning as training progresses ‚Äì from ~500 tokens initially to nearly 10,000 tokens,
                indicating deeper chain-of-thought reasoning without explicit instruction. Figure from DeepSeek-AI et al, 2025
            </figcaption>
            </figure>
            
            <div class="highlight-box">
                <p>
                    <strong>Key Insight:</strong> The model wasn't told to write longer responses. 
                    It discovered on its own that complex problems require more detailed thinking, improving its accuracy.
                </p>
            </div>
            
            <h3>What This Breakthrough Means</h3>
            <p>
            The training lead to some interesting findings. The extra length of the answers also came with an emergence of reasoning behaviours
            similar to that of humans. The LLM was able to realize its apporach to solve some problems was mistaken, leading to self-correction
            and an exploration of alternative approaches to solve the issues.
            with no need of human intervention.  
            </p>

        </div>
    </div>

  <div class="wrap loss2-sec" id="distillation-text">
    <h2>Distillation</h2>

    <p>
      Distillation transfers skills from a larger teacher model to a smaller student model, 
      the idea is to use the strong model to create data, then let a compact model learn from it.
      In the DeepSeek-R1 paper the authors use a large reasoning model to create a CoT dataset, which is then used to
      finetune a smaller model using SFT with the goal of transferring the reasoning abilities to the smaller 
      model and demonstrating the effects of distillation. Although the authors did not use any RL for this distillation task, they speculate that doing so could have improved the 
      model‚Äôs performance.
    </p>

    <p>
      As we can see in the animation the output of an explicit reasoning model is used to create 
      an implicit reasoning model.
    </p>


  </div>




  <section style="margin-top:60px; margin-bottom:40px;">
    <h2 style="
      font-size:2em;
      font-weight:700;
      letter-spacing:0.01em;
      margin-bottom:6px;
      background:linear-gradient(90deg, #010610 70%, #eab308 100%);
      -webkit-background-clip:text;
      -webkit-text-fill-color:transparent;
      background-clip:text;
      text-align:center;
      ">
      Reasoning architectures
    </h2>
  </section>

  <div class="flexrow">
    
    <div class="summary">

      <p>
        While DeepSeek R1 is a dedicated reasoning model trained with chain-of-thought data and methods such as
        supervised fine-tuning and reinforcement learning, it's also possible to build a reasoning architecture around a
        standard language model.
          </p>

      <p>
        In this setup, an LLM (e.g., a Llama 1B or 8b model) produces multiple candidate answers. A reward model then
        scores these candidates, and a search strategy selects the best answer based on the rewards. This kind of architecture is described 
        in the Hugging Face cookbook:
        <a href="https://huggingface.co/learn/cookbook/search_and_learn" target="_blank" rel="noopener">‚ÄúSearch &amp; Learn‚Äù</a>.
          </p>

      <p>
        The dynamic chart below visualizes the architecture: the LLM proposes candidates, the reward model scores them, and the search strategy
        steers the loop toward the final answer.
          </p>
    </div>

    <div class="bubble right">
      <p>
        To reproduce the pipeline one can use the Search-and-Learn repository: <a href="https://github.com/huggingface/search-and-learn" 
        target="_blank" rel="noopener">huggingface/search-and-learn</a>.</p> 
      <p>
        Clone the repository, install dependencies as described in the repo, follow the 
        pipeline guide in the cookbook: <a href="https://huggingface.co/learn/cookbook/search_and_learn" target="_blank" rel="noopener">
        Search &amp; Learn tutorial</a>. </p>
    </div>
  </div>
  

<div class="wrap">
  <h3 style="text-align:center">Reasoning Search Flow</h3>
  <div class="panel">
    <p class="sub">Gets a problem ‚Üí produce answers ‚Üí score answers - choose the best with the strategy - give final answer.</p>
    <svg id="scene"
     viewBox="0 0 1200 350"
     width="100%"
     height="350"
     preserveAspectRatio="xMidYMin meet">
      <defs>
        <marker id="arrow" markerWidth="10" markerHeight="10" refX="8" refY="3" orient="auto" markerUnits="userSpaceOnUse">
          <path d="M0,0 L9,3 L0,6 Z" fill="var(--arrow)" />
        </marker>
        <filter id="soft" x="-50%" y="-50%" width="200%" height="200%">
          <feGaussianBlur in="SourceGraphic" stdDeviation="3" />
        </filter>
      </defs>

      <script type="application/json" id="points">{
        "math": {"x":90,  "y":320},
        "llm":  {"x":400, "y":260},
        "prm":  {"x":700, "y":260},
        "search":{"x":620, "y":100 },
        "final": {"x":1070,"y":350}
      }</script>

      <g id="edges" stroke="var(--arrow)" stroke-width="3" marker-end="url(#arrow)" opacity=".65" fill="none"></g>
      <g id="nodes"></g>
      <circle cx="620" cy="100" r="62" fill="var(--accent)" opacity=".15" filter="url(#soft)" />
      <g id="tokenLayer"></g>
    </svg>
  </div>
</div>

<script>
(function(){
  const P = JSON.parse(document.getElementById('points').textContent);
  const svgNS = 'http://www.w3.org/2000/svg';
  const rightOf = (p)=>[p.x+90,p.y];
  const leftOf  = (p)=>[p.x-90,p.y];
  const topOf   = (p)=>[p.x,p.y-40];
  const C = (x1,y1,x2,y2,x,y)=>`C ${x1} ${y1}, ${x2} ${y2}, ${x} ${y}`;
  const seg=(from,to,bendY)=>{const midX=(from[0]+to[0])/2,by=bendY??from[1];return `M ${from[0]} ${from[1]} ${C(midX,by,midX,by,to[0],to[1])}`};

  const paths={
    mathToLLM: seg(rightOf(P.math), leftOf(P.llm)),
    llmToPRM: seg(rightOf(P.llm), leftOf(P.prm)),
    prmToSearch: seg(rightOf(P.prm), [P.search.x, P.search.y], P.prm.y - 80),
    searchToLLM: seg([P.search.x-90,P.search.y],[P.llm.x,P.llm.y-40], P.search.y-30),
    prmToFinal: seg(rightOf(P.prm), leftOf(P.final))
  };

  function addNode(group,x,y,label,fill){
    const g=document.createElementNS(svgNS,'g');
    g.setAttribute('transform',`translate(${x-90},${y-40})`);
    const rect=document.createElementNS(svgNS,'rect');
    rect.setAttribute('rx','18');rect.setAttribute('width','180');rect.setAttribute('height','80');
    rect.setAttribute('fill',fill);rect.setAttribute('style','filter:drop-shadow(0 1px 2px rgba(0,0,0,.15))');
    const text=document.createElementNS(svgNS,'text');
    text.setAttribute('x','90');text.setAttribute('y','44');text.setAttribute('text-anchor','middle');
    text.setAttribute('fill','#111827');text.setAttribute('style','font-weight:600');
    text.textContent=label;
    g.appendChild(rect);g.appendChild(text);group.appendChild(g);
  }

  const nodesG=document.getElementById('nodes');
  addNode(nodesG,P.math.x,P.math.y,'Input problem','#bae6fd');
  addNode(nodesG,P.llm.x,P.llm.y,'LLM','#d8b4fe');
  addNode(nodesG,P.prm.x,P.prm.y,'PRM','#fef08a');
  addNode(nodesG,P.search.x,P.search.y,'Search strategy','#fdba74');
  addNode(nodesG,P.final.x,P.final.y,'Final answer','#bae6fd');

  const edgesG=document.getElementById('edges');
  Object.values(paths).forEach(d=>{const path=document.createElementNS(svgNS,'path');path.setAttribute('d',d);edgesG.appendChild(path);});

  const tokens=[
    {d:paths.mathToLLM, delay:0.0, color:'#3b82f6'},
    {d:paths.llmToPRM, delay:0.0, color:'#7c3aed'},
    {d:paths.prmToSearch, delay:0.0, color:'#f59e0b'},
    {d:paths.searchToLLM, delay:0.0, color:'#10b981'},
    {d:paths.prmToFinal, delay:0.0, color:'#22c55e'}
  ];

  const tokenLayer=document.getElementById('tokenLayer');
  tokens.forEach(t=>{
    const c=document.createElementNS(svgNS,'circle');
    c.setAttribute('r','10');
    c.setAttribute('fill',t.color);
    c.classList.add('token');
    c.style.offsetPath=`path('${t.d}')`;
    c.style.animation=`move 4s linear ${t.delay}s infinite`;
    c.style.animationPlayState='running';
    tokenLayer.appendChild(c);
  });
})();
</script>

<!--HERE WAS REASONING STRUCTURES-->

<h2 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Search Strategies</h2>

  <p>There are several search strategies to select the best answer including Best-of-N, Beam Search and Diverse Verifier Tree. Let's have a closer look at them</p>

<h3 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Best-of-N</h3>

  <p>The Best-of-N strategy is a simple but powerful way to improve the quality of responses from a language model. </p>
    
    <p>Instead of relying on a single output, the model generates N different candidate answers to the same problem. </p>
    <p>Each candidate is then evaluated to measure how good or useful it is. Finally, the system selects the candidate with 
    the highest score.</p>

<div class="wrap">
  <h3 style="text-align:center">Best-of-N Search</h3>
  <p class="sub">Generate N candidate answers ‚Üí score each ‚Üí select the one with the highest reward.</p>
  <svg id="bon" viewBox="0 0 1200 400"></svg>
</div>

<script>
function startViz(){
  const N = 5;                 // number of candidates
  const CYCLE_S = 4;           // seconds per winner refresh
  const DRAW_MS = 800;         // ms it takes to draw one arrow
  const STAGGER_MS = 180;      // ms between arrow starts

  const svg = document.getElementById('bon');
  svg.innerHTML = '';
  const NS = 'http://www.w3.org/2000/svg';

  // Layout
  const cx = 600, cy = 80;            // math problem center
  const leafY = 300;                   // y for final answers
  const span = 700;                    // horizontal spread for leaves
  const leftX = cx - span/2, step = span/(N-1);

  // Problem node
  const gProb = document.createElementNS(NS,'g');
  gProb.setAttribute('transform',`translate(${cx-80},${cy-28})`);
  const rProb = document.createElementNS(NS,'rect');
  rProb.setAttribute('width',160); rProb.setAttribute('height',56); rProb.setAttribute('rx',18); rProb.setAttribute('fill','var(--node)'); rProb.setAttribute('class','node');
  const tProb = document.createElementNS(NS,'text'); tProb.setAttribute('x',80); tProb.setAttribute('y',34); tProb.setAttribute('text-anchor','middle'); tProb.textContent='Math problem';
  gProb.appendChild(rProb); gProb.appendChild(tProb); svg.appendChild(gProb);

  const finals = [], balls = [], edges = [];

  for(let i=0;i<N;i++){
    const x = leftX + i*step;
    const start = [cx, cy+28];
    const end = [x, leafY];
    const midX = (start[0]+end[0])/2;
    const bendY = cy + 40;
    const d = `M ${start[0]} ${start[1]} C ${midX} ${bendY}, ${midX} ${bendY}, ${end[0]} ${end[1]}`;

    // Edge path (to be drawn later)
    const p = document.createElementNS(NS,'path');
    p.setAttribute('d', d); p.setAttribute('class','edge draw');
    svg.appendChild(p); edges.push(p);

    // Final nodes (show after edge draws)
    const fin = document.createElementNS(NS,'circle');
    fin.setAttribute('cx', end[0]); fin.setAttribute('cy', end[1]); fin.setAttribute('r', 14); fin.setAttribute('fill', '#e5e7eb');
    fin.setAttribute('class','fin');
    svg.appendChild(fin); finals.push(fin);

    // Moving token (start after its edge finishes drawing)
    const ball = document.createElementNS(NS,'circle');
    ball.setAttribute('class','token');
    ball.style.offsetPath = `path('${d}')`;
    svg.appendChild(ball); balls.push(ball);
  }

  // Animate drawing of each edge with stroke-dasharray
  edges.forEach((path, idx)=>{
    const len = path.getTotalLength();
    path.style.strokeDasharray = `${len}`;
    path.style.strokeDashoffset = `${len}`; // hidden
    const startAt = idx*STAGGER_MS;
    setTimeout(()=>{
      path.style.strokeDashoffset = '0'; // draw
      // show final node
      finals[idx].classList.add('show');
      // start the token along this path slightly after drawing begins
      const delayForBall = 0.65*DRAW_MS; // ms
      setTimeout(()=>{
        balls[idx].style.animation = `travel ${Math.max(1200, DRAW_MS*2)/1000}s linear 0s infinite`;
      }, delayForBall);
    }, startAt);
  });

  function pickWinner(){
    const winner = Math.floor(Math.random()*N);
    finals.forEach((f,i)=>{
      if(i===winner){ f.setAttribute('fill','var(--good)'); f.classList.add('pulse'); }
      else { f.setAttribute('fill','var(--bad)'); f.classList.remove('pulse'); }
    });
  }

  // first pick after all edges have drawn once
  const totalDrawTime = (N-1)*STAGGER_MS + DRAW_MS + 200;
  setTimeout(()=>{ pickWinner()}, totalDrawTime);
}

// ---- Auto-start when the chart scrolls into view ----
document.addEventListener('DOMContentLoaded', () => {
  const target = document.getElementById('bon');
  if (!('IntersectionObserver' in window)) {
    // Fallback: start immediately
    startViz();
    return;
  }
  const obs = new IntersectionObserver(
    entries => {
      if (entries[0].isIntersecting) {
        startViz();
        obs.disconnect(); // run only once
      }
    },
    { threshold: 0.25 }
  );
  obs.observe(target);
});
</script>

<h3 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Beam Search</h3>

  <p>Beam Search is a structured search strategy designed to explore multiple solution paths in parallel. </p>
    
    <p>The model generates several possibilities, known as beams.</p>
    <p>At each level, these beams are evaluated and only the top-scoring ones are retained for further expansion.</p>
   <p>This pruning ensures the search stays tractable while still covering a diverse set of promising candidates.</p>


<div class="wrap">
  <h1>Beam Search</h1>
  <p class="sub">N beams ‚Üí keep top-M mid-steps ‚Üí expand only those ‚Üí pick best final answer.</p>
  <svg id="beam" viewBox="0 0 1200 400"></svg>
</div>

<script>
function startBeam(){
  const NS  = 'http://www.w3.org/2000/svg';
  const svg = document.getElementById('beam');
  if (!svg || svg.dataset.started === '1') return; // guard: run once
  svg.dataset.started = '1';
  svg.innerHTML = '';

  // ---- Tunables ----
  const N = 5;            // first-layer beams
  const M = 2;            // beam width (survivors at intermediate)
  const CHILDREN = 2;     // children per surviving mid-step
  const DRAW_MS = 700;    // time to draw one edge
  const STAGGER_MS = 150; // delay between sibling edges
  const BALL_MS = 1400;   // token travel time per edge

  // ---- Layout ----
  const root  = {x:600, y:70};
  const y1    = 180;      // intermediate layer
  const y2    = 320;      // leaves
  const span1 = 560;      // spread for first layer
  const span2 = 360;      // spread per subtree

  // Root node
  const gRoot = document.createElementNS(NS,'g');
  gRoot.setAttribute('transform',`translate(${root.x-80},${root.y-28})`);
  const r = document.createElementNS(NS,'rect');
  r.setAttribute('width',160); r.setAttribute('height',56); r.setAttribute('rx',18);
  r.setAttribute('fill','var(--node)'); r.setAttribute('class','node');
  const t = document.createElementNS(NS,'text');
  t.setAttribute('x',80); t.setAttribute('y',34); t.setAttribute('text-anchor','middle');
  t.textContent = 'Math problem';
  gRoot.appendChild(r); gRoot.appendChild(t); svg.appendChild(gRoot);

  // Helpers
  const pathD = (x1,y1,x2,y2) => {
    const midX = (x1+x2)/2, bendY = (y1+y2)/2;
    return `M ${x1} ${y1} C ${midX} ${bendY}, ${midX} ${bendY}, ${x2} ${y2}`;
  };
  const drawEdge = (d) => {
    const p = document.createElementNS(NS,'path');
    p.setAttribute('d', d); p.setAttribute('class','edge draw'); svg.appendChild(p);
    const len = p.getTotalLength();
    p.style.strokeDasharray = `${len}`;
    p.style.strokeDashoffset = `${len}`;
    return {path:p, len};
  };
  const animateDraw = (edge, delay=0) => {
    setTimeout(()=>{ edge.path.style.strokeDashoffset='0'; }, delay);
  };
  const token = (d, delay=0) => {
    const c = document.createElementNS(NS,'circle');
    c.setAttribute('class','token');
    c.setAttribute('r','8');                           // radius set as attribute
    c.style.offsetPath = `path('${d}')`;
    setTimeout(()=>{ c.style.animation = `travel ${BALL_MS/1000}s linear 0s 1 forwards`; }, delay);
    svg.appendChild(c);
    return c;
  };
  const midBall = (x,y,good) => {
    const c = document.createElementNS(NS,'circle');
    c.setAttribute('cx',x); c.setAttribute('cy',y);
    c.setAttribute('r','10');
    c.setAttribute('class', `mid ${good?'good pulse':'bad'}`);
    svg.appendChild(c); return c;
  };
  const leaf = (x,y) => {
    const c = document.createElementNS(NS,'circle');
    c.setAttribute('cx',x); c.setAttribute('cy',y);
    c.setAttribute('r','12');
    c.setAttribute('class','leaf');
    svg.appendChild(c); return c;
  };

  // First layer edges + tokens
  const first = [];
  const firstXStart = root.x - span1/2;
  for (let i=0;i<N;i++){
    const x = firstXStart + (span1/(N-1))*i;
    const d = pathD(root.x, root.y+28, x, y1);
    const e = drawEdge(d);
    const startDelay = i*STAGGER_MS;
    animateDraw(e, startDelay);
    token(d, startDelay + DRAW_MS*0.4);
    first.push({x, d, edge:e});
  }

  // After first layer drawn ‚Äî pick top-M and show mid balls (green/red)
  const firstLayerDone = (N-1)*STAGGER_MS + DRAW_MS + 100;
  setTimeout(()=>{
    // deterministic center-out winners (replace with random if you prefer)
    const order = [...Array(N).keys()].sort((a,b)=>Math.abs(a-(N-1)/2)-Math.abs(b-(N-1)/2));
    const winnersIdx = order.slice(0, M).sort((a,b)=>a-b);

    const winners = [];
    first.forEach((node, idx)=>{
      const isWinner = winnersIdx.includes(idx);
      midBall(node.x, y1, isWinner);
      if (isWinner) winners.push(node);
    });

    // Expand only winners
    const leaves = [];
    winners.forEach((w, wi)=>{
      const baseX = w.x - span2/2;
      for (let c=0;c<CHILDREN;c++){
        const x2 = baseX + (span2/(CHILDREN-1))*c;
        const d2 = pathD(w.x, y1+10, x2, y2);
        const e2 = drawEdge(d2);
        const delay = wi*CHILDREN*STAGGER_MS + c*STAGGER_MS;
        animateDraw(e2, delay);
        token(d2, delay + DRAW_MS*0.4);
        leaves.push(leaf(x2, y2));
      }
    });

    // Final: choose one green winner among leaves (others red) ‚Äî once
    const finalPickAt = winners.length*CHILDREN*STAGGER_MS + DRAW_MS + 200;
    setTimeout(()=>{
      const winIdx = 0; // deterministic left-most; swap to Math.random() for randomness
      leaves.forEach((l, i)=>{
        if (i===winIdx){ l.classList.add('good','pulse'); }
        else { l.classList.add('bad'); }
      });
    }, finalPickAt);

  }, firstLayerDone);
}

// Auto-start once when the SVG scrolls into view
document.addEventListener('DOMContentLoaded', () => {
  const target = document.getElementById('beam');
  if (!target) return;
  if (!('IntersectionObserver' in window)) { startBeam(); return; }
  const obs = new IntersectionObserver(
    entries => { if (entries[0]?.isIntersecting) { startBeam(); obs.disconnect(); } },
    { threshold: 0.25 }
  );
  obs.observe(target);
});
</script>

<h3 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Diverse Verifier Tree Search</h3>

  <p>Unlike standard beam search, where all candidates share a single prefix and compete within the same pool, Diverse Verifier Tree Search 
    ensures that different subtrees evolve on their own. </p>
    
    <p>This separation encourages the search to explore a wider variety of reasoning paths instead of collapsing too quickly on similar solutions.</p>
    <p>Starting from an initial set of beams, the algorithm splits them into separate subtrees that are expanded independently.</p>
   <p>At each step, a reward model evaluates the partial solutions and guides which branches to keep growing.</p>


<div class="wrap">
  <h1>Diverse Verifier Tree Search</h1>
  <p class="sub">Split beams into independent subtrees ‚Üí verifier selects the best step in each ‚Üí expand and choose final answers.</p>
  <svg id="diverse" viewBox="0 0 1200 400"></svg>
</div>

<script>
function startDiverse(){
  const NS  = 'http://www.w3.org/2000/svg';
  const svg = document.getElementById('diverse');
  if (!svg || svg.dataset.started === '1') return;
  svg.dataset.started = '1';
  svg.innerHTML = '';

  // ---- Tunables ----
  const TREES = 2;          // how many subtrees
  const CHILDREN = 2;       // leaves per subtree
  const DRAW_MS = 700;
  const STAGGER_MS = 150;
  const BALL_MS = 1400;

  // Layout
  const root = {x:600,y:70};
  const y1=180, y2=320;
  const span=400; // spacing between subtrees

  // Root node
  const gRoot = document.createElementNS(NS,'g');
  gRoot.setAttribute('transform',`translate(${root.x-80},${root.y-28})`);
  const r = document.createElementNS(NS,'rect');
  r.setAttribute('width',160); r.setAttribute('height',56); r.setAttribute('rx',18);
  r.setAttribute('fill','var(--node)'); r.setAttribute('class','node');
  const t = document.createElementNS(NS,'text');
  t.setAttribute('x',80); t.setAttribute('y',34); t.setAttribute('text-anchor','middle');
  t.textContent='Math problem';
  gRoot.appendChild(r); gRoot.appendChild(t); svg.appendChild(gRoot);

  // Helpers
  const pathD=(x1,y1,x2,y2)=>{
    const midX=(x1+x2)/2,bendY=(y1+y2)/2;
    return `M ${x1} ${y1} C ${midX} ${bendY}, ${midX} ${bendY}, ${x2} ${y2}`;
  };
  const drawEdge=(d)=>{
    const p=document.createElementNS(NS,'path');
    p.setAttribute('d',d);p.setAttribute('class','edge draw');svg.appendChild(p);
    const len=p.getTotalLength();p.style.strokeDasharray=`${len}`;p.style.strokeDashoffset=`${len}`;
    return {path:p,len};
  };
  const animateDraw=(edge,delay=0)=>{setTimeout(()=>{edge.path.style.strokeDashoffset='0';},delay)};
  const token=(d,delay=0)=>{
    const c=document.createElementNS(NS,'circle');
    c.setAttribute('r','8');c.setAttribute('class','token');
    c.style.offsetPath=`path('${d}')`;
    setTimeout(()=>{c.style.animation=`travel ${BALL_MS/1000}s linear 0s 1 forwards`;},delay);
    svg.appendChild(c);return c;
  };
  const midBall=(x,y,good)=>{
    const c=document.createElementNS(NS,'circle');
    c.setAttribute('cx',x);c.setAttribute('cy',y);c.setAttribute('r','10');
    c.setAttribute('class',`mid ${good?'good pulse':'bad'}`);svg.appendChild(c);return c;
  };
  const leaf=(x,y)=>{
    const c=document.createElementNS(NS,'circle');
    c.setAttribute('cx',x);c.setAttribute('cy',y);c.setAttribute('r','12');
    c.setAttribute('class','leaf');svg.appendChild(c);return c;
  };

  // Create subtrees
  const subtreeCenters=[root.x-span/2, root.x+span/2];
  const allLeaves=[];

  subtreeCenters.forEach((cx,i)=>{
    const d=pathD(root.x,root.y+28,cx,y1);
    const e=drawEdge(d);
    animateDraw(e,i*STAGGER_MS);
    token(d,i*STAGGER_MS+DRAW_MS*0.4);

    // Pick a winner at intermediate level (deterministic: left subtree good, right subtree bad)
    const isWinner=true;
    midBall(cx,y1,isWinner);

    // Expand children
    const baseX=cx-120;
    for(let j=0;j<CHILDREN;j++){
      const x2=baseX+(240/(CHILDREN-1))*j;
      const d2=pathD(cx,y1+10,x2,y2);
      const e2=drawEdge(d2);
      animateDraw(e2,j*STAGGER_MS+200);
      token(d2,j*STAGGER_MS+DRAW_MS*0.4+200);
      allLeaves.push(leaf(x2,y2));
    }
  });

  // Final pick: one green leaf per subtree
  const finalPickAt=DRAW_MS*2+500;
  setTimeout(()=>{
    // deterministically choose left-most of each subtree
    const leavesPerTree=CHILDREN;
    allLeaves.forEach((l,i)=>{
      if(i%leavesPerTree===0){l.classList.add('good','pulse');}
      else{l.classList.add('bad');}
    });
  },finalPickAt);
}

// Auto-start once on scroll into view
document.addEventListener('DOMContentLoaded',()=>{
  const target=document.getElementById('diverse');
  if(!target)return;
  if(!('IntersectionObserver'in window)){startDiverse();return;}
  const obs=new IntersectionObserver(
    entries=>{if(entries[0]?.isIntersecting){startDiverse();obs.disconnect();}},
    {threshold:0.25}
  );
  obs.observe(target);
});
</script>






<!--Strategies comparision-->

<h3 style="margin:1.25rem 0 0.5rem 0; font-size:1.25rem; font-weight:700;">Strategies performance comparison</h3>

  <p>When applying reasoning strategies to language models, it is important to understand how different approaches trade off between 
    accuracy, diversity, and compute cost.</p>
    
    <p>Methods like Best-of-N, Beam Search, and Diverse Verifier Tree Search all leverage verifier feedback in different ways,
       and comparing their performance highlights which strategies scale most effectively as we increase the number of generations.</p>

<h1>Strategies Performance Comparison</h1>
<svg id="chart" viewBox="0 0 900 540"></svg> <!-- a bit taller to fit bottom legend -->

<script>
(function(){
  // ---- Data ---------------------------------------------------------------
  const series = {
    Majority:        [[2,27],[4,33],[8,38],[16,41],[32,44],[64,45],[128,46]],
    "Best-of-N":     [[2,28],[4,36],[8,40],[16,43],[32,45],[64,48],[128,49]],
    "Beam search":   [[2,29],[4,38],[8,44],[16,49],[32,51],[64,52],[128,53]],
    DVTS:            [[2,28],[4,39],[8,42],[16,47],[32,51],[64,55],[128,56]],
  };
  const colors = {
    Majority:"#3b82f6",
    "Best-of-N":"#f59e0b",
    "Beam search":"#22c55e",
    DVTS:"#ef4444"
  };

  // ---- Layout -------------------------------------------------------------
  const svg = d3.select("#chart");
  const margin = {top:30, right:20, bottom:90, left:60}; // extra bottom for legend
  const width  = 900 - margin.left - margin.right;
  const height = 540 - margin.top - margin.bottom;

  // Guard to run once
  function started(){ return svg.attr("data-started")==="1"; }
  function markStarted(){ svg.attr("data-started","1"); }

  function startChart(){
    if (started()) return;
    markStarted();

    const root = svg.append("g")
      .attr("transform", `translate(${margin.left},${margin.top})`);

    const x = d3.scaleLog().domain([2,128]).range([0,width]);
    const y = d3.scaleLinear().domain([25,60]).range([height,0]);

    // Axes
    root.append("g")
      .attr("transform",`translate(0,${height})`)
      .attr("class","axis")
      .call(d3.axisBottom(x).ticks(7,",d"));
    root.append("g")
      .attr("class","axis")
      .call(d3.axisLeft(y));

    // Labels
    root.append("text")
      .attr("x",width/2).attr("y",height+40)
      .attr("text-anchor","middle").text("Number of generations per problem");
    root.append("text")
      .attr("transform","rotate(-90)")
      .attr("x",-height/2).attr("y",-45)
      .attr("text-anchor","middle").text("MATH-500 accuracy (%)");

    // Baselines
    root.append("line")
      .attr("class","baseline").attr("x1",0).attr("x2",width)
      .attr("y1",y(29)).attr("y2",y(29));
    root.append("text").attr("x",width-4).attr("y",y(29)-6)
      .attr("text-anchor","end").text("Llama 3.2 1B");
    root.append("line")
      .attr("class","baseline").attr("x1",0).attr("x2",width)
      .attr("y1",y(51)).attr("y2",y(51));
    root.append("text").attr("x",width-4).attr("y",y(51)-6)
      .attr("text-anchor","end").text("Llama 3.1 8B");

    const line = d3.line()
      .x(d=>x(d[0]))
      .y(d=>y(d[1]))
      .curve(d3.curveMonotoneX);

    // Animate each series
    const names = Object.keys(series);
    names.forEach((name,i)=>{
      const values = series[name];
      const path = root.append("path")
        .datum(values)
        .attr("fill","none")
        .attr("stroke",colors[name])
        .attr("stroke-width",2)
        .attr("d",line);

      const L = path.node().getTotalLength();
      path.attr("stroke-dasharray", `${L} ${L}`)
          .attr("stroke-dashoffset", L)
        .transition()
          .duration(2500)
          .delay(i*350)
          .ease(d3.easeLinear)
          .attr("stroke-dashoffset", 0);

      // Dots that rise up from the bottom
      root.selectAll(`.dot-${i}`)
        .data(values).enter()
        .append("circle")
        .attr("class",`dot-${i}`)
        .attr("cx",d=>x(d[0]))
        .attr("cy",y(25)) // bottom start
        .attr("r",4)
        .attr("fill",colors[name])
        .transition().duration(2500).delay(i*350)
        .attr("cy",d=>y(d[1]));
    });

    // ---- Bottom legend (centered) ----------------------------------------
    const legend = root.append("g")
      .attr("class","legend")
      .attr("transform", `translate(${width/2}, ${height+60})`);

    const items = names.map((n,i)=>({name:n, color:colors[n]}));
    const spacing = 130;  // horizontal spacing between legend items
    const startX = -((items.length-1)*spacing)/2;

    const lg = legend.selectAll("g.item")
      .data(items).enter()
      .append("g")
      .attr("class","item")
      .attr("transform",(d,i)=>`translate(${startX + i*spacing},0)`);

    lg.append("circle").attr("r",5).attr("fill",d=>d.color).attr("cx",0).attr("cy",0);
    lg.append("text").text(d=>d.name).attr("x",10).attr("y",0);
  }

  // ---- Auto-start when scrolled into view --------------------------------
  document.addEventListener("DOMContentLoaded", ()=>{
    const el = document.getElementById("chart");
    if (!("IntersectionObserver" in window)) { startChart(); return; }
    const obs = new IntersectionObserver(
      entries => {
        if (entries[0]?.isIntersecting) {
          startChart();
          obs.disconnect();
        }
      },
      { threshold: 0.25 }
    );
    obs.observe(el);
  });
})();
</script>

<section style="margin-top:60px; margin-bottom:40px; max-width:900px; margin-left:auto; margin-right:auto;">
  <h2 style="
    font-size:2em;
    font-weight:700;
    letter-spacing:0.01em;
    margin-bottom:18px;
    background:linear-gradient(90deg, #010610 70%, #eab308 100%);
    -webkit-background-clip:text;
    -webkit-text-fill-color:transparent;
    background-clip:text;
    text-align:center;
  ">
    Verifiers in Reasoning Models
  </h2>

  <p>
    But how do we know which paths to explore along the branches of our trees when using each strategy? 
    This is when <b>verifiers</b> come in play.
    Verifiers are part of a larger category of tools we use for the the models reasoning process called operators. 
    And operators do basically everything you can think of. There are operators for refining the reasoning 
    steps when they fall under certain threshold, or multiply a branch with a policy model. 
    They can also prune entire paths that are going nowhere. And verifiers do exactly what their name tells us. 
  </p>

  <blockquote style="font-style:italic; background:#fefce8; padding:12px 18px; border-left:4px solid #eab308; border-radius:6px; margin:20px 0;">
    ‚ÄúThe verifier assesses the quality of the final answer or intermediate reasoning steps and provides feedback to the reasoner.‚Äù 
    <a href="https://arxiv.org/html/2504.09037v1" target="_blank" rel="noopener" style="color:#2563eb; text-decoration:none;">
      (Ke et al., 2025)
    </a>
  </blockquote>

  <p>
    They can be divided based on different categories: <strong>the form</strong> of the verifier's result, the level of <strong>granularity</strong>, the <strong>source</strong> of the verifier, and if they require <strong>  additional training</strong>. 
    <a href="https://arxiv.org/abs/2411.11504" target="_blank" rel="noopener">(Guan et al., 2022)</a>.
  </p>

  <p>
    The feedback can be presented back to the reasoner in many forms. <strong>Binary feedback</strong> provides a simple pass/fail judgment, while <strong> score-based feedback</strong>  offers continuous values that 
    indicate the degree of correctness. <strong>  Ranking feedback </strong>compares multiple outputs and orders them, giving the optimization 
    process relative preferences. Finally, <strong> textual feedback </strong>delivers the richest information, often including rationales, critiques, 
    or detailed explanations of why an answer is considered strong or weak.
  </p>

  <p>
    The granularity of verification can be considered at three different levels. <strong> Token-level verifiers</strong> evaluate predictions 
    one token at a time, offering the most fine-grained form of feedback. <strong>Thought-level verifiers</strong> instead examine reasoning 
    steps or sentences as whole units, providing judgments at an intermediate scale. <strong>Trajectory-level verifiers</strong> operate at the 
    entire sequence of reasoning from start to finish. Each level offers different advantages, with token-level being the most 
    detailed, and trajectory-level giving a rounded view of reasoning quality.
  </p>

  <p>
    From the perspective of their source, verifiers can be divided into program-based and model-based approaches. 
    <strong>Program-based verifiers rely on deterministic rules</strong>, which makes them consistent, interpretable, and transparent. 
    However, this is in exchange for no adaptability in more dynamic tasks. <strong>Model-based verifiers generate judgments through 
    probabilistic models instead</strong>. This allows them to adapt to diverse contexts and tasks, but increases the uncertainty 
    and decreases reliability.
  </p>

  <p>
    Finally, verifiers differ in whether they require additional training. <strong>Those that do are often fine-tuned on task-specific 
    data improving their judgement in one domain</strong>. The drawback is that they are not generalizable outside of the task at hand. 
    <strong>Verifiers that do not require additional training are based on pre-existing models</strong>. They will not be as precise as trained 
    verifiers, but handle data variation better, and are not dependant on task-specific datasets.
  </p>

  <p><strong>Now, here is a table for you to see some examples of verifiers after this long read:</strong></p>
  <div style="background:#f1f5f9; padding:16px; border-radius:10px; text-align:center; margin:20px 0;">
    <h3 style="text-align:center; margin:30px 0 14px 0; font-size:1.4em; font-weight:600; color:#111;">
  Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering
</h3>

<table style="width:100%; border-collapse:collapse; font-size:0.95em; margin:0 auto; background:#fff; box-shadow:0 3px 14px rgba(0,0,0,0.05); border-radius:10px; overflow:hidden;">
  <thead style="background:#1e293b; color:#fff;">
    <tr>
      <th style="padding:12px; text-align:center;">Verifier Type</th>
      <th style="padding:12px; text-align:center;">Verification Form</th>
      <th style="padding:12px; text-align:center;">Verify Granularity</th>
      <th style="padding:12px; text-align:center;">Verifier Source</th>
      <th style="padding:12px; text-align:center;">Extra Training</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="padding:10px;">Golden Annotation</td>
      <td style="padding:10px;">Binary/Text</td>
      <td style="padding:10px;">Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Program Based</td>
      <td style="padding:10px;">No</td>
    </tr>
    <tr style="background:#f9fafb;">
      <td style="padding:10px;">Rule-based</td>
      <td style="padding:10px;">Binary/Text</td>
      <td style="padding:10px;">Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Program Based</td>
      <td style="padding:10px;">No</td>
    </tr>
    <tr>
      <td style="padding:10px;">Code Interpreter</td>
      <td style="padding:10px;">Binary/Score/Text</td>
      <td style="padding:10px;">Token/Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Program Based</td>
      <td style="padding:10px;">No</td>
    </tr>
    <tr style="background:#f9fafb;">
      <td style="padding:10px;">ORM</td>
      <td style="padding:10px;">Binary/Score/Rank/Text</td>
      <td style="padding:10px;">Full Trajectory</td>
      <td style="padding:10px;">Model Based</td>
      <td style="padding:10px;">Yes</td>
    </tr>
    <tr>
      <td style="padding:10px;">Language Model</td>
      <td style="padding:10px;">Binary/Score/Rank/Text</td>
      <td style="padding:10px;">Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Model Based</td>
      <td style="padding:10px;">Yes</td>
    </tr>
    <tr style="background:#f9fafb;">
      <td style="padding:10px;">Tool</td>
      <td style="padding:10px;">Binary/Score/Rank/Text</td>
      <td style="padding:10px;">Token/Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Program Based</td>
      <td style="padding:10px;">No</td>
    </tr>
    <tr>
      <td style="padding:10px;">Search Engine</td>
      <td style="padding:10px;">Text</td>
      <td style="padding:10px;">Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Program Based</td>
      <td style="padding:10px;">No</td>
    </tr>
    <tr style="background:#f9fafb;">
      <td style="padding:10px;">PRM</td>
      <td style="padding:10px;">Score</td>
      <td style="padding:10px;">Token/Thought Step</td>
      <td style="padding:10px;">Model Based</td>
      <td style="padding:10px;">Yes</td>
    </tr>
    <tr>
      <td style="padding:10px;">Knowledge Graph</td>
      <td style="padding:10px;">Text</td>
      <td style="padding:10px;">Thought Step/Full Trajectory</td>
      <td style="padding:10px;">Program Based</td>
      <td style="padding:10px;">No</td>
    </tr>
  </tbody>
</table>

  </div>

  <p>
    As a last note, it important to notice that verifiers in a sense imitate a kind of response from the environment for the LM, 
    allowing it to have a perception on how their action had a response. If this want to see how much can be done with this interaction, 
    maybe check this <a href="[HYPERLINK]" target="_blank" rel="noopener">link [the idea is to connect our blog to the other team's]</a>.
  </p>
</section>


  <h1 style="text-align:left">How Good Are Models at Reasoning?</h1>
  <h2>Benchmarks</h2>
  <p>
    Benchmarks are widely used to evaluate the capabilities of large language models (LLMs).
    They consist of curated problem sets focused on specific skills - such as mathematics, programming, scientific understanding, or medical diagnostics.
    By testing models on these tasks, we get a snapshot of their reasoning power and generalization abilities.
  </p>
  <h3>Benchmark Reasoning Categories</h3>
  <p>
    Please see the description of possible problem sets in the table below. Each category represents a distinct type of reasoning  challenge used in 
    benchmark evaluations. You can search through the table by typing keywords (e.g. <em>"math"</em>, <em>"medical"</em>, <em>"code"</em>) into the
    search bar in the top-right corner of the table.
  </p>
    <table id="benchmark-table" class="display">
      <thead>
        <tr>
          <th>Category</th>
          <th>Description</th>
          <th>Benchmarks</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Math Problems</td>
          <td>Solves mathematical problems showcasing capabilities of reasoning LLMs.</td>
          <td>AIME, MATH-500, OlympiadBench</td>
        </tr>
        <tr>
          <td>Code Problems</td>
          <td>Uses logical thinking and structured problem-solving in programming tasks.</td>
          <td>Codeforces, LiveCodeBench</td>
        </tr>
        <tr>
          <td>Scientific Problems</td>
          <td>Involves multi-domain reasoning across physics, chemistry, and biology.</td>
          <td>GPQA Diamond, MMLU-Pro</td>
        </tr>
        <tr>
          <td>Agent Reasoning</td>
          <td>Tests planning and decision-making in interactive and tool-using environments.</td>
          <td>WebShop, WebArena, SciWorld</td>
        </tr>
        <tr>
          <td>Medical Reasoning</td>
          <td>Mimics diagnostic reasoning and treatment planning in clinical contexts.</td>
          <td>MedQA, Medbullets</td>
        </tr>
        <tr>
          <td>Multimodal Reasoning</td>
          <td>Combines text and visual input to test cross-modal reasoning skills.</td>
          <td>MMMU, MathVista, MM-IQ</td>
        </tr>
      </tbody>
    </table>
    <br>
      A strong model is expected to perform well across multiple diverse benchmarks, demonstrating not just memorization or task-specific tricks, but real, 
      transferable reasoning. In this way, benchmarks help define what it means for a model to be universal rather than narrowly overfitted.
    <br>
    <script>
      $(document).ready(function () {
        $('#benchmark-table').DataTable({
          paging: false,
          info: false,
          language: {
            search: "üîç Search benchmarks:"
          }
        });
      });
    </script>

<body>
  <h3>State-of-the-Art Model Performance on Benchmarks</h3>
  <p>
    The following table shows Pass@1 or Percentile scores across several benchmark tasks.
  </p>
  <table id="benchmarkTable" class="display">
    <thead>
      <tr>
        <th>Benchmark</th>
        <th>Metric</th>
        <th>DeepSeek-R1</th>
        <th>OpenAI-o1-1217</th>
        <th>DeepSeek-R1-32B</th>
        <th>OpenAI-o1-mini</th>
        <th>DeepSeek-V3</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>AIME 2024</td>
        <td>Pass@1</td>
        <td>79.8%</td>
        <td>79.2%</td>
        <td>72.6%</td>
        <td>63.6%</td>
        <td>39.2%</td>
      </tr>
      <tr>
        <td>Codeforces</td>
        <td>Percentile</td>
        <td>96.3%</td>
        <td>96.6%</td>
        <td>90.6%</td>
        <td>93.4%</td>
        <td>58.7%</td>
      </tr>
      <tr>
        <td>GPQA Diamond</td>
        <td>Pass@1</td>
        <td>71.5%</td>
        <td>75.7%</td>
        <td>62.1%</td>
        <td>60.0%</td>
        <td>59.1%</td>
      </tr>
      <tr>
        <td>MATH-500</td>
        <td>Pass@1</td>
        <td>97.3%</td>
        <td>96.4%</td>
        <td>94.3%</td>
        <td>90.0%</td>
        <td>90.2%</td>
      </tr>
      <tr>
        <td>MMLU</td>
        <td>Pass@1</td>
        <td>90.8%</td>
        <td>91.8%</td>
        <td>87.4%</td>
        <td>85.2%</td>
        <td>88.5%</td>
      </tr>
      <tr>
        <td>SWE-bench Verified</td>
        <td>Resolved</td>
        <td>49.2%</td>
        <td>48.9%</td>
        <td>41.6%</td>
        <td>36.8%</td>
        <td>42.0%</td>
      </tr>
    </tbody>
  </table>

  <p class="citation">(DeepSeek-AI et al., 2025)</p>

  <script>
    $(document).ready(function () {
      $('#benchmarkTable').DataTable({
        paging: false,
        info: false,
        responsive: true,
        language: {
          search: "üîç Search benchmarks:"
        }
      });
    });
  </script>

<hr>

<h3>The Problem With Benchmarks</h3>
<p>Despite their usefulness, traditional benchmarks are facing growing criticism.</p>

<p>
  Task difficulty is often hard to define. What makes a problem "difficult" is often subjective. This makes it tricky to scale problem sets meaningfully 
  or test a model's performance on progressively harder tasks.
</p>
<p>
  Another concern is data leakage from training corpora. Many benchmark problems have ended up in the training data of large models, whether intentionally or not. 
  This makes it unclear whether the model is reasoning through a solution ‚Äî or simply memorizing and regurgitating it.
</p>

<h2>Puzzle-Based Benchmarks</h2>
<p><em>Shojaee et al., 2025</em></p>
<p>
  To address these limitations, researchers are exploring alternative benchmarks. One compelling direction is using puzzle-like problems, such as the 
  Tower of Hanoi, where complexity can be precisely controlled ‚Äî by simply increasing the number of disks. In this setup, models are evaluated not just 
  on their accuracy, but on how well they scale with increasing task difficulty.
</p>

<div class="hanoi-section">
<h4>Tower of Hanoi (3 Disks)</h4>
<p>Auto-solving animation (A ‚Üí C)</p>

<div class="peg-container">
  <div class="peg" id="pegA"></div>
  <div class="peg" id="pegB"></div>
  <div class="peg" id="pegC"></div>
</div>

<div class="peg-container">
  <div class="peg-label">A</div>
  <div class="peg-label">B</div>
  <div class="peg-label">C</div>
</div>

<script>
  const pegs = {
    A: document.getElementById('pegA'),
    B: document.getElementById('pegB'),
    C: document.getElementById('pegC'),
  };

  const disks = [
    { class: 'disk1', size: 3 },
    { class: 'disk2', size: 2 },
    { class: 'disk3', size: 1 },
  ];

  let state;

  const sleep = (ms) => new Promise(res => setTimeout(res, ms));

  function resetPegs() {
    // Clear pegs
    pegs.A.innerHTML = '';
    pegs.B.innerHTML = '';
    pegs.C.innerHTML = '';

    // Reset state
    state = { A: [], B: [], C: [] };

    // Add disks back to peg A
    disks.forEach((d, i) => {
      const el = document.createElement('div');
      el.className = `disk ${d.class}`;
      el.style.bottom = `${i * 22}px`;
      pegs.A.appendChild(el);
      state.A.push(el);
    });
  }

  async function move(n, from, to, aux) {
    if (n === 0) return;
    await move(n - 1, from, aux, to);

    await sleep(800);
    const disk = state[from].pop();
    state[to].push(disk);
    pegs[to].appendChild(disk);
    disk.style.bottom = `${(state[to].length - 1) * 22}px`;

    await move(n - 1, aux, to, from);
  }

  async function loopHanoi() {
    while (true) {
      resetPegs();
      await move(3, 'A', 'C', 'B');
      await sleep(1000); // pause before restarting
    }
  }

  // Start looping animation
  loopHanoi();
</script>

</div>

<h3>When Reasoning Models Collapse</h3>
<p>
  A surprising trend emerges as task complexity increases: reasoning-tuned models initially outperform their baseline counterparts, 
  but beyond a certain threshold, both collapse in performance. This sharp decline challenges assumptions about the robustness of current 
  reasoning models and exposes their fragility under higher cognitive demands.
</p>

<h4 style="text-align: center;">Model Accuracy vs. Task Complexity</h4>
<canvas id="accuracyChart" class="chart-on-scroll" width="700" height="400"></canvas>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
  function renderAccuracyChart() {
  const ctx = document.getElementById('accuracyChart').getContext('2d');

  new Chart(ctx, {
    type: 'line',
    data: {
      labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
      datasets: [
        {
          label: 'DeepSeek-R1',
          data: [100, 98, 95, 90, 85, 70, 60, 10, 2, 1, 0, 0],
          borderColor: '#4ECDC4',
          backgroundColor: 'transparent',
          tension: 0.3
        },
        {
          label: 'DeepSeek-V3',
          data: [98, 97, 96, 85, 30, 20, 10, 5, 1, 0, 0, 0],
          borderColor: '#FF6B6B',
          backgroundColor: 'transparent',
          tension: 0.3
        }
      ]
    },
    options: {
      responsive: true,
      animation: {
        duration: 2500,
        easing: 'easeOutQuart'
      },
      plugins: {
        legend: {
          position: 'bottom'
        },
        title: {
          display: false
        }
      },
      scales: {
        y: {
          beginAtZero: false,
          max: 100,
          title: {
            display: true,
            text: 'Accuracy (%)'
          }
        },
        x: {
          title: {
            display: true,
            text: 'Task Complexity'
          }
        }
      }
    }
  });
  }
</script>

<script>
  const observer = new IntersectionObserver((entries, observer) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        if (entry.target.id === 'accuracyChart') {
          renderAccuracyChart();
        }
        observer.unobserve(entry.target); // only run once
      }
    });
  }, {
    threshold: 0.4 // Trigger when 40% visible
  });

  document.querySelectorAll('.chart-on-scroll').forEach(chart => {
    observer.observe(chart);
  });
</script>


<h3>Even With Help, They Struggle</h3>
<p>
  Interestingly, even when the model is given the solution in the prompt, performance doesn‚Äôt improve significantly.
  This suggests that the model doesn't simply fail to find a solution ‚Äî it struggles to use or interpret one when given.
</p>

<h4 style="text-align: center;">Claude-3.7-Sonnet Performance</h4>
<canvas id="claudeChart" class="chart-on-scroll" width="700" height="400"></canvas>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<!-- Chart render function -->
<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {

    function renderClaudeChart() {
      const ctx = document.getElementById('claudeChart').getContext('2d');

      new Chart(ctx, {
        type: 'line',
        data: {
          labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
          datasets: [
            {
              label: 'Default',
              data: [99.5, 99.5, 99.5, 99.5, 90, 55, 70, 15, 5, 2, 1, 1],
              borderColor: '#DE6E4B',
              borderWidth: 3,
              pointStyle: 'rect',
              pointRadius: 6,
              tension: 0.3
            },
            {
              label: 'Algorithm Given',
              data: [99.5, 99.5, 99.5, 99.5, 90, 70, 65, 10, 2, 1, 1, 1],
              borderColor: '#B0422F',
              borderDash: [6, 4],
              borderWidth: 3,
              pointStyle: 'circle',
              pointRadius: 6,
              tension: 0.3
            }
          ]
        },
        options: {
          responsive: true,
          animation: {
            duration: 1200,
            easing: 'easeOutQuart'
          },
          plugins: {
            legend: { position: 'bottom' },
            title: {
              display: true,
              text: 'Accuracy vs Complexity (Number of Disks)'
            }
          },
          scales: {
            y: {
              beginAtZero: true,
              max: 100,
              title: { display: true, text: 'Accuracy (%)' }
            },
            x: {
              title: { display: true, text: 'Complexity (Number of Disks)' }
            }
          }
        }
      });
    }

    const observer = new IntersectionObserver((entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.target.id === 'claudeChart') {
          renderClaudeChart();
          observer.unobserve(entry.target);
        }
      });
    }, {
      threshold: 0.4
    });

    document.querySelectorAll('.chart-on-scroll').forEach(chart => {
      observer.observe(chart);
    });

  });
</script>

<h3>A Curious Token Length Effect</h3>
<p>
  Another surprising effect: as task complexity grows, models tend to produce shorter outputs - even though longer answers 
  would likely lead to better performance. This indicates a possible failure in internal planning or token budgeting, challenging the assumption that more capable models
  will naturally expand their answers as needed.</p>

<h4 style="text-align: center;">o3-mini (high): Output Token Count vs Complexity</h4>
<canvas id="tokenChart" class="chart-on-scroll" width="700" height="400"></canvas>

<!-- Chart.js -->
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {

    function renderTokenChart() {
      const ctx = document.getElementById('tokenChart').getContext('2d');

      new Chart(ctx, {
        type: 'line',
        data: {
          labels: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20],
          datasets: [
            {
              label: 'Average Token Count',
              data: [500, 900, 2000, 4500, 8500, 20000, 18000, 12000, 7000, 5000, 3000, 2000],
              borderColor: '#0077BB',
              borderDash: [6, 4],
              borderWidth: 3,
              fill: false,
              tension: 0.3,
              pointRadius: 6,
              pointStyle: 'circle',
              backgroundColor: 'transparent'
            },
            {
              label: 'Individual Runs',
              type: 'scatter',
              data: [
                { x: 4, y: 4300 }, { x: 4, y: 4700 }, { x: 5, y: 8800 },
                { x: 5, y: 8000 }, { x: 6, y: 19500 }, { x: 6, y: 20500 },
                { x: 7, y: 18000 }, { x: 7, y: 17500 }, { x: 8, y: 11000 },
                { x: 9, y: 7500 }, { x: 9, y: 6900 }, { x: 10, y: 4500 },
                { x: 15, y: 3100 }, { x: 15, y: 2900 }, { x: 20, y: 2100 }
              ],
              backgroundColor: '#E63946',
              pointRadius: 4,
              showLine: false
            }
          ]
        },
        options: {
          responsive: true,
          animation: {
            duration: 1500,
            easing: 'easeOutQuart'
          },
          plugins: {
            legend: {
              position: 'bottom'
            },
            title: {
              display: true,
              text: 'Output Length vs. Complexity (Number of Disks)'
            }
          },
          scales: {
            y: {
              beginAtZero: true,
              title: {
                display: true,
                text: 'Output Tokens'
              }
            },
            x: {
              title: {
                display: true,
                text: 'Complexity (Number of Disks)'
              }
            }
          }
        }
      });
    }

    const observer = new IntersectionObserver((entries, observer) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.target.id === 'tokenChart') {
          renderTokenChart();
          observer.unobserve(entry.target);
        }
      });
    }, { threshold: 0.4 });

    document.querySelectorAll('.chart-on-scroll').forEach(chart => {
      observer.observe(chart);
    });

  });
</script>


<h2>Open Questions Remain</h2>
<p>
  To what extent can current models really reason? While benchmarks remain a useful tool, these findings highlight the fragility of current 
  approaches and the limitations of today's so-called ‚Äúreasoning models.‚Äù There is still a long road ahead to build systems that reason 
  reliably under increasing task difficulty ‚Äî and to prove that their reasoning is more than just a memorized pattern.
</p>

</body>

<section>
  <footer>
    <p> This blogpost grew out of work done as part of a research seminar at the University of Potsdam by
    Heitke Grops, Juan Danza Rovira, Lara Neubauer da Costa Schertel and Rodion Zorin under the
    supervision of Prof. David Schlangen. Thank you for reading!
    </p>
  </footer>
</section>
</html>
